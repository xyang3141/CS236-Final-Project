{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "normal = pd.read_csv('data.csv', sep=',', header = None)\n",
    "abnormal = pd.read_csv('data_abnormal.csv', sep=',', header = None)\n",
    "abnormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate([normal.T, abnormal.T], axis =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "y_train = np.concatenate([np.ones(5888), np.zeros(2878)])\n",
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "# from keras.utils import objectives\n",
    "import numpy as np\n",
    "from math import floor\n",
    "import tensorflow as tf\n",
    "# tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, LSTM, RepeatVector, Layer\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "# from keras.optimizers import SGD, RMSprop, Adam\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# from keras import objectives\n",
    "# import tensorflow.contrib.keras as keras\n",
    "from tensorflow.python.framework.ops import disable_eager_execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE:\n",
    "    def __init__(self, input_dim, latent_dim, hidden_dims, batch_size, optimizer='rmsprop', epsilon_std = .01):\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer\n",
    "        self.epsilon_std = epsilon_std\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        input_layer = Input(batch_shape=(self.batch_size, self.input_dim))\n",
    "        self.build_encoder(input_layer)\n",
    "        self.build_decoder()\n",
    "        self.autoencoder = Model(input_layer, self.x_decoded_mean)\n",
    "        vae_loss = self._get_vae_loss()\n",
    "        self.autoencoder.compile(optimizer=self.optimizer, loss=vae_loss)\n",
    "\n",
    "    def build_encoder(self, input_layer):\n",
    "        prev_layer = input_layer\n",
    "        for q in self.hidden_dims:\n",
    "            hidden = Dense(q, activation='relu')(prev_layer)\n",
    "            prev_layer = hidden\n",
    "        self._build_z_layers(hidden)\n",
    "        self.encoder = Model(input_layer, self.z_mean)\n",
    "\n",
    "    def _build_z_layers(self, hidden_layer):\n",
    "        self.z_mean = Dense(self.latent_dim)(hidden_layer)\n",
    "        self.z_log_sigma = Dense(self.latent_dim)(hidden_layer)\n",
    "\n",
    "    def build_decoder(self):\n",
    "        z = self._get_sampling_layer()\n",
    "        prev_layer = z\n",
    "        for q in self.hidden_dims:\n",
    "            hidden = Dense(q, activation='relu')(prev_layer)\n",
    "            prev_layer = hidden\n",
    "        self.x_decoded_mean = Dense(self.input_dim, activation='sigmoid')(prev_layer)\n",
    "\n",
    "        # Build the stand-alone generator\n",
    "        generator_input = Input((self.latent_dim,))\n",
    "        prev_layer = generator_input\n",
    "        for q in self.hidden_dims:\n",
    "            hidden = Dense(q, activation='relu')(prev_layer)\n",
    "            prev_layer = hidden\n",
    "        gen_x_decoded_mean = Dense(self.input_dim, activation='sigmoid')(prev_layer)\n",
    "        self.generator = Model(generator_input, gen_x_decoded_mean)\n",
    "\n",
    "    def _get_sampling_layer(self):\n",
    "        def sampling(args):\n",
    "            z_mean, z_log_sigma = args\n",
    "            epsilon = K.random_normal(shape=(self.batch_size, self.latent_dim),\n",
    "                                      mean=0., stddev=self.epsilon_std)\n",
    "            return z_mean + z_log_sigma * epsilon\n",
    "        return Lambda(sampling, output_shape=(self.latent_dim,))([self.z_mean, self.z_log_sigma])\n",
    "\n",
    "    def _get_vae_loss(self):\n",
    "        z_log_sigma = self.z_log_sigma\n",
    "        z_mean = self.z_mean\n",
    "        def vae_loss(x, x_decoded_mean):\n",
    "            reconstruction_loss = tf.keras.metrics.mean_squared_error(x, x_decoded_mean)\n",
    "            kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma))\n",
    "            return reconstruction_loss + kl_loss\n",
    "\n",
    "        return vae_loss\n",
    "    def sample(self, eps=None):\n",
    "        if eps is None:\n",
    "            eps = tf.random.normal(shape=(64, self.latent_dim))\n",
    "        return self.decode(eps, apply_sigmoid=True)\n",
    "\n",
    "    def encode(self, x):   #encode the images into the latent space\n",
    "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "    \n",
    "    #the reparametrization trick suggests that we randomly sample 'e' from a unit Gaussian,\n",
    "    #and then shift the randomly sampled 'e' by the latent distribution's mean 'mu' and scale it \n",
    "    #by the latent distribution's variance 'rho'.\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "\n",
    "    def decode(self, z, apply_sigmoid=False):  #decodes the latent space into the image\n",
    "        # logits = self.generator(tf.concat([z, self.z_mean], 1))\n",
    "        logits = self.generator(self.z_mean)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_LSTM(VAE):\n",
    "    def __init__(self, input_dim, latent_dim, hidden_dims, timesteps, batch_size, optimizer='rmsprop', epsilon_std = .01):\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.batch_size = batch_size\n",
    "        self.timesteps = timesteps\n",
    "        self.optimizer = optimizer\n",
    "        self.epsilon_std = epsilon_std\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        input_layer = Input(shape=(self.timesteps, self.input_dim,))\n",
    "        self.build_encoder(input_layer)\n",
    "        self.build_decoder()\n",
    "        self.autoencoder = Model(input_layer, self.x_decoded_mean)\n",
    "        vae_loss = self._get_vae_loss()\n",
    "        self.autoencoder.compile(optimizer=self.optimizer, loss=vae_loss)\n",
    "\n",
    "    def build_encoder(self, input_layer):\n",
    "        prev_layer = input_layer\n",
    "        for q in self.hidden_dims:\n",
    "            hidden = LSTM(q)(prev_layer)\n",
    "            prev_layer = hidden\n",
    "        self._build_z_layers(hidden)\n",
    "        self.encoder = Model(input_layer, self.z_mean)\n",
    "\n",
    "    def build_decoder(self):\n",
    "        z = self._get_sampling_layer()\n",
    "        prev_layer = RepeatVector(self.timesteps)(z)\n",
    "        for q in self.hidden_dims:\n",
    "            hidden = LSTM(q, return_sequences=True)(prev_layer)\n",
    "            prev_layer = hidden\n",
    "        self.x_decoded_mean = LSTM(self.input_dim, return_sequences=True)(prev_layer)\n",
    "\n",
    "        # Build the stand-alone generator\n",
    "        generator_input = Input((self.latent_dim,))\n",
    "        prev_layer = RepeatVector(self.timesteps)(generator_input)\n",
    "        for q in self.hidden_dims:\n",
    "            hidden = LSTM(q, return_sequences=True)(prev_layer)\n",
    "            prev_layer = hidden\n",
    "        gen_x_decoded_mean = LSTM(self.input_dim, return_sequences=True)(prev_layer)\n",
    "        self.generator = Model(generator_input, gen_x_decoded_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-03 08:28:11.921179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-03 08:28:11.928262: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2021-12-03 08:28:11.929409: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-12-03 08:28:11.931435: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8750 samples, validate on 50 samples\n",
      "Epoch 1/300\n",
      "8750/8750 [==============================] - ETA: 0s - loss: 0.0251"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/engine/training_v1.py:2057: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8750/8750 [==============================] - 14s 2ms/sample - loss: 0.0251 - val_loss: 0.0018\n",
      "Epoch 2/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0246 - val_loss: 0.0017\n",
      "Epoch 3/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0239 - val_loss: 0.0016\n",
      "Epoch 4/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0236 - val_loss: 0.0015\n",
      "Epoch 5/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0233 - val_loss: 0.0015\n",
      "Epoch 6/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0229 - val_loss: 0.0015\n",
      "Epoch 7/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0226 - val_loss: 0.0014\n",
      "Epoch 8/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0224 - val_loss: 0.0014\n",
      "Epoch 9/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0223 - val_loss: 0.0014\n",
      "Epoch 10/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0221 - val_loss: 0.0013\n",
      "Epoch 11/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0218 - val_loss: 0.0012\n",
      "Epoch 12/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0215 - val_loss: 0.0012\n",
      "Epoch 13/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0212 - val_loss: 0.0011\n",
      "Epoch 14/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0210 - val_loss: 0.0011\n",
      "Epoch 15/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0207 - val_loss: 0.0011\n",
      "Epoch 16/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0205 - val_loss: 0.0011\n",
      "Epoch 17/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0203 - val_loss: 0.0011\n",
      "Epoch 18/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0201 - val_loss: 0.0010\n",
      "Epoch 19/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0200 - val_loss: 9.7387e-04\n",
      "Epoch 20/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0198 - val_loss: 9.8506e-04\n",
      "Epoch 21/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0197 - val_loss: 9.5703e-04\n",
      "Epoch 22/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0196 - val_loss: 9.8071e-04\n",
      "Epoch 23/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0195 - val_loss: 9.4683e-04\n",
      "Epoch 24/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0193 - val_loss: 9.1963e-04\n",
      "Epoch 25/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0192 - val_loss: 9.1539e-04\n",
      "Epoch 26/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0191 - val_loss: 9.2738e-04\n",
      "Epoch 27/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0189 - val_loss: 9.0311e-04\n",
      "Epoch 28/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0188 - val_loss: 8.6014e-04\n",
      "Epoch 29/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0187 - val_loss: 8.4953e-04\n",
      "Epoch 30/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0186 - val_loss: 8.2724e-04\n",
      "Epoch 31/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0185 - val_loss: 8.5241e-04\n",
      "Epoch 32/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0184 - val_loss: 7.8676e-04\n",
      "Epoch 33/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0183 - val_loss: 7.6957e-04\n",
      "Epoch 34/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0182 - val_loss: 8.1676e-04\n",
      "Epoch 35/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0182 - val_loss: 7.9181e-04\n",
      "Epoch 36/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0181 - val_loss: 7.5120e-04\n",
      "Epoch 37/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0180 - val_loss: 7.5034e-04\n",
      "Epoch 38/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0179 - val_loss: 7.7246e-04\n",
      "Epoch 39/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0178 - val_loss: 7.4429e-04\n",
      "Epoch 40/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0177 - val_loss: 7.1286e-04\n",
      "Epoch 41/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0177 - val_loss: 6.9473e-04\n",
      "Epoch 42/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0176 - val_loss: 7.3885e-04\n",
      "Epoch 43/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0176 - val_loss: 7.0509e-04\n",
      "Epoch 44/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0175 - val_loss: 6.8417e-04\n",
      "Epoch 45/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0175 - val_loss: 6.9470e-04\n",
      "Epoch 46/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0174 - val_loss: 7.0778e-04\n",
      "Epoch 47/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0174 - val_loss: 7.0503e-04\n",
      "Epoch 48/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0173 - val_loss: 7.2190e-04\n",
      "Epoch 49/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0173 - val_loss: 6.7788e-04\n",
      "Epoch 50/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0172 - val_loss: 6.5885e-04\n",
      "Epoch 51/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0171 - val_loss: 6.7465e-04\n",
      "Epoch 52/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0171 - val_loss: 6.8911e-04\n",
      "Epoch 53/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0170 - val_loss: 6.4512e-04\n",
      "Epoch 54/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0170 - val_loss: 6.4603e-04\n",
      "Epoch 55/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0170 - val_loss: 6.2939e-04\n",
      "Epoch 56/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0169 - val_loss: 6.1116e-04\n",
      "Epoch 57/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0169 - val_loss: 6.1423e-04\n",
      "Epoch 58/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0169 - val_loss: 6.4451e-04\n",
      "Epoch 59/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0168 - val_loss: 6.5872e-04\n",
      "Epoch 60/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0168 - val_loss: 6.0678e-04\n",
      "Epoch 61/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0168 - val_loss: 6.5185e-04\n",
      "Epoch 62/300\n",
      "8750/8750 [==============================] - 12s 1ms/sample - loss: 0.0168 - val_loss: 6.4840e-04\n",
      "Epoch 63/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0167 - val_loss: 6.7556e-04\n",
      "Epoch 64/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0167 - val_loss: 6.2252e-04\n",
      "Epoch 65/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0167 - val_loss: 5.9768e-04\n",
      "Epoch 66/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0167 - val_loss: 6.1040e-04\n",
      "Epoch 67/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0166 - val_loss: 6.1071e-04\n",
      "Epoch 68/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0166 - val_loss: 5.9572e-04\n",
      "Epoch 69/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0166 - val_loss: 5.5411e-04\n",
      "Epoch 70/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0165 - val_loss: 5.6649e-04\n",
      "Epoch 71/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0165 - val_loss: 6.2713e-04\n",
      "Epoch 72/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0165 - val_loss: 6.0446e-04\n",
      "Epoch 73/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0164 - val_loss: 5.7482e-04\n",
      "Epoch 74/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0164 - val_loss: 5.9133e-04\n",
      "Epoch 75/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0164 - val_loss: 5.7235e-04\n",
      "Epoch 76/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0163 - val_loss: 5.3957e-04\n",
      "Epoch 77/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0163 - val_loss: 5.4156e-04\n",
      "Epoch 78/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0163 - val_loss: 5.6926e-04\n",
      "Epoch 79/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0163 - val_loss: 5.4668e-04\n",
      "Epoch 80/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0163 - val_loss: 5.4523e-04\n",
      "Epoch 81/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0162 - val_loss: 5.5422e-04\n",
      "Epoch 82/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0162 - val_loss: 5.2900e-04\n",
      "Epoch 83/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0162 - val_loss: 5.4662e-04\n",
      "Epoch 84/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0162 - val_loss: 5.2625e-04\n",
      "Epoch 85/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0162 - val_loss: 5.4062e-04\n",
      "Epoch 86/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0162 - val_loss: 5.4769e-04\n",
      "Epoch 87/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0161 - val_loss: 5.4242e-04\n",
      "Epoch 88/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0161 - val_loss: 5.0895e-04\n",
      "Epoch 89/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0161 - val_loss: 5.2941e-04\n",
      "Epoch 90/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0161 - val_loss: 5.4585e-04\n",
      "Epoch 91/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0161 - val_loss: 5.2779e-04\n",
      "Epoch 92/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0161 - val_loss: 5.2989e-04\n",
      "Epoch 93/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0161 - val_loss: 5.1836e-04\n",
      "Epoch 94/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0160 - val_loss: 5.2734e-04\n",
      "Epoch 95/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0160 - val_loss: 5.1104e-04\n",
      "Epoch 96/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0160 - val_loss: 5.3393e-04\n",
      "Epoch 97/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0160 - val_loss: 5.1273e-04\n",
      "Epoch 98/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0160 - val_loss: 5.3518e-04\n",
      "Epoch 99/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0160 - val_loss: 5.3642e-04\n",
      "Epoch 100/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0160 - val_loss: 5.0040e-04\n",
      "Epoch 101/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0160 - val_loss: 5.2754e-04\n",
      "Epoch 102/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0159 - val_loss: 5.2477e-04\n",
      "Epoch 103/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0159 - val_loss: 5.2381e-04\n",
      "Epoch 104/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0159 - val_loss: 5.4275e-04\n",
      "Epoch 105/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0159 - val_loss: 5.1456e-04\n",
      "Epoch 106/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0159 - val_loss: 5.0511e-04\n",
      "Epoch 107/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0159 - val_loss: 5.1369e-04\n",
      "Epoch 108/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0159 - val_loss: 4.9903e-04\n",
      "Epoch 109/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0158 - val_loss: 5.2062e-04\n",
      "Epoch 110/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0158 - val_loss: 5.4442e-04\n",
      "Epoch 111/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0158 - val_loss: 5.2144e-04\n",
      "Epoch 112/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0158 - val_loss: 5.2043e-04\n",
      "Epoch 113/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0158 - val_loss: 5.0916e-04\n",
      "Epoch 114/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0158 - val_loss: 5.4247e-04\n",
      "Epoch 115/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0158 - val_loss: 5.0202e-04\n",
      "Epoch 116/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0158 - val_loss: 5.1000e-04\n",
      "Epoch 117/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0158 - val_loss: 5.2016e-04\n",
      "Epoch 118/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0158 - val_loss: 4.7214e-04\n",
      "Epoch 119/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0158 - val_loss: 4.7195e-04\n",
      "Epoch 120/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0158 - val_loss: 4.9709e-04\n",
      "Epoch 121/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0157 - val_loss: 5.0605e-04\n",
      "Epoch 122/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0157 - val_loss: 5.1687e-04\n",
      "Epoch 123/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0157 - val_loss: 4.8626e-04\n",
      "Epoch 124/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0157 - val_loss: 4.8720e-04\n",
      "Epoch 125/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0157 - val_loss: 5.1167e-04\n",
      "Epoch 126/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0157 - val_loss: 4.9974e-04\n",
      "Epoch 127/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0157 - val_loss: 5.2176e-04\n",
      "Epoch 128/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0157 - val_loss: 4.9536e-04\n",
      "Epoch 129/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0157 - val_loss: 5.0276e-04\n",
      "Epoch 130/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0157 - val_loss: 4.9837e-04\n",
      "Epoch 131/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0157 - val_loss: 5.0309e-04\n",
      "Epoch 132/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0156 - val_loss: 5.4874e-04\n",
      "Epoch 133/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0156 - val_loss: 5.1548e-04\n",
      "Epoch 134/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0156 - val_loss: 5.2097e-04\n",
      "Epoch 135/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0156 - val_loss: 5.1452e-04\n",
      "Epoch 136/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0156 - val_loss: 5.0082e-04\n",
      "Epoch 137/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0156 - val_loss: 5.0189e-04\n",
      "Epoch 138/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0156 - val_loss: 4.8091e-04\n",
      "Epoch 139/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0156 - val_loss: 5.0670e-04\n",
      "Epoch 140/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0156 - val_loss: 5.0167e-04\n",
      "Epoch 141/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0156 - val_loss: 4.9180e-04\n",
      "Epoch 142/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0156 - val_loss: 4.8485e-04\n",
      "Epoch 143/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0156 - val_loss: 5.1293e-04\n",
      "Epoch 144/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0156 - val_loss: 5.1292e-04\n",
      "Epoch 145/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0156 - val_loss: 5.0842e-04\n",
      "Epoch 146/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0156 - val_loss: 5.1506e-04\n",
      "Epoch 147/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0156 - val_loss: 5.1224e-04\n",
      "Epoch 148/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0156 - val_loss: 5.2192e-04\n",
      "Epoch 149/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0156 - val_loss: 5.2466e-04\n",
      "Epoch 150/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0155 - val_loss: 4.7779e-04\n",
      "Epoch 151/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0155 - val_loss: 5.1426e-04\n",
      "Epoch 152/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0155 - val_loss: 5.2207e-04\n",
      "Epoch 153/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0155 - val_loss: 5.2254e-04\n",
      "Epoch 154/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0155 - val_loss: 5.4343e-04\n",
      "Epoch 155/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0155 - val_loss: 5.0220e-04\n",
      "Epoch 156/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0155 - val_loss: 4.9552e-04\n",
      "Epoch 157/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0155 - val_loss: 4.8848e-04\n",
      "Epoch 158/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0155 - val_loss: 5.0703e-04\n",
      "Epoch 159/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0155 - val_loss: 4.9916e-04\n",
      "Epoch 160/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0155 - val_loss: 5.0465e-04\n",
      "Epoch 161/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0155 - val_loss: 5.0942e-04\n",
      "Epoch 162/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0155 - val_loss: 4.9436e-04\n",
      "Epoch 163/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0155 - val_loss: 4.9920e-04\n",
      "Epoch 164/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0155 - val_loss: 4.9744e-04\n",
      "Epoch 165/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0155 - val_loss: 4.8240e-04\n",
      "Epoch 166/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0155 - val_loss: 4.9454e-04\n",
      "Epoch 167/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0155 - val_loss: 5.0613e-04\n",
      "Epoch 168/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0155 - val_loss: 5.0330e-04\n",
      "Epoch 169/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0155 - val_loss: 5.2724e-04\n",
      "Epoch 170/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0155 - val_loss: 5.0462e-04\n",
      "Epoch 171/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0155 - val_loss: 4.9193e-04\n",
      "Epoch 172/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0155 - val_loss: 4.8307e-04\n",
      "Epoch 173/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0154 - val_loss: 4.8973e-04\n",
      "Epoch 174/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0154 - val_loss: 5.1997e-04\n",
      "Epoch 175/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0154 - val_loss: 4.8128e-04\n",
      "Epoch 176/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0154 - val_loss: 5.1453e-04\n",
      "Epoch 177/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0154 - val_loss: 4.9462e-04\n",
      "Epoch 178/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0154 - val_loss: 4.9915e-04\n",
      "Epoch 179/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0154 - val_loss: 5.0804e-04\n",
      "Epoch 180/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0154 - val_loss: 5.0710e-04\n",
      "Epoch 181/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0154 - val_loss: 4.9435e-04\n",
      "Epoch 182/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0154 - val_loss: 4.7176e-04\n",
      "Epoch 183/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0154 - val_loss: 5.0652e-04\n",
      "Epoch 184/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0154 - val_loss: 4.8372e-04\n",
      "Epoch 185/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0154 - val_loss: 5.0367e-04\n",
      "Epoch 186/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0154 - val_loss: 4.9029e-04\n",
      "Epoch 187/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0154 - val_loss: 4.7930e-04\n",
      "Epoch 188/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0154 - val_loss: 5.1575e-04\n",
      "Epoch 189/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0154 - val_loss: 4.9863e-04\n",
      "Epoch 190/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0154 - val_loss: 4.9454e-04\n",
      "Epoch 191/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0154 - val_loss: 5.0769e-04\n",
      "Epoch 192/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0154 - val_loss: 4.7694e-04\n",
      "Epoch 193/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0154 - val_loss: 4.8275e-04\n",
      "Epoch 194/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0154 - val_loss: 4.9113e-04\n",
      "Epoch 195/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0154 - val_loss: 4.7412e-04\n",
      "Epoch 196/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0154 - val_loss: 4.8175e-04\n",
      "Epoch 197/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0154 - val_loss: 4.9261e-04\n",
      "Epoch 198/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0154 - val_loss: 4.9404e-04\n",
      "Epoch 199/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0154 - val_loss: 5.2408e-04\n",
      "Epoch 200/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0154 - val_loss: 4.9741e-04\n",
      "Epoch 201/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0154 - val_loss: 4.9946e-04\n",
      "Epoch 202/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0154 - val_loss: 4.6732e-04\n",
      "Epoch 203/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0153 - val_loss: 4.8956e-04\n",
      "Epoch 204/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0153 - val_loss: 4.9578e-04\n",
      "Epoch 205/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0154 - val_loss: 4.8572e-04\n",
      "Epoch 206/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0153 - val_loss: 4.9437e-04\n",
      "Epoch 207/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0153 - val_loss: 4.8403e-04\n",
      "Epoch 208/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0153 - val_loss: 4.9185e-04\n",
      "Epoch 209/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0153 - val_loss: 4.8038e-04\n",
      "Epoch 210/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0153 - val_loss: 4.8152e-04\n",
      "Epoch 211/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0153 - val_loss: 5.0448e-04\n",
      "Epoch 212/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0153 - val_loss: 4.9441e-04\n",
      "Epoch 213/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0153 - val_loss: 4.7707e-04\n",
      "Epoch 214/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0153 - val_loss: 4.9335e-04\n",
      "Epoch 215/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0153 - val_loss: 5.1678e-04\n",
      "Epoch 216/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0153 - val_loss: 4.6524e-04\n",
      "Epoch 217/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0153 - val_loss: 4.7296e-04\n",
      "Epoch 218/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0153 - val_loss: 4.6676e-04\n",
      "Epoch 219/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0153 - val_loss: 4.9196e-04\n",
      "Epoch 220/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0153 - val_loss: 4.9572e-04\n",
      "Epoch 221/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0153 - val_loss: 4.9759e-04\n",
      "Epoch 222/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0153 - val_loss: 4.7674e-04\n",
      "Epoch 223/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0153 - val_loss: 4.7020e-04\n",
      "Epoch 224/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0153 - val_loss: 4.6812e-04\n",
      "Epoch 225/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0153 - val_loss: 4.7858e-04\n",
      "Epoch 226/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0153 - val_loss: 4.6951e-04\n",
      "Epoch 227/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0153 - val_loss: 4.9156e-04\n",
      "Epoch 228/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0153 - val_loss: 4.8534e-04\n",
      "Epoch 229/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0153 - val_loss: 4.6191e-04\n",
      "Epoch 230/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0153 - val_loss: 4.8243e-04\n",
      "Epoch 231/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0153 - val_loss: 4.8039e-04\n",
      "Epoch 232/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0153 - val_loss: 4.8374e-04\n",
      "Epoch 233/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0153 - val_loss: 4.7107e-04\n",
      "Epoch 234/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0153 - val_loss: 4.6803e-04\n",
      "Epoch 235/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0153 - val_loss: 4.8510e-04\n",
      "Epoch 236/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0153 - val_loss: 4.8555e-04\n",
      "Epoch 237/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0153 - val_loss: 4.8364e-04\n",
      "Epoch 238/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0153 - val_loss: 4.6522e-04\n",
      "Epoch 239/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0153 - val_loss: 4.7313e-04\n",
      "Epoch 240/300\n",
      "8750/8750 [==============================] - 12s 1ms/sample - loss: 0.0153 - val_loss: 4.9180e-04\n",
      "Epoch 241/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0153 - val_loss: 4.9215e-04\n",
      "Epoch 242/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0153 - val_loss: 4.9063e-04\n",
      "Epoch 243/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0153 - val_loss: 4.9382e-04\n",
      "Epoch 244/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0152 - val_loss: 4.9562e-04\n",
      "Epoch 245/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0153 - val_loss: 4.7286e-04\n",
      "Epoch 246/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0152 - val_loss: 4.8027e-04\n",
      "Epoch 247/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 4.6249e-04\n",
      "Epoch 248/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 5.0000e-04\n",
      "Epoch 249/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 4.9742e-04\n",
      "Epoch 250/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 4.7736e-04\n",
      "Epoch 251/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 4.7861e-04\n",
      "Epoch 252/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 4.6965e-04\n",
      "Epoch 253/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0152 - val_loss: 4.7742e-04\n",
      "Epoch 254/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 4.9921e-04\n",
      "Epoch 255/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 5.1231e-04\n",
      "Epoch 256/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 4.6701e-04\n",
      "Epoch 257/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0152 - val_loss: 4.8117e-04\n",
      "Epoch 258/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0152 - val_loss: 4.7811e-04\n",
      "Epoch 259/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0152 - val_loss: 4.5210e-04\n",
      "Epoch 260/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0152 - val_loss: 4.6871e-04\n",
      "Epoch 261/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0152 - val_loss: 4.9088e-04\n",
      "Epoch 262/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 4.7454e-04\n",
      "Epoch 263/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 4.7625e-04\n",
      "Epoch 264/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 4.8963e-04\n",
      "Epoch 265/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 4.9952e-04\n",
      "Epoch 266/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 4.4805e-04\n",
      "Epoch 267/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 4.7613e-04\n",
      "Epoch 268/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 4.7609e-04\n",
      "Epoch 269/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 4.9949e-04\n",
      "Epoch 270/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 4.8459e-04\n",
      "Epoch 271/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 4.5816e-04\n",
      "Epoch 272/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 4.8315e-04\n",
      "Epoch 273/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0152 - val_loss: 4.4507e-04\n",
      "Epoch 274/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 4.9545e-04\n",
      "Epoch 275/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 4.8493e-04\n",
      "Epoch 276/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0152 - val_loss: 4.7307e-04\n",
      "Epoch 277/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 4.8108e-04\n",
      "Epoch 278/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0152 - val_loss: 4.6799e-04\n",
      "Epoch 279/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0152 - val_loss: 4.7656e-04\n",
      "Epoch 280/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 5.0781e-04\n",
      "Epoch 281/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 4.9027e-04\n",
      "Epoch 282/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 4.8476e-04\n",
      "Epoch 283/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 4.9571e-04\n",
      "Epoch 284/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0152 - val_loss: 4.6144e-04\n",
      "Epoch 285/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0152 - val_loss: 4.6775e-04\n",
      "Epoch 286/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 4.6676e-04\n",
      "Epoch 287/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0152 - val_loss: 4.8081e-04\n",
      "Epoch 288/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 4.7341e-04\n",
      "Epoch 289/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 4.8411e-04\n",
      "Epoch 290/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 4.6186e-04\n",
      "Epoch 291/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0152 - val_loss: 4.8355e-04\n",
      "Epoch 292/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 4.6169e-04\n",
      "Epoch 293/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 4.7084e-04\n",
      "Epoch 294/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0152 - val_loss: 4.7996e-04\n",
      "Epoch 295/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0152 - val_loss: 4.6797e-04\n",
      "Epoch 296/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0151 - val_loss: 4.7020e-04\n",
      "Epoch 297/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 4.5920e-04\n",
      "Epoch 298/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0152 - val_loss: 5.0286e-04\n",
      "Epoch 299/300\n",
      "8750/8750 [==============================] - 11s 1ms/sample - loss: 0.0151 - val_loss: 4.6974e-04\n",
      "Epoch 300/300\n",
      "8750/8750 [==============================] - 10s 1ms/sample - loss: 0.0151 - val_loss: 4.8177e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1ac4bda650>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(normal.T) + len(abnormal.T)\n",
    "disable_eager_execution()\n",
    "train = np.array(X_train).astype(float)\n",
    "train = train.reshape([N,1,1000])\n",
    "\n",
    "batch_size = 50\n",
    "epochs = 300\n",
    "input_dim = train.shape[-1]\n",
    "timesteps = train.shape[1]\n",
    "\n",
    "model = VAE_LSTM(input_dim=input_dim, latent_dim=100, hidden_dims=[32], timesteps=timesteps, batch_size=batch_size)\n",
    "vae, encoder, generator = model.autoencoder, model.encoder, model.generator\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "vae.fit(train[:floor(N/batch_size)*batch_size],train[:floor(N/batch_size)*batch_size], shuffle=True, epochs=epochs, batch_size=batch_size, validation_data=(train[N-1-batch_size:N-1],train[N-1-batch_size:N-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uYOvjfYcUOYQ"
   },
   "outputs": [],
   "source": [
    "vector_vae = encoder.predict(np.array(train), batch_size = batch_size)\n",
    "pd.DataFrame(vector_vae).to_csv('vector_vae.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 1, 1000)]    0           []                               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 32)           132224      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 100)          3300        ['lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 100)          3300        ['lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (50, 100)            0           ['dense[0][0]',                  \n",
      "                                                                  'dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " repeat_vector (RepeatVector)   (50, 1, 100)         0           ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (50, 1, 32)          17024       ['repeat_vector[0][0]']          \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  (50, 1, 1000)        4132000     ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,287,848\n",
      "Trainable params: 4,287,848\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RBsYehjWg7nZ",
    "outputId": "2ddcfb71-bcc4-46b7-cfd1-a8ddda656a4f"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN \n",
    "\n",
    "ada = ADASYN(random_state=42)\n",
    "vector_vae_balanced, label_new_balanced = ada.fit_resample(vector_vae, y_train)\n",
    "# vector_vae_balanced, label_new_balanced = vector_vae, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "1fz_bO1QUWco"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(vector_vae_balanced).to_csv('track_vae_balanced.csv', index=False)\n",
    "pd.DataFrame(label_new_balanced).to_csv('track_new_balanced.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Gxn1wjSzUZWg"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, n_hidden_1, n_hidden_2, out_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.droprate = 0.95\n",
    "        self.layer1 = nn.Sequential(nn.Linear(in_dim, n_hidden_1), nn.Dropout(p=self.droprate), nn.BatchNorm1d(n_hidden_1), nn.ReLU(True))\n",
    "        self.layer2 = nn.Sequential(nn.Linear(n_hidden_1, n_hidden_2), nn.Dropout(p=self.droprate), nn.BatchNorm1d(n_hidden_2), nn.ReLU(True))\n",
    "        self.layer3 = nn.Sequential(nn.Linear(n_hidden_2, out_dim), nn.Dropout(p=self.droprate))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = torch.from_numpy(pd.read_csv('track_vae_balanced.csv').values).float()\n",
    "        self.labels = torch.from_numpy(pd.read_csv('track_new_balanced.csv').values).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx,:]\n",
    "        labels = self.labels[idx]\n",
    "        return sample, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "9tDOq8a4UcAv"
   },
   "outputs": [],
   "source": [
    "def process(X_train, X_test, y_train, y_test):\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "    Y_train = np.array(y_train)\n",
    "    Y_test = np.array(y_test)\n",
    "    X_train = torch.from_numpy(X_train).float()\n",
    "    X_test = torch.from_numpy(X_test).float()\n",
    "    Y_train = torch.from_numpy(Y_train).squeeze().to(torch.int64)\n",
    "    Y_test = torch.from_numpy(Y_test).squeeze().to(torch.int64)\n",
    "    train_dataset = []\n",
    "    test_dataset = []\n",
    "    for i in range(len(X_train)):\n",
    "        train_dataset.append((X_train[i],Y_train[i]))\n",
    "    for i in range(len(X_test)):\n",
    "        test_dataset.append((X_test[i],Y_test[i]))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "#do not standardize the one hot features\n",
    "def process_sentiment(X_train, X_test, y_train, y_test):\n",
    "    sentiment_train = X_train[:,len(X_train[0])-5:len(X_train[0])]\n",
    "    sentiment_test = X_test[:,len(X_test[0])-5:len(X_test[0])]\n",
    "    X_train = X_train[:,:len(X_train[0])-5]\n",
    "    X_test = X_test[:,:len(X_test[0])-5]\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    X_train = np.hstack((X_train, sentiment_train))\n",
    "    X_test = np.hstack((X_test, sentiment_test))\n",
    "    Y_train = np.array(y_train)\n",
    "    Y_test = np.array(y_test)\n",
    "    X_train = torch.from_numpy(X_train).float()\n",
    "    X_test = torch.from_numpy(X_test).float()\n",
    "    Y_train = torch.from_numpy(Y_train).squeeze().to(torch.int64)\n",
    "    Y_test = torch.from_numpy(Y_test).squeeze().to(torch.int64)\n",
    "    train_dataset = []\n",
    "    test_dataset = []\n",
    "    for i in range(len(X_train)):\n",
    "        train_dataset.append((X_train[i],Y_train[i]))\n",
    "    for i in range(len(X_test)):\n",
    "        test_dataset.append((X_test[i],Y_test[i]))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def classifier(input_dim, train_loader, test_loader, totEpoch, num_class, len_test):\n",
    "    model = Net(input_dim, 100, 30, 4)\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    epoch = 0\n",
    "    train_loss_list = []\n",
    "    loss_list = []\n",
    "    accuracy_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    F1_list = []\n",
    "    F1_list_train = []\n",
    "    TPR0_list=[]\n",
    "    FPR0_list=[]\n",
    "    y_pred_list=[]\n",
    "    lable_list=[]\n",
    "\n",
    "    for epoch2 in range(0, 0 + totEpoch):\n",
    "      # model.eval()\n",
    "        num_TP = [0]*num_class\n",
    "        num_FP = [0]*num_class\n",
    "        num_FN = [0]*num_class\n",
    "        eval_loss = 0\n",
    "        eval_acc = 0\n",
    "        eval_TP = [0]*num_class\n",
    "        eval_FP = [0]*num_class\n",
    "        eval_FN = [0]*num_class\n",
    "        precision = [0]*num_class\n",
    "        recall = [0]*num_class\n",
    "        F1 = [0]*num_class\n",
    "        for data in train_loader:\n",
    "            img, label = data\n",
    "            if torch.cuda.is_available():\n",
    "                img = img.cuda()\n",
    "                label = label.cuda()\n",
    "            else:\n",
    "                img = Variable(img)\n",
    "                label = Variable(label)\n",
    "            out = model(img)\n",
    "            loss = criterion(out, label)\n",
    "            print_loss = loss.data.item()\n",
    "            _, pred = torch.max(out, 1)\n",
    "            num_correct = (pred == label).sum()\n",
    "\n",
    "            for i in range(num_class):\n",
    "                num_TP[i] = (((pred == i) & (label == i))).sum()\n",
    "                num_FP[i] = (((pred == i) & (label != i))).sum()\n",
    "                num_FN[i] = (((pred != i) & (label == i))).sum()\n",
    "                eval_TP[i] += num_TP[i].item()\n",
    "                eval_FP[i] += num_FP[i].item()\n",
    "                eval_FN[i] += num_FN[i].item()\n",
    "            eval_acc += num_correct.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch+=1\n",
    "\n",
    "        for i in range(num_class):\n",
    "            if (eval_TP[i] + eval_FP[i]) == 0:\n",
    "                precision[i] = 1\n",
    "            else:\n",
    "                precision[i] = eval_TP[i] / (eval_TP[i] + eval_FP[i])\n",
    "            if (eval_TP[i] + eval_FN[i]) == 0:\n",
    "                recall[i] = 1\n",
    "            else:\n",
    "                recall[i] = eval_TP[i] / (eval_TP[i] + eval_FN[i])\n",
    "            if (precision[i]+recall[i]) == 0:\n",
    "                F1[i] = 0\n",
    "            else:\n",
    "                F1[i] = 2*precision[i]*recall[i]/(precision[i]+recall[i])\n",
    "            print('Train F1: {:.6f}'.format(\n",
    "          sum(F1)/num_class,\n",
    "      ))\n",
    "            F1_list_train.append(sum(F1)/num_class)\n",
    "\n",
    "        model.eval()\n",
    "        print('epoch: {}, loss: {:.4}'.format(epoch2, loss.data.item()))\n",
    "        train_loss_list.append(loss.data.item())\n",
    "\n",
    "      # model.eval()\n",
    "        num_TP = [0]*num_class\n",
    "        num_FP = [0]*num_class\n",
    "        num_FN = [0]*num_class\n",
    "        num_TN = [0]*num_class\n",
    "        eval_loss = 0\n",
    "        eval_acc = 0\n",
    "        eval_TP = [0]*num_class\n",
    "        eval_FP = [0]*num_class\n",
    "        eval_FN = [0]*num_class\n",
    "        eval_TN  = [0]*num_class\n",
    "        precision = [0]*num_class\n",
    "        recall = [0]*num_class\n",
    "        F1 = [0]*num_class\n",
    "        \n",
    "        \n",
    "        # here add roc\n",
    "        FPR = [0]*num_class\n",
    "        TPR = [0]*num_class\n",
    "        y_pred = [0]*num_class\n",
    "        \n",
    "        \n",
    "        \n",
    "        for data in test_loader:\n",
    "            img, label = data\n",
    "          # img = img.view(img.size(0), -1)\n",
    "            if torch.cuda.is_available():\n",
    "                img = img.cuda()\n",
    "                label = label.cuda()\n",
    "\n",
    "            out = model(img)\n",
    "            loss = criterion(out, label)\n",
    "            eval_loss += loss.data.item()*label.size(0)\n",
    "            _, pred = torch.max(out, 1)\n",
    "            num_correct = (pred == label).sum()\n",
    "            y_pred = pred.cpu().numpy()\n",
    "            lable_pred = label.cpu().numpy()\n",
    "            if epoch2 == totEpoch - 1:\n",
    "                y_pred_list.append(y_pred)\n",
    "                lable_list.append(lable_pred)\n",
    "            for i in range(num_class):\n",
    "                num_TP[i] = (((pred == i) & (label == i))).sum()\n",
    "                num_FP[i] = (((pred == i) & (label != i))).sum()\n",
    "                num_FN[i] = (((pred != i) & (label == i))).sum()\n",
    "                num_TN[i] = (((pred != i) & (label != i))).sum()\n",
    "                eval_TP[i] += num_TP[i].item()\n",
    "                eval_FP[i] += num_FP[i].item()\n",
    "                eval_FN[i] += num_FN[i].item()\n",
    "                eval_TN[i] += num_TN[i].item()\n",
    "            eval_acc += num_correct.item()\n",
    "        for i in range(num_class):\n",
    "            if (eval_TP[i] + eval_FP[i]) == 0:\n",
    "                precision[i] = 1\n",
    "            elif (eval_FP[i] + eval_TN[i]) == 0:\n",
    "                FPR[i] = 0\n",
    "            elif (eval_TP[i] + eval_FN[i]) == 0:\n",
    "                TPR[i] = 0\n",
    "            else:\n",
    "                FPR[i] = eval_FP[i] / (eval_FP[i] + eval_TN[i])\n",
    "                TPR[i] = eval_TP[i] / (eval_TP[i] + eval_FN[i])\n",
    "                precision[i] = eval_TP[i] / (eval_TP[i] + eval_FP[i])\n",
    "            if (eval_TP[i] + eval_FN[i]) == 0:\n",
    "                recall[i] = 1\n",
    "            else:\n",
    "                recall[i] = eval_TP[i] / (eval_TP[i] + eval_FN[i])\n",
    "            if (precision[i]+recall[i]) == 0:\n",
    "                F1[i] = 0\n",
    "            else:\n",
    "                F1[i] = 2*precision[i]*recall[i]/(precision[i]+recall[i])\n",
    "        print('Test Loss: {:.6f}, Acc: {:.6f}, Pre: {:.6f}, Rec: {:.6f}, F1: {:.6f}'.format(\n",
    "          eval_loss / (len_test),\n",
    "          eval_acc / (len_test),\n",
    "          sum(precision)/num_class,\n",
    "          sum(recall)/num_class,\n",
    "          sum(F1)/num_class\n",
    "      ))\n",
    "        loss_list.append(loss.data.item())\n",
    "        accuracy_list.append(eval_acc / (len_test))\n",
    "        precision_list.append(sum(precision)/num_class)\n",
    "        recall_list.append(sum(recall)/num_class)\n",
    "        F1_list.append(sum(F1)/num_class)\n",
    "        TPR0_list.append(TPR[1])\n",
    "        FPR0_list.append(FPR[1])\n",
    "    return [np.array(accuracy_list), np.array(precision_list), np.array(recall_list), np.array(F1_list), np.array(F1_list_train), np.array(loss_list), np.array(train_loss_list)], TPR0_list, FPR0_list, y_pred_list, lable_list\n",
    "\n",
    "def train_model(vector_vae, label, model, totEpoch, num_class, random_s):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(vector_vae, label, test_size=0.2, random_state = random_s)\n",
    "    train_loader, test_loader = process_sentiment(X_train, X_test, y_train, y_test)\n",
    "    results= classifier(model, train_loader, test_loader, totEpoch, num_class, len(y_test))\n",
    "    return results\n",
    "\n",
    "def train_cross_val(input_dim, vector_vae, label, totEpoch, num_class, k):\n",
    "    results = [np.array([0.0]*totEpoch),np.array([0.0]*totEpoch),np.array([0.0]*totEpoch),np.array([0.0]*totEpoch),np.array([0.0]*3*totEpoch),np.array([0.0]*totEpoch),np.array([0.0]*totEpoch)]\n",
    "    kf = StratifiedKFold(n_splits=k)\n",
    "    c = 0\n",
    "    var_test, var_train = [], []\n",
    "    F1_train, F1_test = [], []\n",
    "    Loss_train, Loss_test = [], []\n",
    "    for train_index, test_index in kf.split(vector_vae, label):\n",
    "        print('The ', c, ' th fold cross validation:')\n",
    "        X_train = vector_vae[train_index]\n",
    "        y_train = label[train_index]\n",
    "        X_test = vector_vae[test_index]\n",
    "        y_test = label[test_index]\n",
    "        train_loader, test_loader = process_sentiment(X_train, X_test, y_train, y_test)\n",
    "        result_list, TPR, FPR, y_pred, lable_list= classifier(input_dim, train_loader, test_loader, totEpoch, num_class, len(y_test))\n",
    "        var_test.append(result_list[3])\n",
    "        var_train.append(result_list[4])\n",
    "        # print((var_train))\n",
    "        F1_train.append(result_list[4])\n",
    "        # print((F1_train))\n",
    "        F1_test.append(result_list[3])\n",
    "        Loss_train.append(result_list[6])\n",
    "        Loss_test.append(result_list[5])\n",
    "        for i in range(7):\n",
    "            # print(i)\n",
    "            results[i] += result_list[i]\n",
    "        c += 1\n",
    "    for i in range(7):\n",
    "        results[i] /= k\n",
    "    var_test = np.array(var_test)\n",
    "    var_train = np.array(var_train)\n",
    "    results.append(np.var(var_train, axis = 0))\n",
    "    results.append(np.var(var_test, axis = 0))\n",
    "    return results, F1_train, F1_test, Loss_train, Loss_test, TPR, FPR, y_pred, lable_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GN7G8skgFUR8",
    "outputId": "0fa55c5a-456d-4daf-bd8f-22c45fe58fed",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  0  th fold cross validation:\n",
      "Train F1: 0.216642\n",
      "Train F1: 0.246627\n",
      "Train F1: 0.246627\n",
      "epoch: 0, loss: 1.366\n",
      "Test Loss: 1.260628, Acc: 0.551828, Pre: 0.735265, Rec: 0.697946, F1: 0.652180\n",
      "Train F1: 0.217904\n",
      "Train F1: 0.444000\n",
      "Train F1: 0.777333\n",
      "epoch: 1, loss: 0.7849\n",
      "Test Loss: 0.724382, Acc: 0.578495, Pre: 0.765387, Rec: 0.715857, F1: 0.676926\n",
      "Train F1: 0.195335\n",
      "Train F1: 0.428903\n",
      "Train F1: 0.762236\n",
      "epoch: 2, loss: 0.5815\n",
      "Test Loss: 0.521637, Acc: 0.783656, Pre: 0.868322, Rec: 0.856865, F1: 0.853916\n",
      "Train F1: 0.256028\n",
      "Train F1: 0.486788\n",
      "Train F1: 0.820121\n",
      "epoch: 3, loss: 0.4994\n",
      "Test Loss: 0.462415, Acc: 0.800430, Pre: 0.887680, Rec: 0.868306, F1: 0.864235\n",
      "Train F1: 0.259568\n",
      "Train F1: 0.493596\n",
      "Train F1: 0.826929\n",
      "epoch: 4, loss: 0.4348\n",
      "Test Loss: 0.447418, Acc: 0.805591, Pre: 0.887097, Rec: 0.871610, F1: 0.868303\n",
      "Train F1: 0.262181\n",
      "Train F1: 0.499069\n",
      "Train F1: 0.832403\n",
      "epoch: 5, loss: 0.4907\n",
      "Test Loss: 0.432664, Acc: 0.812903, Pre: 0.894365, Rec: 0.876551, F1: 0.873011\n",
      "Train F1: 0.264138\n",
      "Train F1: 0.505759\n",
      "Train F1: 0.839092\n",
      "epoch: 6, loss: 0.4875\n",
      "Test Loss: 0.428940, Acc: 0.798710, Pre: 0.872307, Rec: 0.866586, F1: 0.865006\n",
      "Train F1: 0.265553\n",
      "Train F1: 0.510301\n",
      "Train F1: 0.843634\n",
      "epoch: 7, loss: 0.4413\n",
      "Test Loss: 0.420940, Acc: 0.812043, Pre: 0.895511, Rec: 0.876031, F1: 0.872214\n",
      "Train F1: 0.267506\n",
      "Train F1: 0.515686\n",
      "Train F1: 0.849020\n",
      "epoch: 8, loss: 0.5236\n",
      "Test Loss: 0.414251, Acc: 0.803871, Pre: 0.883925, Rec: 0.870394, F1: 0.867392\n",
      "Train F1: 0.268137\n",
      "Train F1: 0.518270\n",
      "Train F1: 0.851603\n",
      "epoch: 9, loss: 0.5061\n",
      "Test Loss: 0.410289, Acc: 0.806452, Pre: 0.886611, Rec: 0.872145, F1: 0.869027\n",
      "Train F1: 0.270046\n",
      "Train F1: 0.523474\n",
      "Train F1: 0.856807\n",
      "epoch: 10, loss: 0.3832\n",
      "Test Loss: 0.404562, Acc: 0.809892, Pre: 0.881846, Rec: 0.874142, F1: 0.872269\n",
      "Train F1: 0.270898\n",
      "Train F1: 0.526085\n",
      "Train F1: 0.859418\n",
      "epoch: 11, loss: 0.3942\n",
      "Test Loss: 0.406465, Acc: 0.808172, Pre: 0.882601, Rec: 0.873086, F1: 0.870862\n",
      "Train F1: 0.271768\n",
      "Train F1: 0.528679\n",
      "Train F1: 0.862012\n",
      "epoch: 12, loss: 0.3965\n",
      "Test Loss: 0.406122, Acc: 0.803441, Pre: 0.871664, Rec: 0.869461, F1: 0.868688\n",
      "Train F1: 0.272983\n",
      "Train F1: 0.533045\n",
      "Train F1: 0.866378\n",
      "epoch: 13, loss: 0.3943\n",
      "Test Loss: 0.407269, Acc: 0.803011, Pre: 0.875126, Rec: 0.869446, F1: 0.867908\n",
      "Train F1: 0.275008\n",
      "Train F1: 0.537004\n",
      "Train F1: 0.870337\n",
      "epoch: 14, loss: 0.3506\n",
      "Test Loss: 0.416868, Acc: 0.793978, Pre: 0.862971, Rec: 0.862823, F1: 0.862642\n",
      "Train F1: 0.275980\n",
      "Train F1: 0.540582\n",
      "Train F1: 0.873915\n",
      "epoch: 15, loss: 0.3715\n",
      "Test Loss: 0.404135, Acc: 0.806452, Pre: 0.874541, Rec: 0.871541, F1: 0.870594\n",
      "Train F1: 0.277855\n",
      "Train F1: 0.545134\n",
      "Train F1: 0.878467\n",
      "epoch: 16, loss: 0.3875\n",
      "Test Loss: 0.414855, Acc: 0.809032, Pre: 0.874489, Rec: 0.873094, F1: 0.872533\n",
      "Train F1: 0.278059\n",
      "Train F1: 0.546164\n",
      "Train F1: 0.879498\n",
      "epoch: 17, loss: 0.4516\n",
      "Test Loss: 0.416110, Acc: 0.806452, Pre: 0.871025, Rec: 0.871044, F1: 0.870967\n",
      "Train F1: 0.279282\n",
      "Train F1: 0.549598\n",
      "Train F1: 0.882932\n",
      "epoch: 18, loss: 0.3735\n",
      "Test Loss: 0.399357, Acc: 0.806882, Pre: 0.874506, Rec: 0.871801, F1: 0.870922\n",
      "Train F1: 0.280786\n",
      "Train F1: 0.552842\n",
      "Train F1: 0.886176\n",
      "epoch: 19, loss: 0.3541\n",
      "Test Loss: 0.415177, Acc: 0.811613, Pre: 0.876023, Rec: 0.874792, F1: 0.874278\n",
      "Train F1: 0.280184\n",
      "Train F1: 0.552297\n",
      "Train F1: 0.885630\n",
      "epoch: 20, loss: 0.4674\n",
      "Test Loss: 0.422797, Acc: 0.803871, Pre: 0.870533, Rec: 0.869591, F1: 0.869145\n",
      "Train F1: 0.280975\n",
      "Train F1: 0.554854\n",
      "Train F1: 0.888188\n",
      "epoch: 21, loss: 0.4151\n",
      "Test Loss: 0.403584, Acc: 0.809892, Pre: 0.874806, Rec: 0.873637, F1: 0.873137\n",
      "Train F1: 0.282566\n",
      "Train F1: 0.557826\n",
      "Train F1: 0.891159\n",
      "epoch: 22, loss: 0.4205\n",
      "Test Loss: 0.421282, Acc: 0.807312, Pre: 0.871797, Rec: 0.871365, F1: 0.871421\n",
      "Train F1: 0.283503\n",
      "Train F1: 0.560008\n",
      "Train F1: 0.893342\n",
      "epoch: 23, loss: 0.2971\n",
      "Test Loss: 0.439488, Acc: 0.793118, Pre: 0.863322, Rec: 0.861706, F1: 0.861698\n",
      "Train F1: 0.284311\n",
      "Train F1: 0.562552\n",
      "Train F1: 0.895885\n",
      "epoch: 24, loss: 0.2891\n",
      "Test Loss: 0.372912, Acc: 0.824086, Pre: 0.888201, Rec: 0.883418, F1: 0.882194\n",
      "Train F1: 0.285782\n",
      "Train F1: 0.565480\n",
      "Train F1: 0.898813\n",
      "epoch: 25, loss: 0.2596\n",
      "Test Loss: 0.431073, Acc: 0.818495, Pre: 0.880511, Rec: 0.879365, F1: 0.878883\n",
      "Train F1: 0.285909\n",
      "Train F1: 0.566655\n",
      "Train F1: 0.899988\n",
      "epoch: 26, loss: 0.3813\n",
      "Test Loss: 0.429779, Acc: 0.817634, Pre: 0.882941, Rec: 0.879059, F1: 0.877974\n",
      "Train F1: 0.286142\n",
      "Train F1: 0.566920\n",
      "Train F1: 0.900253\n",
      "epoch: 27, loss: 0.3192\n",
      "Test Loss: 0.425389, Acc: 0.824516, Pre: 0.886467, Rec: 0.883564, F1: 0.882703\n",
      "Train F1: 0.287938\n",
      "Train F1: 0.571052\n",
      "Train F1: 0.904385\n",
      "epoch: 28, loss: 0.2592\n",
      "Test Loss: 0.402810, Acc: 0.819785, Pre: 0.881283, Rec: 0.880214, F1: 0.879754\n",
      "Train F1: 0.289529\n",
      "Train F1: 0.574195\n",
      "Train F1: 0.907528\n",
      "epoch: 29, loss: 0.295\n",
      "Test Loss: 0.460241, Acc: 0.803441, Pre: 0.868941, Rec: 0.868926, F1: 0.868933\n",
      "Train F1: 0.287787\n",
      "Train F1: 0.571006\n",
      "Train F1: 0.904339\n",
      "epoch: 30, loss: 0.4065\n",
      "Test Loss: 0.437720, Acc: 0.821505, Pre: 0.885404, Rec: 0.881629, F1: 0.880583\n",
      "Train F1: 0.290719\n",
      "Train F1: 0.576951\n",
      "Train F1: 0.910284\n",
      "epoch: 31, loss: 0.3166\n",
      "Test Loss: 0.410643, Acc: 0.821075, Pre: 0.880695, Rec: 0.880719, F1: 0.880703\n",
      "Train F1: 0.291001\n",
      "Train F1: 0.577524\n",
      "Train F1: 0.910858\n",
      "epoch: 32, loss: 0.3567\n",
      "Test Loss: 0.436903, Acc: 0.805161, Pre: 0.871851, Rec: 0.869683, F1: 0.869662\n",
      "Train F1: 0.291590\n",
      "Train F1: 0.579140\n",
      "Train F1: 0.912474\n",
      "epoch: 33, loss: 0.2805\n",
      "Test Loss: 0.425699, Acc: 0.825376, Pre: 0.885679, Rec: 0.884015, F1: 0.883421\n",
      "Train F1: 0.291013\n",
      "Train F1: 0.578310\n",
      "Train F1: 0.911643\n",
      "epoch: 34, loss: 0.2716\n",
      "Test Loss: 0.430479, Acc: 0.818065, Pre: 0.880130, Rec: 0.879067, F1: 0.878606\n",
      "Train F1: 0.292666\n",
      "Train F1: 0.581914\n",
      "Train F1: 0.915247\n",
      "epoch: 35, loss: 0.2981\n",
      "Test Loss: 0.438969, Acc: 0.824946, Pre: 0.887104, Rec: 0.883877, F1: 0.882954\n",
      "Train F1: 0.292434\n",
      "Train F1: 0.581487\n",
      "Train F1: 0.914820\n",
      "epoch: 36, loss: 0.279\n",
      "Test Loss: 0.417989, Acc: 0.822796, Pre: 0.881944, Rec: 0.881759, F1: 0.881802\n",
      "Train F1: 0.293317\n",
      "Train F1: 0.583045\n",
      "Train F1: 0.916378\n",
      "epoch: 37, loss: 0.2286\n",
      "Test Loss: 0.464856, Acc: 0.818925, Pre: 0.879281, Rec: 0.879319, F1: 0.879277\n",
      "Train F1: 0.292367\n",
      "Train F1: 0.581653\n",
      "Train F1: 0.914986\n",
      "epoch: 38, loss: 0.3335\n",
      "Test Loss: 0.460747, Acc: 0.816774, Pre: 0.877839, Rec: 0.877805, F1: 0.877819\n",
      "Train F1: 0.295278\n",
      "Train F1: 0.587296\n",
      "Train F1: 0.920629\n",
      "epoch: 39, loss: 0.3323\n",
      "Test Loss: 0.436372, Acc: 0.818925, Pre: 0.879492, Rec: 0.879128, F1: 0.879186\n",
      "Train F1: 0.295850\n",
      "Train F1: 0.588907\n",
      "Train F1: 0.922240\n",
      "epoch: 40, loss: 0.2291\n",
      "Test Loss: 0.426054, Acc: 0.817204, Pre: 0.878412, Rec: 0.878294, F1: 0.878130\n",
      "Train F1: 0.297671\n",
      "Train F1: 0.592560\n",
      "Train F1: 0.925893\n",
      "epoch: 41, loss: 0.3598\n",
      "Test Loss: 0.443433, Acc: 0.809892, Pre: 0.875142, Rec: 0.872826, F1: 0.872807\n",
      "Train F1: 0.296050\n",
      "Train F1: 0.589262\n",
      "Train F1: 0.922595\n",
      "epoch: 42, loss: 0.2642\n",
      "Test Loss: 0.488157, Acc: 0.811613, Pre: 0.874386, Rec: 0.874409, F1: 0.874394\n",
      "Train F1: 0.298951\n",
      "Train F1: 0.595371\n",
      "Train F1: 0.928704\n",
      "epoch: 43, loss: 0.253\n",
      "Test Loss: 0.460575, Acc: 0.799570, Pre: 0.868993, Rec: 0.865859, F1: 0.865736\n",
      "Train F1: 0.296366\n",
      "Train F1: 0.590597\n",
      "Train F1: 0.923930\n",
      "epoch: 44, loss: 0.2226\n",
      "Test Loss: 0.502756, Acc: 0.810753, Pre: 0.874589, Rec: 0.873553, F1: 0.873606\n",
      "Train F1: 0.297226\n",
      "Train F1: 0.592316\n",
      "Train F1: 0.925649\n",
      "epoch: 45, loss: 0.3626\n",
      "Test Loss: 0.397394, Acc: 0.828387, Pre: 0.885771, Rec: 0.885720, F1: 0.885590\n",
      "Train F1: 0.299524\n",
      "Train F1: 0.596824\n",
      "Train F1: 0.930158\n",
      "epoch: 46, loss: 0.3475\n",
      "Test Loss: 0.455504, Acc: 0.820645, Pre: 0.880553, Rec: 0.880306, F1: 0.880356\n",
      "Train F1: 0.300098\n",
      "Train F1: 0.598351\n",
      "Train F1: 0.931684\n",
      "epoch: 47, loss: 0.2419\n",
      "Test Loss: 0.461079, Acc: 0.836129, Pre: 0.891569, Rec: 0.891020, F1: 0.890714\n",
      "Train F1: 0.299767\n",
      "Train F1: 0.597403\n",
      "Train F1: 0.930737\n",
      "epoch: 48, loss: 0.2017\n",
      "Test Loss: 0.417199, Acc: 0.829247, Pre: 0.886570, Rec: 0.886355, F1: 0.886153\n",
      "Train F1: 0.300576\n",
      "Train F1: 0.599281\n",
      "Train F1: 0.932615\n",
      "epoch: 49, loss: 0.1691\n",
      "Test Loss: 0.535431, Acc: 0.792688, Pre: 0.868264, Rec: 0.860980, F1: 0.860341\n",
      "Train F1: 0.301122\n",
      "Train F1: 0.600355\n",
      "Train F1: 0.933688\n",
      "epoch: 50, loss: 0.2475\n",
      "Test Loss: 0.443374, Acc: 0.820645, Pre: 0.880810, Rec: 0.880229, F1: 0.880295\n",
      "Train F1: 0.301371\n",
      "Train F1: 0.601084\n",
      "Train F1: 0.934417\n",
      "epoch: 51, loss: 0.2152\n",
      "Test Loss: 0.482817, Acc: 0.814624, Pre: 0.877034, Rec: 0.876161, F1: 0.876222\n",
      "Train F1: 0.301874\n",
      "Train F1: 0.602087\n",
      "Train F1: 0.935420\n",
      "epoch: 52, loss: 0.3092\n",
      "Test Loss: 0.476866, Acc: 0.812903, Pre: 0.879291, Rec: 0.874646, F1: 0.874448\n",
      "Train F1: 0.301788\n",
      "Train F1: 0.602096\n",
      "Train F1: 0.935429\n",
      "epoch: 53, loss: 0.1864\n",
      "Test Loss: 0.438321, Acc: 0.840860, Pre: 0.894078, Rec: 0.894033, F1: 0.893906\n",
      "Train F1: 0.302846\n",
      "Train F1: 0.604172\n",
      "Train F1: 0.937506\n",
      "epoch: 54, loss: 0.2845\n",
      "Test Loss: 0.487417, Acc: 0.830968, Pre: 0.889039, Rec: 0.887701, F1: 0.887191\n",
      "Train F1: 0.300863\n",
      "Train F1: 0.600232\n",
      "Train F1: 0.933565\n",
      "epoch: 55, loss: 0.1186\n",
      "Test Loss: 0.446213, Acc: 0.839140, Pre: 0.893978, Rec: 0.893085, F1: 0.892691\n",
      "Train F1: 0.303310\n",
      "Train F1: 0.605104\n",
      "Train F1: 0.938437\n",
      "epoch: 56, loss: 0.1633\n",
      "Test Loss: 0.445368, Acc: 0.832258, Pre: 0.888495, Rec: 0.887992, F1: 0.888062\n",
      "Train F1: 0.304562\n",
      "Train F1: 0.607613\n",
      "Train F1: 0.940946\n",
      "epoch: 57, loss: 0.266\n",
      "Test Loss: 0.480228, Acc: 0.815054, Pre: 0.879149, Rec: 0.876214, F1: 0.876166\n",
      "Train F1: 0.305312\n",
      "Train F1: 0.609493\n",
      "Train F1: 0.942827\n",
      "epoch: 58, loss: 0.3196\n",
      "Test Loss: 0.461688, Acc: 0.841720, Pre: 0.896359, Rec: 0.894882, F1: 0.894358\n",
      "Train F1: 0.303578\n",
      "Train F1: 0.605755\n",
      "Train F1: 0.939088\n",
      "epoch: 59, loss: 0.1921\n",
      "Test Loss: 0.426350, Acc: 0.843871, Pre: 0.895922, Rec: 0.895861, F1: 0.895883\n",
      "Train F1: 0.304619\n",
      "Train F1: 0.608207\n",
      "Train F1: 0.941540\n",
      "epoch: 60, loss: 0.2047\n",
      "Test Loss: 0.542504, Acc: 0.830108, Pre: 0.887742, Rec: 0.887036, F1: 0.886682\n",
      "Train F1: 0.306738\n",
      "Train F1: 0.612211\n",
      "Train F1: 0.945544\n",
      "epoch: 61, loss: 0.2376\n",
      "Test Loss: 0.492663, Acc: 0.841720, Pre: 0.894459, Rec: 0.894469, F1: 0.894464\n",
      "Train F1: 0.307269\n",
      "Train F1: 0.613660\n",
      "Train F1: 0.946993\n",
      "epoch: 62, loss: 0.2501\n",
      "Test Loss: 0.507308, Acc: 0.829247, Pre: 0.886412, Rec: 0.886003, F1: 0.886067\n",
      "Train F1: 0.309142\n",
      "Train F1: 0.617313\n",
      "Train F1: 0.950646\n",
      "epoch: 63, loss: 0.1918\n",
      "Test Loss: 0.469271, Acc: 0.830108, Pre: 0.888385, Rec: 0.886347, F1: 0.886394\n",
      "Train F1: 0.309294\n",
      "Train F1: 0.617969\n",
      "Train F1: 0.951303\n",
      "epoch: 64, loss: 0.2194\n",
      "Test Loss: 0.514149, Acc: 0.842151, Pre: 0.897665, Rec: 0.895265, F1: 0.894556\n",
      "Train F1: 0.308245\n",
      "Train F1: 0.615595\n",
      "Train F1: 0.948928\n",
      "epoch: 65, loss: 0.2567\n",
      "Test Loss: 0.445012, Acc: 0.842151, Pre: 0.896159, Rec: 0.894416, F1: 0.894495\n",
      "Train F1: 0.309486\n",
      "Train F1: 0.618253\n",
      "Train F1: 0.951587\n",
      "epoch: 66, loss: 0.1636\n",
      "Test Loss: 0.524682, Acc: 0.852473, Pre: 0.903255, Rec: 0.902017, F1: 0.901560\n",
      "Train F1: 0.309176\n",
      "Train F1: 0.617607\n",
      "Train F1: 0.950941\n",
      "epoch: 67, loss: 0.1659\n",
      "Test Loss: 0.528006, Acc: 0.842151, Pre: 0.894750, Rec: 0.894744, F1: 0.894747\n",
      "Train F1: 0.310022\n",
      "Train F1: 0.619403\n",
      "Train F1: 0.952736\n",
      "epoch: 68, loss: 0.1662\n",
      "Test Loss: 0.494478, Acc: 0.840000, Pre: 0.894506, Rec: 0.893008, F1: 0.893090\n",
      "Train F1: 0.310601\n",
      "Train F1: 0.620623\n",
      "Train F1: 0.953956\n",
      "epoch: 69, loss: 0.1274\n",
      "Test Loss: 0.478981, Acc: 0.850323, Pre: 0.900194, Rec: 0.900220, F1: 0.900204\n",
      "Train F1: 0.311531\n",
      "Train F1: 0.622487\n",
      "Train F1: 0.955821\n",
      "epoch: 70, loss: 0.1729\n",
      "Test Loss: 0.563506, Acc: 0.840000, Pre: 0.893355, Rec: 0.893268, F1: 0.893297\n",
      "Train F1: 0.311984\n",
      "Train F1: 0.623420\n",
      "Train F1: 0.956753\n",
      "epoch: 71, loss: 0.2761\n",
      "Test Loss: 0.489865, Acc: 0.845161, Pre: 0.896819, Rec: 0.896848, F1: 0.896773\n",
      "Train F1: 0.311741\n",
      "Train F1: 0.622770\n",
      "Train F1: 0.956103\n",
      "epoch: 72, loss: 0.1482\n",
      "Test Loss: 0.514106, Acc: 0.851613, Pre: 0.901195, Rec: 0.901184, F1: 0.901075\n",
      "Train F1: 0.311348\n",
      "Train F1: 0.622202\n",
      "Train F1: 0.955536\n",
      "epoch: 73, loss: 0.1721\n",
      "Test Loss: 0.509956, Acc: 0.833978, Pre: 0.892389, Rec: 0.888795, F1: 0.888772\n",
      "Train F1: 0.311882\n",
      "Train F1: 0.623278\n",
      "Train F1: 0.956611\n",
      "epoch: 74, loss: 0.1701\n",
      "Test Loss: 0.553603, Acc: 0.851183, Pre: 0.900821, Rec: 0.900855, F1: 0.900787\n",
      "Train F1: 0.312744\n",
      "Train F1: 0.625072\n",
      "Train F1: 0.958405\n",
      "epoch: 75, loss: 0.2479\n",
      "Test Loss: 0.535953, Acc: 0.846022, Pre: 0.897444, Rec: 0.897245, F1: 0.897294\n",
      "Train F1: 0.314556\n",
      "Train F1: 0.628729\n",
      "Train F1: 0.962063\n",
      "epoch: 76, loss: 0.2052\n",
      "Test Loss: 0.522022, Acc: 0.841290, Pre: 0.894846, Rec: 0.893949, F1: 0.894035\n",
      "Train F1: 0.315127\n",
      "Train F1: 0.629950\n",
      "Train F1: 0.963283\n",
      "epoch: 77, loss: 0.1456\n",
      "Test Loss: 0.483940, Acc: 0.858065, Pre: 0.905431, Rec: 0.905298, F1: 0.905338\n",
      "Train F1: 0.314752\n",
      "Train F1: 0.629233\n",
      "Train F1: 0.962567\n",
      "epoch: 78, loss: 0.1034\n",
      "Test Loss: 0.492323, Acc: 0.846452, Pre: 0.898030, Rec: 0.897444, F1: 0.897525\n",
      "Train F1: 0.312685\n",
      "Train F1: 0.625147\n",
      "Train F1: 0.958480\n",
      "epoch: 79, loss: 0.07364\n",
      "Test Loss: 0.478855, Acc: 0.846882, Pre: 0.897957, Rec: 0.897849, F1: 0.897883\n",
      "Train F1: 0.314641\n",
      "Train F1: 0.628945\n",
      "Train F1: 0.962279\n",
      "epoch: 80, loss: 0.2576\n",
      "Test Loss: 0.480148, Acc: 0.848172, Pre: 0.899215, Rec: 0.898584, F1: 0.898667\n",
      "Train F1: 0.315136\n",
      "Train F1: 0.630095\n",
      "Train F1: 0.963428\n",
      "epoch: 81, loss: 0.08886\n",
      "Test Loss: 0.650127, Acc: 0.843011, Pre: 0.895548, Rec: 0.895479, F1: 0.895339\n",
      "Train F1: 0.317131\n",
      "Train F1: 0.634038\n",
      "Train F1: 0.967371\n",
      "epoch: 82, loss: 0.1077\n",
      "Test Loss: 0.540949, Acc: 0.829247, Pre: 0.889607, Rec: 0.885605, F1: 0.885538\n",
      "Train F1: 0.314230\n",
      "Train F1: 0.628302\n",
      "Train F1: 0.961636\n",
      "epoch: 83, loss: 0.2088\n",
      "Test Loss: 0.591898, Acc: 0.849032, Pre: 0.901379, Rec: 0.899769, F1: 0.899229\n",
      "Train F1: 0.315340\n",
      "Train F1: 0.630380\n",
      "Train F1: 0.963713\n",
      "epoch: 84, loss: 0.2689\n",
      "Test Loss: 0.557316, Acc: 0.829247, Pre: 0.891232, Rec: 0.885491, F1: 0.885299\n",
      "Train F1: 0.315223\n",
      "Train F1: 0.630238\n",
      "Train F1: 0.963571\n",
      "epoch: 85, loss: 0.2155\n",
      "Test Loss: 0.549512, Acc: 0.840430, Pre: 0.894656, Rec: 0.893314, F1: 0.893398\n",
      "Train F1: 0.315962\n",
      "Train F1: 0.631744\n",
      "Train F1: 0.965077\n",
      "epoch: 86, loss: 0.1018\n",
      "Test Loss: 0.505436, Acc: 0.849892, Pre: 0.900519, Rec: 0.899700, F1: 0.899791\n",
      "Train F1: 0.318166\n",
      "Train F1: 0.636118\n",
      "Train F1: 0.969451\n",
      "epoch: 87, loss: 0.1216\n",
      "Test Loss: 0.537115, Acc: 0.850323, Pre: 0.900960, Rec: 0.899960, F1: 0.900055\n",
      "Train F1: 0.317392\n",
      "Train F1: 0.634612\n",
      "Train F1: 0.967946\n",
      "epoch: 88, loss: 0.08819\n",
      "Test Loss: 0.652662, Acc: 0.849892, Pre: 0.900674, Rec: 0.900182, F1: 0.899899\n",
      "Train F1: 0.318133\n",
      "Train F1: 0.636118\n",
      "Train F1: 0.969452\n",
      "epoch: 89, loss: 0.1309\n",
      "Test Loss: 0.554664, Acc: 0.836559, Pre: 0.893521, Rec: 0.890569, F1: 0.890590\n",
      "Train F1: 0.320739\n",
      "Train F1: 0.641425\n",
      "Train F1: 0.974759\n",
      "epoch: 90, loss: 0.1432\n",
      "Test Loss: 0.575181, Acc: 0.838280, Pre: 0.893589, Rec: 0.891831, F1: 0.891903\n",
      "Train F1: 0.319963\n",
      "Train F1: 0.639848\n",
      "Train F1: 0.973181\n",
      "epoch: 91, loss: 0.1668\n",
      "Test Loss: 0.593002, Acc: 0.850323, Pre: 0.900195, Rec: 0.900205, F1: 0.900200\n",
      "Train F1: 0.320595\n",
      "Train F1: 0.641139\n",
      "Train F1: 0.974472\n",
      "epoch: 92, loss: 0.1129\n",
      "Test Loss: 0.577612, Acc: 0.843011, Pre: 0.896230, Rec: 0.895058, F1: 0.895147\n",
      "Train F1: 0.321772\n",
      "Train F1: 0.643505\n",
      "Train F1: 0.976838\n",
      "epoch: 93, loss: 0.1464\n",
      "Test Loss: 0.615290, Acc: 0.829247, Pre: 0.887806, Rec: 0.885774, F1: 0.885819\n",
      "Train F1: 0.318676\n",
      "Train F1: 0.637266\n",
      "Train F1: 0.970600\n",
      "epoch: 94, loss: 0.2431\n",
      "Test Loss: 0.557062, Acc: 0.852473, Pre: 0.902127, Rec: 0.901444, F1: 0.901532\n",
      "Train F1: 0.319734\n",
      "Train F1: 0.639418\n",
      "Train F1: 0.972751\n",
      "epoch: 95, loss: 0.1884\n",
      "Test Loss: 0.557638, Acc: 0.851613, Pre: 0.901325, Rec: 0.900924, F1: 0.900995\n",
      "Train F1: 0.320913\n",
      "Train F1: 0.641784\n",
      "Train F1: 0.975117\n",
      "epoch: 96, loss: 0.1228\n",
      "Test Loss: 0.682060, Acc: 0.845591, Pre: 0.897054, Rec: 0.897024, F1: 0.897036\n",
      "Train F1: 0.318180\n",
      "Train F1: 0.636334\n",
      "Train F1: 0.969668\n",
      "epoch: 97, loss: 0.0999\n",
      "Test Loss: 0.588744, Acc: 0.843441, Pre: 0.896105, Rec: 0.895417, F1: 0.895500\n",
      "Train F1: 0.316004\n",
      "Train F1: 0.631960\n",
      "Train F1: 0.965293\n",
      "epoch: 98, loss: 0.1433\n",
      "Test Loss: 0.637720, Acc: 0.860645, Pre: 0.907544, Rec: 0.907294, F1: 0.907086\n",
      "Train F1: 0.317385\n",
      "Train F1: 0.634757\n",
      "Train F1: 0.968090\n",
      "epoch: 99, loss: 0.0884\n",
      "Test Loss: 0.630356, Acc: 0.855914, Pre: 0.903927, Rec: 0.903922, F1: 0.903924\n",
      "The  1  th fold cross validation:\n",
      "Train F1: 0.216104\n",
      "Train F1: 0.247685\n",
      "Train F1: 0.247685\n",
      "epoch: 0, loss: 1.536\n",
      "Test Loss: 1.251085, Acc: 0.499785, Pre: 0.167895, Rec: 0.662139, F1: 0.222286\n",
      "Train F1: 0.169277\n",
      "Train F1: 0.399415\n",
      "Train F1: 0.399415\n",
      "epoch: 1, loss: 0.7056\n",
      "Test Loss: 0.737305, Acc: 0.587527, Pre: 0.729360, Rec: 0.726051, F1: 0.721749\n",
      "Train F1: 0.262963\n",
      "Train F1: 0.508313\n",
      "Train F1: 0.841647\n",
      "epoch: 2, loss: 0.4917\n",
      "Test Loss: 0.673335, Acc: 0.579355, Pre: 0.746374, Rec: 0.722036, F1: 0.697576\n",
      "Train F1: 0.267400\n",
      "Train F1: 0.514335\n",
      "Train F1: 0.847668\n",
      "epoch: 3, loss: 0.4972\n",
      "Test Loss: 0.704764, Acc: 0.587097, Pre: 0.757518, Rec: 0.727305, F1: 0.701001\n",
      "Train F1: 0.270093\n",
      "Train F1: 0.520538\n",
      "Train F1: 0.853871\n",
      "epoch: 4, loss: 0.43\n",
      "Test Loss: 0.668505, Acc: 0.603011, Pre: 0.764084, Rec: 0.737668, F1: 0.717124\n",
      "Train F1: 0.272442\n",
      "Train F1: 0.526696\n",
      "Train F1: 0.860030\n",
      "epoch: 5, loss: 0.411\n",
      "Test Loss: 0.646295, Acc: 0.626237, Pre: 0.783562, Rec: 0.753109, F1: 0.734410\n",
      "Train F1: 0.275330\n",
      "Train F1: 0.533993\n",
      "Train F1: 0.867326\n",
      "epoch: 6, loss: 0.4632\n",
      "Test Loss: 0.646493, Acc: 0.632688, Pre: 0.787755, Rec: 0.757369, F1: 0.739641\n",
      "Train F1: 0.277397\n",
      "Train F1: 0.540306\n",
      "Train F1: 0.873639\n",
      "epoch: 7, loss: 0.3854\n",
      "Test Loss: 0.616925, Acc: 0.643871, Pre: 0.783117, Rec: 0.764382, F1: 0.753298\n",
      "Train F1: 0.279050\n",
      "Train F1: 0.545541\n",
      "Train F1: 0.878874\n",
      "epoch: 8, loss: 0.4484\n",
      "Test Loss: 0.609803, Acc: 0.649892, Pre: 0.780367, Rec: 0.768075, F1: 0.760631\n",
      "Train F1: 0.279904\n",
      "Train F1: 0.548832\n",
      "Train F1: 0.882166\n",
      "epoch: 9, loss: 0.4223\n",
      "Test Loss: 0.626333, Acc: 0.659785, Pre: 0.794997, Rec: 0.774966, F1: 0.764605\n",
      "Train F1: 0.282206\n",
      "Train F1: 0.554759\n",
      "Train F1: 0.888092\n",
      "epoch: 10, loss: 0.4007\n",
      "Test Loss: 0.623161, Acc: 0.663226, Pre: 0.789925, Rec: 0.776947, F1: 0.769906\n",
      "Train F1: 0.284284\n",
      "Train F1: 0.560667\n",
      "Train F1: 0.894001\n",
      "epoch: 11, loss: 0.3956\n",
      "Test Loss: 0.663268, Acc: 0.658065, Pre: 0.796790, Rec: 0.773926, F1: 0.762244\n",
      "Train F1: 0.284842\n",
      "Train F1: 0.562479\n",
      "Train F1: 0.895812\n",
      "epoch: 12, loss: 0.2692\n",
      "Test Loss: 0.631255, Acc: 0.670968, Pre: 0.794377, Rec: 0.782048, F1: 0.775677\n",
      "Train F1: 0.286804\n",
      "Train F1: 0.567393\n",
      "Train F1: 0.900726\n",
      "epoch: 13, loss: 0.3527\n",
      "Test Loss: 0.629678, Acc: 0.676989, Pre: 0.799154, Rec: 0.786078, F1: 0.779666\n",
      "Train F1: 0.287265\n",
      "Train F1: 0.568971\n",
      "Train F1: 0.902305\n",
      "epoch: 14, loss: 0.4132\n",
      "Test Loss: 0.651617, Acc: 0.679570, Pre: 0.805940, Rec: 0.787998, F1: 0.779797\n",
      "Train F1: 0.289247\n",
      "Train F1: 0.573301\n",
      "Train F1: 0.906635\n",
      "epoch: 15, loss: 0.2087\n",
      "Test Loss: 0.682601, Acc: 0.671828, Pre: 0.800413, Rec: 0.782851, F1: 0.774337\n",
      "Train F1: 0.289944\n",
      "Train F1: 0.575216\n",
      "Train F1: 0.908549\n",
      "epoch: 16, loss: 0.1881\n",
      "Test Loss: 0.638711, Acc: 0.678710, Pre: 0.797004, Rec: 0.787057, F1: 0.782031\n",
      "Train F1: 0.291240\n",
      "Train F1: 0.578107\n",
      "Train F1: 0.911440\n",
      "epoch: 17, loss: 0.339\n",
      "Test Loss: 0.612763, Acc: 0.687312, Pre: 0.798353, Rec: 0.792502, F1: 0.789481\n",
      "Train F1: 0.291748\n",
      "Train F1: 0.579502\n",
      "Train F1: 0.912836\n",
      "epoch: 18, loss: 0.3048\n",
      "Test Loss: 0.672204, Acc: 0.670968, Pre: 0.795694, Rec: 0.782109, F1: 0.775195\n",
      "Train F1: 0.292425\n",
      "Train F1: 0.580941\n",
      "Train F1: 0.914274\n",
      "epoch: 19, loss: 0.2182\n",
      "Test Loss: 0.638479, Acc: 0.676559, Pre: 0.791214, Rec: 0.785359, F1: 0.782109\n",
      "Train F1: 0.292812\n",
      "Train F1: 0.581808\n",
      "Train F1: 0.915142\n",
      "epoch: 20, loss: 0.3566\n",
      "Test Loss: 0.650172, Acc: 0.680000, Pre: 0.797301, Rec: 0.787883, F1: 0.783128\n",
      "Train F1: 0.294609\n",
      "Train F1: 0.585699\n",
      "Train F1: 0.919033\n",
      "epoch: 21, loss: 0.2529\n",
      "Test Loss: 0.625768, Acc: 0.689032, Pre: 0.800111, Rec: 0.793688, F1: 0.790455\n",
      "Train F1: 0.296105\n",
      "Train F1: 0.588941\n",
      "Train F1: 0.922275\n",
      "epoch: 22, loss: 0.3171\n",
      "Test Loss: 0.653838, Acc: 0.691183, Pre: 0.806392, Rec: 0.795393, F1: 0.790371\n",
      "Train F1: 0.295762\n",
      "Train F1: 0.588066\n",
      "Train F1: 0.921400\n",
      "epoch: 23, loss: 0.2837\n",
      "Test Loss: 0.667045, Acc: 0.687312, Pre: 0.804130, Rec: 0.792839, F1: 0.787567\n",
      "Train F1: 0.297938\n",
      "Train F1: 0.593058\n",
      "Train F1: 0.926392\n",
      "epoch: 24, loss: 0.3683\n",
      "Test Loss: 0.672607, Acc: 0.692903, Pre: 0.806136, Rec: 0.796464, F1: 0.792011\n",
      "Train F1: 0.297471\n",
      "Train F1: 0.592128\n",
      "Train F1: 0.925461\n",
      "epoch: 25, loss: 0.286\n",
      "Test Loss: 0.664898, Acc: 0.695054, Pre: 0.807459, Rec: 0.797886, F1: 0.793534\n",
      "Train F1: 0.297583\n",
      "Train F1: 0.592650\n",
      "Train F1: 0.925984\n",
      "epoch: 26, loss: 0.3294\n",
      "Test Loss: 0.666646, Acc: 0.691613, Pre: 0.809261, Rec: 0.795798, F1: 0.789862\n",
      "Train F1: 0.298649\n",
      "Train F1: 0.594954\n",
      "Train F1: 0.928287\n",
      "epoch: 27, loss: 0.276\n",
      "Test Loss: 0.659399, Acc: 0.700215, Pre: 0.806154, Rec: 0.801022, F1: 0.798531\n",
      "Train F1: 0.299701\n",
      "Train F1: 0.597183\n",
      "Train F1: 0.930516\n",
      "epoch: 28, loss: 0.1528\n",
      "Test Loss: 0.704168, Acc: 0.698065, Pre: 0.818875, Rec: 0.800288, F1: 0.792851\n",
      "Train F1: 0.300383\n",
      "Train F1: 0.598773\n",
      "Train F1: 0.932106\n",
      "epoch: 29, loss: 0.247\n",
      "Test Loss: 0.714374, Acc: 0.700215, Pre: 0.817888, Rec: 0.801626, F1: 0.795065\n",
      "Train F1: 0.300331\n",
      "Train F1: 0.598703\n",
      "Train F1: 0.932036\n",
      "epoch: 30, loss: 0.3469\n",
      "Test Loss: 0.664184, Acc: 0.702796, Pre: 0.811983, Rec: 0.802995, F1: 0.799073\n",
      "Train F1: 0.301474\n",
      "Train F1: 0.601224\n",
      "Train F1: 0.934557\n",
      "epoch: 31, loss: 0.1793\n",
      "Test Loss: 0.667662, Acc: 0.700215, Pre: 0.811300, Rec: 0.801335, F1: 0.796985\n",
      "Train F1: 0.302252\n",
      "Train F1: 0.602574\n",
      "Train F1: 0.935907\n",
      "epoch: 32, loss: 0.2624\n",
      "Test Loss: 0.658175, Acc: 0.706237, Pre: 0.807887, Rec: 0.804838, F1: 0.803272\n",
      "Train F1: 0.302535\n",
      "Train F1: 0.603451\n",
      "Train F1: 0.936784\n",
      "epoch: 33, loss: 0.2025\n",
      "Test Loss: 0.676495, Acc: 0.709677, Pre: 0.809755, Rec: 0.807086, F1: 0.805706\n",
      "Train F1: 0.303935\n",
      "Train F1: 0.606471\n",
      "Train F1: 0.939805\n",
      "epoch: 34, loss: 0.1372\n",
      "Test Loss: 0.658047, Acc: 0.708817, Pre: 0.812001, Rec: 0.806750, F1: 0.804342\n",
      "Train F1: 0.304397\n",
      "Train F1: 0.607554\n",
      "Train F1: 0.940887\n",
      "epoch: 35, loss: 0.1765\n",
      "Test Loss: 0.679986, Acc: 0.708387, Pre: 0.810835, Rec: 0.806398, F1: 0.804298\n",
      "Train F1: 0.304042\n",
      "Train F1: 0.606611\n",
      "Train F1: 0.939945\n",
      "epoch: 36, loss: 0.2376\n",
      "Test Loss: 0.672656, Acc: 0.710538, Pre: 0.811879, Rec: 0.807798, F1: 0.805861\n",
      "Train F1: 0.304070\n",
      "Train F1: 0.606834\n",
      "Train F1: 0.940167\n",
      "epoch: 37, loss: 0.2138\n",
      "Test Loss: 0.661951, Acc: 0.715269, Pre: 0.817437, Rec: 0.811117, F1: 0.808417\n",
      "Train F1: 0.303804\n",
      "Train F1: 0.606408\n",
      "Train F1: 0.939742\n",
      "epoch: 38, loss: 0.3434\n",
      "Test Loss: 0.791275, Acc: 0.704946, Pre: 0.824554, Rec: 0.804891, F1: 0.797451\n",
      "Train F1: 0.303789\n",
      "Train F1: 0.606260\n",
      "Train F1: 0.939593\n",
      "epoch: 39, loss: 0.4079\n",
      "Test Loss: 0.710272, Acc: 0.715699, Pre: 0.819860, Rec: 0.811530, F1: 0.808140\n",
      "Train F1: 0.306508\n",
      "Train F1: 0.611932\n",
      "Train F1: 0.945266\n",
      "epoch: 40, loss: 0.1809\n",
      "Test Loss: 0.708247, Acc: 0.716989, Pre: 0.820980, Rec: 0.812402, F1: 0.808955\n",
      "Train F1: 0.306983\n",
      "Train F1: 0.612938\n",
      "Train F1: 0.946271\n",
      "epoch: 41, loss: 0.2033\n",
      "Test Loss: 0.729019, Acc: 0.713548, Pre: 0.821572, Rec: 0.810260, F1: 0.805821\n",
      "Train F1: 0.307763\n",
      "Train F1: 0.614441\n",
      "Train F1: 0.947774\n",
      "epoch: 42, loss: 0.2384\n",
      "Test Loss: 0.663833, Acc: 0.716129, Pre: 0.811756, Rec: 0.811086, F1: 0.810605\n",
      "Train F1: 0.306868\n",
      "Train F1: 0.612798\n",
      "Train F1: 0.946131\n",
      "epoch: 43, loss: 0.2699\n",
      "Test Loss: 0.703781, Acc: 0.717419, Pre: 0.816128, Rec: 0.812348, F1: 0.810600\n",
      "Train F1: 0.308871\n",
      "Train F1: 0.616738\n",
      "Train F1: 0.950071\n",
      "epoch: 44, loss: 0.2264\n",
      "Test Loss: 0.723459, Acc: 0.701075, Pre: 0.801266, Rec: 0.800961, F1: 0.800652\n",
      "Train F1: 0.307774\n",
      "Train F1: 0.614664\n",
      "Train F1: 0.947997\n",
      "epoch: 45, loss: 0.1743\n",
      "Test Loss: 0.706941, Acc: 0.712688, Pre: 0.809144, Rec: 0.808731, F1: 0.808374\n",
      "Train F1: 0.308023\n",
      "Train F1: 0.615165\n",
      "Train F1: 0.948499\n",
      "epoch: 46, loss: 0.2593\n",
      "Test Loss: 0.708422, Acc: 0.710108, Pre: 0.811537, Rec: 0.807507, F1: 0.805586\n",
      "Train F1: 0.308484\n",
      "Train F1: 0.616098\n",
      "Train F1: 0.949431\n",
      "epoch: 47, loss: 0.2023\n",
      "Test Loss: 0.757152, Acc: 0.724301, Pre: 0.830093, Rec: 0.817464, F1: 0.812918\n",
      "Train F1: 0.309816\n",
      "Train F1: 0.618898\n",
      "Train F1: 0.952231\n",
      "epoch: 48, loss: 0.1874\n",
      "Test Loss: 0.808029, Acc: 0.712258, Pre: 0.821391, Rec: 0.809434, F1: 0.804745\n",
      "Train F1: 0.308990\n",
      "Train F1: 0.617175\n",
      "Train F1: 0.950509\n",
      "epoch: 49, loss: 0.18\n",
      "Test Loss: 0.737970, Acc: 0.713978, Pre: 0.813506, Rec: 0.810031, F1: 0.808366\n",
      "Train F1: 0.309405\n",
      "Train F1: 0.618110\n",
      "Train F1: 0.951444\n",
      "epoch: 50, loss: 0.1872\n",
      "Test Loss: 0.727017, Acc: 0.712258, Pre: 0.812597, Rec: 0.808907, F1: 0.807141\n",
      "Train F1: 0.310659\n",
      "Train F1: 0.620620\n",
      "Train F1: 0.953953\n",
      "epoch: 51, loss: 0.1421\n",
      "Test Loss: 0.816363, Acc: 0.723011, Pre: 0.831681, Rec: 0.816707, F1: 0.811426\n",
      "Train F1: 0.309854\n",
      "Train F1: 0.618970\n",
      "Train F1: 0.952303\n",
      "epoch: 52, loss: 0.2777\n",
      "Test Loss: 0.816752, Acc: 0.726882, Pre: 0.834025, Rec: 0.819269, F1: 0.814178\n",
      "Train F1: 0.310908\n",
      "Train F1: 0.621195\n",
      "Train F1: 0.954529\n",
      "epoch: 53, loss: 0.08693\n",
      "Test Loss: 0.830235, Acc: 0.717419, Pre: 0.825120, Rec: 0.812876, F1: 0.808248\n",
      "Train F1: 0.310056\n",
      "Train F1: 0.619401\n",
      "Train F1: 0.952734\n",
      "epoch: 54, loss: 0.1491\n",
      "Test Loss: 0.846949, Acc: 0.725591, Pre: 0.833773, Rec: 0.818436, F1: 0.813135\n",
      "Train F1: 0.311489\n",
      "Train F1: 0.622489\n",
      "Train F1: 0.955823\n",
      "epoch: 55, loss: 0.1076\n",
      "Test Loss: 0.778104, Acc: 0.725591, Pre: 0.827053, Rec: 0.818137, F1: 0.814762\n",
      "Train F1: 0.313592\n",
      "Train F1: 0.626647\n",
      "Train F1: 0.959980\n",
      "epoch: 56, loss: 0.1613\n",
      "Test Loss: 0.786481, Acc: 0.720430, Pre: 0.820669, Rec: 0.814535, F1: 0.811982\n",
      "Train F1: 0.313321\n",
      "Train F1: 0.626146\n",
      "Train F1: 0.959479\n",
      "epoch: 57, loss: 0.0871\n",
      "Test Loss: 0.839029, Acc: 0.727742, Pre: 0.831522, Rec: 0.819713, F1: 0.815504\n",
      "Train F1: 0.312343\n",
      "Train F1: 0.624137\n",
      "Train F1: 0.957470\n",
      "epoch: 58, loss: 0.1904\n",
      "Test Loss: 0.810256, Acc: 0.720860, Pre: 0.818961, Rec: 0.814681, F1: 0.812785\n",
      "Train F1: 0.313805\n",
      "Train F1: 0.627077\n",
      "Train F1: 0.960410\n",
      "epoch: 59, loss: 0.1318\n",
      "Test Loss: 0.821655, Acc: 0.720430, Pre: 0.820551, Rec: 0.814528, F1: 0.812013\n",
      "Train F1: 0.312979\n",
      "Train F1: 0.625501\n",
      "Train F1: 0.958835\n",
      "epoch: 60, loss: 0.1701\n",
      "Test Loss: 0.775073, Acc: 0.715269, Pre: 0.810801, Rec: 0.810436, F1: 0.810108\n",
      "Train F1: 0.313028\n",
      "Train F1: 0.625646\n",
      "Train F1: 0.958979\n",
      "epoch: 61, loss: 0.1048\n",
      "Test Loss: 0.854852, Acc: 0.729892, Pre: 0.831162, Rec: 0.821059, F1: 0.817411\n",
      "Train F1: 0.314030\n",
      "Train F1: 0.627653\n",
      "Train F1: 0.960987\n",
      "epoch: 62, loss: 0.2138\n",
      "Test Loss: 0.970562, Acc: 0.725161, Pre: 0.837665, Rec: 0.818298, F1: 0.811854\n",
      "Train F1: 0.313450\n",
      "Train F1: 0.626506\n",
      "Train F1: 0.959840\n",
      "epoch: 63, loss: 0.2929\n",
      "Test Loss: 0.801544, Acc: 0.727742, Pre: 0.821401, Rec: 0.819070, F1: 0.817943\n",
      "Train F1: 0.315536\n",
      "Train F1: 0.630665\n",
      "Train F1: 0.963998\n",
      "epoch: 64, loss: 0.1045\n",
      "Test Loss: 0.932472, Acc: 0.724731, Pre: 0.830493, Rec: 0.817755, F1: 0.813188\n",
      "Train F1: 0.314322\n",
      "Train F1: 0.628227\n",
      "Train F1: 0.961560\n",
      "epoch: 65, loss: 0.1545\n",
      "Test Loss: 0.851457, Acc: 0.732473, Pre: 0.828544, Rec: 0.822535, F1: 0.820196\n",
      "Train F1: 0.315030\n",
      "Train F1: 0.629734\n",
      "Train F1: 0.963068\n",
      "epoch: 66, loss: 0.1373\n",
      "Test Loss: 0.826673, Acc: 0.736774, Pre: 0.831045, Rec: 0.825372, F1: 0.823195\n",
      "Train F1: 0.315453\n",
      "Train F1: 0.630522\n",
      "Train F1: 0.963855\n",
      "epoch: 67, loss: 0.1777\n",
      "Test Loss: 0.892357, Acc: 0.730753, Pre: 0.829825, Rec: 0.821533, F1: 0.818453\n",
      "Train F1: 0.315411\n",
      "Train F1: 0.630523\n",
      "Train F1: 0.963857\n",
      "epoch: 68, loss: 0.1734\n",
      "Test Loss: 0.854588, Acc: 0.720000, Pre: 0.819191, Rec: 0.814168, F1: 0.811997\n",
      "Train F1: 0.315801\n",
      "Train F1: 0.631385\n",
      "Train F1: 0.964718\n",
      "epoch: 69, loss: 0.2064\n",
      "Test Loss: 0.889719, Acc: 0.737634, Pre: 0.837494, Rec: 0.826259, F1: 0.822459\n",
      "Train F1: 0.316946\n",
      "Train F1: 0.633607\n",
      "Train F1: 0.966940\n",
      "epoch: 70, loss: 0.1115\n",
      "Test Loss: 0.880931, Acc: 0.735054, Pre: 0.830211, Rec: 0.824248, F1: 0.821958\n",
      "Train F1: 0.317809\n",
      "Train F1: 0.635328\n",
      "Train F1: 0.968661\n",
      "epoch: 71, loss: 0.1231\n",
      "Test Loss: 0.904171, Acc: 0.721720, Pre: 0.818268, Rec: 0.815147, F1: 0.813684\n",
      "Train F1: 0.316992\n",
      "Train F1: 0.633751\n",
      "Train F1: 0.967084\n",
      "epoch: 72, loss: 0.09511\n",
      "Test Loss: 0.851017, Acc: 0.723871, Pre: 0.819199, Rec: 0.816531, F1: 0.815254\n",
      "Train F1: 0.317274\n",
      "Train F1: 0.634325\n",
      "Train F1: 0.967658\n",
      "epoch: 73, loss: 0.1365\n",
      "Test Loss: 0.930774, Acc: 0.717849, Pre: 0.816105, Rec: 0.812608, F1: 0.810971\n",
      "Train F1: 0.317189\n",
      "Train F1: 0.634182\n",
      "Train F1: 0.967515\n",
      "epoch: 74, loss: 0.1279\n",
      "Test Loss: 0.920104, Acc: 0.733333, Pre: 0.829505, Rec: 0.823131, F1: 0.820689\n",
      "Train F1: 0.316734\n",
      "Train F1: 0.633249\n",
      "Train F1: 0.966583\n",
      "epoch: 75, loss: 0.1347\n",
      "Test Loss: 0.910001, Acc: 0.733333, Pre: 0.827467, Rec: 0.822994, F1: 0.821162\n",
      "Train F1: 0.316405\n",
      "Train F1: 0.632604\n",
      "Train F1: 0.965938\n",
      "epoch: 76, loss: 0.1361\n",
      "Test Loss: 0.860797, Acc: 0.712258, Pre: 0.809886, Rec: 0.807637, F1: 0.807236\n",
      "Train F1: 0.316405\n",
      "Train F1: 0.632604\n",
      "Train F1: 0.965938\n",
      "epoch: 77, loss: 0.1208\n",
      "Test Loss: 1.042820, Acc: 0.734194, Pre: 0.833900, Rec: 0.823911, F1: 0.820391\n",
      "Train F1: 0.316629\n",
      "Train F1: 0.633034\n",
      "Train F1: 0.966368\n",
      "epoch: 78, loss: 0.1045\n",
      "Test Loss: 0.917239, Acc: 0.732473, Pre: 0.826280, Rec: 0.822374, F1: 0.820724\n",
      "Train F1: 0.316202\n",
      "Train F1: 0.632174\n",
      "Train F1: 0.965507\n",
      "epoch: 79, loss: 0.1321\n",
      "Test Loss: 0.899380, Acc: 0.720430, Pre: 0.814433, Rec: 0.813916, F1: 0.813516\n",
      "Train F1: 0.317566\n",
      "Train F1: 0.634971\n",
      "Train F1: 0.968304\n",
      "epoch: 80, loss: 0.1421\n",
      "Test Loss: 0.943448, Acc: 0.735484, Pre: 0.829531, Rec: 0.824470, F1: 0.822470\n",
      "Train F1: 0.319786\n",
      "Train F1: 0.639417\n",
      "Train F1: 0.972750\n",
      "epoch: 81, loss: 0.1098\n",
      "Test Loss: 0.896685, Acc: 0.723441, Pre: 0.815836, Rec: 0.815767, F1: 0.815621\n",
      "Train F1: 0.319511\n",
      "Train F1: 0.638915\n",
      "Train F1: 0.972249\n",
      "epoch: 82, loss: 0.09377\n",
      "Test Loss: 0.967683, Acc: 0.735054, Pre: 0.831078, Rec: 0.824301, F1: 0.821759\n",
      "Train F1: 0.319809\n",
      "Train F1: 0.639416\n",
      "Train F1: 0.972750\n",
      "epoch: 83, loss: 0.1223\n",
      "Test Loss: 0.921182, Acc: 0.726022, Pre: 0.818689, Rec: 0.817732, F1: 0.817142\n",
      "Train F1: 0.320237\n",
      "Train F1: 0.640349\n",
      "Train F1: 0.973683\n",
      "epoch: 84, loss: 0.116\n",
      "Test Loss: 0.944085, Acc: 0.735484, Pre: 0.828585, Rec: 0.824401, F1: 0.822686\n",
      "Train F1: 0.320106\n",
      "Train F1: 0.640135\n",
      "Train F1: 0.973468\n",
      "epoch: 85, loss: 0.02447\n",
      "Test Loss: 1.038797, Acc: 0.741075, Pre: 0.840650, Rec: 0.828584, F1: 0.824636\n",
      "Train F1: 0.320678\n",
      "Train F1: 0.641282\n",
      "Train F1: 0.974615\n",
      "epoch: 86, loss: 0.05747\n",
      "Test Loss: 1.079103, Acc: 0.730323, Pre: 0.830468, Rec: 0.821296, F1: 0.817938\n",
      "Train F1: 0.320723\n",
      "Train F1: 0.641354\n",
      "Train F1: 0.974687\n",
      "epoch: 87, loss: 0.09668\n",
      "Test Loss: 0.953352, Acc: 0.726882, Pre: 0.819474, Rec: 0.818336, F1: 0.817672\n",
      "Train F1: 0.320990\n",
      "Train F1: 0.641927\n",
      "Train F1: 0.975261\n",
      "epoch: 88, loss: 0.08525\n",
      "Test Loss: 0.966977, Acc: 0.746237, Pre: 0.836536, Rec: 0.831613, F1: 0.829770\n",
      "Train F1: 0.319086\n",
      "Train F1: 0.638055\n",
      "Train F1: 0.971388\n",
      "epoch: 89, loss: 0.1797\n",
      "Test Loss: 1.144265, Acc: 0.737204, Pre: 0.838498, Rec: 0.826030, F1: 0.821877\n",
      "Train F1: 0.319637\n",
      "Train F1: 0.639202\n",
      "Train F1: 0.972536\n",
      "epoch: 90, loss: 0.1201\n",
      "Test Loss: 1.021167, Acc: 0.746237, Pre: 0.838202, Rec: 0.831720, F1: 0.829419\n",
      "Train F1: 0.321723\n",
      "Train F1: 0.643361\n",
      "Train F1: 0.976695\n",
      "epoch: 91, loss: 0.1582\n",
      "Test Loss: 0.973223, Acc: 0.721720, Pre: 0.814448, Rec: 0.814451, F1: 0.814449\n",
      "Train F1: 0.320034\n",
      "Train F1: 0.640063\n",
      "Train F1: 0.973396\n",
      "epoch: 92, loss: 0.1158\n",
      "Test Loss: 1.090640, Acc: 0.733763, Pre: 0.827814, Rec: 0.823284, F1: 0.821439\n",
      "Train F1: 0.321558\n",
      "Train F1: 0.643075\n",
      "Train F1: 0.976408\n",
      "epoch: 93, loss: 0.1279\n",
      "Test Loss: 0.985797, Acc: 0.728602, Pre: 0.819323, Rec: 0.819223, F1: 0.819057\n",
      "Train F1: 0.321619\n",
      "Train F1: 0.643218\n",
      "Train F1: 0.976552\n",
      "epoch: 94, loss: 0.06691\n",
      "Test Loss: 1.012656, Acc: 0.739785, Pre: 0.832948, Rec: 0.827368, F1: 0.825256\n",
      "Train F1: 0.321126\n",
      "Train F1: 0.642214\n",
      "Train F1: 0.975548\n",
      "epoch: 95, loss: 0.1109\n",
      "Test Loss: 1.089897, Acc: 0.741505, Pre: 0.835113, Rec: 0.828576, F1: 0.826198\n",
      "Train F1: 0.322225\n",
      "Train F1: 0.644437\n",
      "Train F1: 0.977771\n",
      "epoch: 96, loss: 0.06987\n",
      "Test Loss: 1.064432, Acc: 0.736774, Pre: 0.830146, Rec: 0.825311, F1: 0.823398\n",
      "Train F1: 0.321970\n",
      "Train F1: 0.643935\n",
      "Train F1: 0.977269\n",
      "epoch: 97, loss: 0.2012\n",
      "Test Loss: 1.193852, Acc: 0.738065, Pre: 0.836091, Rec: 0.826466, F1: 0.823130\n",
      "Train F1: 0.320576\n",
      "Train F1: 0.641139\n",
      "Train F1: 0.974472\n",
      "epoch: 98, loss: 0.07997\n",
      "Test Loss: 1.084946, Acc: 0.741505, Pre: 0.836585, Rec: 0.828661, F1: 0.825876\n",
      "Train F1: 0.322428\n",
      "Train F1: 0.644868\n",
      "Train F1: 0.978201\n",
      "epoch: 99, loss: 0.129\n",
      "Test Loss: 1.050680, Acc: 0.746237, Pre: 0.833648, Rec: 0.831376, F1: 0.830365\n",
      "The  2  th fold cross validation:\n",
      "Train F1: 0.216227\n",
      "Train F1: 0.246630\n",
      "Train F1: 0.246630\n",
      "epoch: 0, loss: 1.346\n",
      "Test Loss: 1.302962, Acc: 0.530981, Pre: 0.835257, Rec: 0.691457, F1: 0.627123\n",
      "Train F1: 0.252863\n",
      "Train F1: 0.468349\n",
      "Train F1: 0.801682\n",
      "epoch: 1, loss: 0.731\n",
      "Test Loss: 0.722948, Acc: 0.717298, Pre: 0.816049, Rec: 0.812290, F1: 0.810530\n",
      "Train F1: 0.249355\n",
      "Train F1: 0.486024\n",
      "Train F1: 0.819357\n",
      "epoch: 2, loss: 0.5438\n",
      "Test Loss: 0.564664, Acc: 0.733649, Pre: 0.848426, Rec: 0.824141, F1: 0.816757\n",
      "Train F1: 0.261593\n",
      "Train F1: 0.497796\n",
      "Train F1: 0.831129\n",
      "epoch: 3, loss: 0.5877\n",
      "Test Loss: 0.533785, Acc: 0.731067, Pre: 0.854857, Rec: 0.822641, F1: 0.813234\n",
      "Train F1: 0.265147\n",
      "Train F1: 0.505958\n",
      "Train F1: 0.839292\n",
      "epoch: 4, loss: 0.5121\n",
      "Test Loss: 0.527151, Acc: 0.724613, Pre: 0.851824, Rec: 0.818389, F1: 0.808299\n",
      "Train F1: 0.268398\n",
      "Train F1: 0.515157\n",
      "Train F1: 0.848490\n",
      "epoch: 5, loss: 0.4474\n",
      "Test Loss: 0.526550, Acc: 0.729346, Pre: 0.854816, Rec: 0.821525, F1: 0.811767\n",
      "Train F1: 0.271466\n",
      "Train F1: 0.524544\n",
      "Train F1: 0.857878\n",
      "epoch: 6, loss: 0.3883\n",
      "Test Loss: 0.536887, Acc: 0.724182, Pre: 0.850967, Rec: 0.818090, F1: 0.808105\n",
      "Train F1: 0.273970\n",
      "Train F1: 0.532173\n",
      "Train F1: 0.865507\n",
      "epoch: 7, loss: 0.4402\n",
      "Test Loss: 0.535512, Acc: 0.731497, Pre: 0.843047, Rec: 0.822592, F1: 0.816081\n",
      "Train F1: 0.276780\n",
      "Train F1: 0.540749\n",
      "Train F1: 0.874082\n",
      "epoch: 8, loss: 0.3448\n",
      "Test Loss: 0.541289, Acc: 0.729346, Pre: 0.832003, Rec: 0.820790, F1: 0.816770\n",
      "Train F1: 0.278194\n",
      "Train F1: 0.545523\n",
      "Train F1: 0.878856\n",
      "epoch: 9, loss: 0.3143\n",
      "Test Loss: 0.562174, Acc: 0.726334, Pre: 0.844841, Rec: 0.819323, F1: 0.811274\n",
      "Train F1: 0.279962\n",
      "Train F1: 0.550544\n",
      "Train F1: 0.883877\n",
      "epoch: 10, loss: 0.5475\n",
      "Test Loss: 0.561081, Acc: 0.725904, Pre: 0.835144, Rec: 0.818732, F1: 0.813102\n",
      "Train F1: 0.281532\n",
      "Train F1: 0.554941\n",
      "Train F1: 0.888274\n",
      "epoch: 11, loss: 0.313\n",
      "Test Loss: 0.573650, Acc: 0.723322, Pre: 0.840217, Rec: 0.817247, F1: 0.809712\n",
      "Train F1: 0.282924\n",
      "Train F1: 0.558881\n",
      "Train F1: 0.892215\n",
      "epoch: 12, loss: 0.3019\n",
      "Test Loss: 0.580995, Acc: 0.720740, Pre: 0.835757, Rec: 0.815447, F1: 0.808517\n",
      "Train F1: 0.283635\n",
      "Train F1: 0.561044\n",
      "Train F1: 0.894377\n",
      "epoch: 13, loss: 0.3605\n",
      "Test Loss: 0.598446, Acc: 0.728485, Pre: 0.844452, Rec: 0.820698, F1: 0.813205\n",
      "Train F1: 0.285103\n",
      "Train F1: 0.564538\n",
      "Train F1: 0.897871\n",
      "epoch: 14, loss: 0.424\n",
      "Test Loss: 0.567519, Acc: 0.725904, Pre: 0.828260, Rec: 0.818431, F1: 0.814750\n",
      "Train F1: 0.286486\n",
      "Train F1: 0.568207\n",
      "Train F1: 0.901540\n",
      "epoch: 15, loss: 0.307\n",
      "Test Loss: 0.585534, Acc: 0.731067, Pre: 0.843583, Rec: 0.822333, F1: 0.815598\n",
      "Train F1: 0.288044\n",
      "Train F1: 0.571674\n",
      "Train F1: 0.905008\n",
      "epoch: 16, loss: 0.3924\n",
      "Test Loss: 0.581113, Acc: 0.733219, Pre: 0.838370, Rec: 0.823526, F1: 0.818576\n",
      "Train F1: 0.288946\n",
      "Train F1: 0.573933\n",
      "Train F1: 0.907266\n",
      "epoch: 17, loss: 0.19\n",
      "Test Loss: 0.593806, Acc: 0.737522, Pre: 0.844282, Rec: 0.826498, F1: 0.820906\n",
      "Train F1: 0.290239\n",
      "Train F1: 0.576977\n",
      "Train F1: 0.910310\n",
      "epoch: 18, loss: 0.3901\n",
      "Test Loss: 0.603473, Acc: 0.740103, Pre: 0.847348, Rec: 0.828259, F1: 0.822427\n",
      "Train F1: 0.290747\n",
      "Train F1: 0.578133\n",
      "Train F1: 0.911466\n",
      "epoch: 19, loss: 0.3638\n",
      "Test Loss: 0.597944, Acc: 0.742255, Pre: 0.848134, Rec: 0.829666, F1: 0.824065\n",
      "Train F1: 0.291952\n",
      "Train F1: 0.580801\n",
      "Train F1: 0.914135\n",
      "epoch: 20, loss: 0.3631\n",
      "Test Loss: 0.594522, Acc: 0.738812, Pre: 0.844243, Rec: 0.827323, F1: 0.821998\n",
      "Train F1: 0.291957\n",
      "Train F1: 0.580878\n",
      "Train F1: 0.914211\n",
      "epoch: 21, loss: 0.2843\n",
      "Test Loss: 0.615388, Acc: 0.743546, Pre: 0.851880, Rec: 0.830618, F1: 0.824384\n",
      "Train F1: 0.292649\n",
      "Train F1: 0.582314\n",
      "Train F1: 0.915647\n",
      "epoch: 22, loss: 0.248\n",
      "Test Loss: 0.612012, Acc: 0.743115, Pre: 0.849239, Rec: 0.830256, F1: 0.824557\n",
      "Train F1: 0.295181\n",
      "Train F1: 0.587554\n",
      "Train F1: 0.920887\n",
      "epoch: 23, loss: 0.3154\n",
      "Test Loss: 0.642970, Acc: 0.747849, Pre: 0.853366, Rec: 0.833432, F1: 0.827671\n",
      "Train F1: 0.296327\n",
      "Train F1: 0.590229\n",
      "Train F1: 0.923563\n",
      "epoch: 24, loss: 0.282\n",
      "Test Loss: 0.608071, Acc: 0.737952, Pre: 0.835108, Rec: 0.826378, F1: 0.823272\n",
      "Train F1: 0.295817\n",
      "Train F1: 0.589228\n",
      "Train F1: 0.922561\n",
      "epoch: 25, loss: 0.2006\n",
      "Test Loss: 0.643128, Acc: 0.744836, Pre: 0.846746, Rec: 0.831269, F1: 0.826498\n",
      "Train F1: 0.297658\n",
      "Train F1: 0.593108\n",
      "Train F1: 0.926441\n",
      "epoch: 26, loss: 0.2913\n",
      "Test Loss: 0.607252, Acc: 0.748709, Pre: 0.840579, Rec: 0.833437, F1: 0.830958\n",
      "Train F1: 0.297274\n",
      "Train F1: 0.592547\n",
      "Train F1: 0.925880\n",
      "epoch: 27, loss: 0.3148\n",
      "Test Loss: 0.662208, Acc: 0.748709, Pre: 0.846872, Rec: 0.833745, F1: 0.829672\n",
      "Train F1: 0.298732\n",
      "Train F1: 0.595787\n",
      "Train F1: 0.929121\n",
      "epoch: 28, loss: 0.2893\n",
      "Test Loss: 0.677353, Acc: 0.756024, Pre: 0.859596, Rec: 0.838887, F1: 0.833236\n",
      "Train F1: 0.299143\n",
      "Train F1: 0.596574\n",
      "Train F1: 0.929907\n",
      "epoch: 29, loss: 0.1949\n",
      "Test Loss: 0.655714, Acc: 0.740964, Pre: 0.835062, Rec: 0.828264, F1: 0.825776\n",
      "Train F1: 0.300942\n",
      "Train F1: 0.600307\n",
      "Train F1: 0.933640\n",
      "epoch: 30, loss: 0.174\n",
      "Test Loss: 0.657202, Acc: 0.751291, Pre: 0.845366, Rec: 0.835316, F1: 0.832092\n",
      "Train F1: 0.301161\n",
      "Train F1: 0.600886\n",
      "Train F1: 0.934220\n",
      "epoch: 31, loss: 0.3271\n",
      "Test Loss: 0.631504, Acc: 0.762909, Pre: 0.853489, Rec: 0.843059, F1: 0.839951\n",
      "Train F1: 0.300264\n",
      "Train F1: 0.599319\n",
      "Train F1: 0.932652\n",
      "epoch: 32, loss: 0.3232\n",
      "Test Loss: 0.731474, Acc: 0.750861, Pre: 0.853814, Rec: 0.835381, F1: 0.830075\n",
      "Train F1: 0.301372\n",
      "Train F1: 0.601317\n",
      "Train F1: 0.934650\n",
      "epoch: 33, loss: 0.2391\n",
      "Test Loss: 0.746000, Acc: 0.757315, Pre: 0.864171, Rec: 0.839854, F1: 0.833472\n",
      "Train F1: 0.303350\n",
      "Train F1: 0.605557\n",
      "Train F1: 0.938890\n",
      "epoch: 34, loss: 0.2004\n",
      "Test Loss: 0.676353, Acc: 0.756024, Pre: 0.853431, Rec: 0.838674, F1: 0.834384\n",
      "Train F1: 0.303490\n",
      "Train F1: 0.605769\n",
      "Train F1: 0.939103\n",
      "epoch: 35, loss: 0.2153\n",
      "Test Loss: 0.724769, Acc: 0.764200, Pre: 0.866627, Rec: 0.844366, F1: 0.838666\n",
      "Train F1: 0.304611\n",
      "Train F1: 0.608139\n",
      "Train F1: 0.941473\n",
      "epoch: 36, loss: 0.3392\n",
      "Test Loss: 0.711803, Acc: 0.757315, Pre: 0.852103, Rec: 0.839444, F1: 0.835687\n",
      "Train F1: 0.303358\n",
      "Train F1: 0.605853\n",
      "Train F1: 0.939186\n",
      "epoch: 37, loss: 0.1767\n",
      "Test Loss: 0.723766, Acc: 0.761188, Pre: 0.862941, Rec: 0.842314, F1: 0.836855\n",
      "Train F1: 0.304999\n",
      "Train F1: 0.608854\n",
      "Train F1: 0.942187\n",
      "epoch: 38, loss: 0.224\n",
      "Test Loss: 0.686098, Acc: 0.752151, Pre: 0.839732, Rec: 0.835519, F1: 0.833920\n",
      "Train F1: 0.304934\n",
      "Train F1: 0.609081\n",
      "Train F1: 0.942414\n",
      "epoch: 39, loss: 0.226\n",
      "Test Loss: 0.702575, Acc: 0.755594, Pre: 0.847436, Rec: 0.838138, F1: 0.835186\n",
      "Train F1: 0.305812\n",
      "Train F1: 0.610948\n",
      "Train F1: 0.944281\n",
      "epoch: 40, loss: 0.2232\n",
      "Test Loss: 0.752972, Acc: 0.762909, Pre: 0.864738, Rec: 0.843478, F1: 0.837942\n",
      "Train F1: 0.306469\n",
      "Train F1: 0.612164\n",
      "Train F1: 0.945497\n",
      "epoch: 41, loss: 0.2065\n",
      "Test Loss: 0.699290, Acc: 0.751721, Pre: 0.843444, Rec: 0.835488, F1: 0.832829\n",
      "Train F1: 0.307457\n",
      "Train F1: 0.614392\n",
      "Train F1: 0.947725\n",
      "epoch: 42, loss: 0.1995\n",
      "Test Loss: 0.707837, Acc: 0.768503, Pre: 0.859721, Rec: 0.846888, F1: 0.843329\n",
      "Train F1: 0.308978\n",
      "Train F1: 0.617329\n",
      "Train F1: 0.950662\n",
      "epoch: 43, loss: 0.1638\n",
      "Test Loss: 0.738360, Acc: 0.768072, Pre: 0.863025, Rec: 0.846739, F1: 0.842412\n",
      "Train F1: 0.307981\n",
      "Train F1: 0.615248\n",
      "Train F1: 0.948581\n",
      "epoch: 44, loss: 0.2296\n",
      "Test Loss: 0.704003, Acc: 0.761618, Pre: 0.851671, Rec: 0.842155, F1: 0.839248\n",
      "Train F1: 0.308596\n",
      "Train F1: 0.616833\n",
      "Train F1: 0.950166\n",
      "epoch: 45, loss: 0.139\n",
      "Test Loss: 0.771842, Acc: 0.763339, Pre: 0.859259, Rec: 0.843571, F1: 0.839254\n",
      "Train F1: 0.309744\n",
      "Train F1: 0.618981\n",
      "Train F1: 0.952314\n",
      "epoch: 46, loss: 0.1704\n",
      "Test Loss: 0.801554, Acc: 0.763339, Pre: 0.864930, Rec: 0.843761, F1: 0.838258\n",
      "Train F1: 0.309405\n",
      "Train F1: 0.618263\n",
      "Train F1: 0.951596\n",
      "epoch: 47, loss: 0.2456\n",
      "Test Loss: 0.757318, Acc: 0.768933, Pre: 0.859742, Rec: 0.847163, F1: 0.843670\n",
      "Train F1: 0.310376\n",
      "Train F1: 0.620418\n",
      "Train F1: 0.953751\n",
      "epoch: 48, loss: 0.1641\n",
      "Test Loss: 0.819810, Acc: 0.757745, Pre: 0.852694, Rec: 0.839742, F1: 0.835924\n",
      "Train F1: 0.310160\n",
      "Train F1: 0.619842\n",
      "Train F1: 0.953175\n",
      "epoch: 49, loss: 0.2605\n",
      "Test Loss: 0.783091, Acc: 0.761188, Pre: 0.851611, Rec: 0.841880, F1: 0.838913\n",
      "Train F1: 0.309696\n",
      "Train F1: 0.618910\n",
      "Train F1: 0.952243\n",
      "epoch: 50, loss: 0.141\n",
      "Test Loss: 0.771155, Acc: 0.771945, Pre: 0.861603, Rec: 0.849159, F1: 0.845756\n",
      "Train F1: 0.311520\n",
      "Train F1: 0.622785\n",
      "Train F1: 0.956118\n",
      "epoch: 51, loss: 0.1275\n",
      "Test Loss: 0.828434, Acc: 0.765491, Pre: 0.860930, Rec: 0.845009, F1: 0.840696\n",
      "Train F1: 0.311377\n",
      "Train F1: 0.622425\n",
      "Train F1: 0.955759\n",
      "epoch: 52, loss: 0.106\n",
      "Test Loss: 0.774589, Acc: 0.761188, Pre: 0.850354, Rec: 0.841816, F1: 0.839147\n",
      "Train F1: 0.312302\n",
      "Train F1: 0.624290\n",
      "Train F1: 0.957623\n",
      "epoch: 53, loss: 0.1705\n",
      "Test Loss: 0.829069, Acc: 0.765921, Pre: 0.859657, Rec: 0.845237, F1: 0.841267\n",
      "Train F1: 0.312484\n",
      "Train F1: 0.624721\n",
      "Train F1: 0.958054\n",
      "epoch: 54, loss: 0.135\n",
      "Test Loss: 0.823174, Acc: 0.767212, Pre: 0.860690, Rec: 0.846102, F1: 0.842124\n",
      "Train F1: 0.313797\n",
      "Train F1: 0.627301\n",
      "Train F1: 0.960635\n",
      "epoch: 55, loss: 0.1703\n",
      "Test Loss: 0.790862, Acc: 0.767642, Pre: 0.854484, Rec: 0.846100, F1: 0.843562\n",
      "Train F1: 0.313204\n",
      "Train F1: 0.626299\n",
      "Train F1: 0.959633\n",
      "epoch: 56, loss: 0.0982\n",
      "Test Loss: 0.824701, Acc: 0.768933, Pre: 0.856244, Rec: 0.847005, F1: 0.844281\n",
      "Train F1: 0.311425\n",
      "Train F1: 0.622569\n",
      "Train F1: 0.955903\n",
      "epoch: 57, loss: 0.1054\n",
      "Test Loss: 0.809920, Acc: 0.760757, Pre: 0.849386, Rec: 0.841494, F1: 0.838982\n",
      "Train F1: 0.313579\n",
      "Train F1: 0.626871\n",
      "Train F1: 0.960204\n",
      "epoch: 58, loss: 0.1166\n",
      "Test Loss: 0.821304, Acc: 0.768503, Pre: 0.859721, Rec: 0.846888, F1: 0.843329\n",
      "Train F1: 0.314400\n",
      "Train F1: 0.628520\n",
      "Train F1: 0.961854\n",
      "epoch: 59, loss: 0.3403\n",
      "Test Loss: 0.853534, Acc: 0.769363, Pre: 0.857618, Rec: 0.847343, F1: 0.844383\n",
      "Train F1: 0.313772\n",
      "Train F1: 0.627447\n",
      "Train F1: 0.960780\n",
      "epoch: 60, loss: 0.1335\n",
      "Test Loss: 0.940798, Acc: 0.771084, Pre: 0.870712, Rec: 0.848925, F1: 0.843552\n",
      "Train F1: 0.314267\n",
      "Train F1: 0.628378\n",
      "Train F1: 0.961712\n",
      "epoch: 61, loss: 0.09484\n",
      "Test Loss: 0.860592, Acc: 0.770224, Pre: 0.861538, Rec: 0.848059, F1: 0.844393\n",
      "Train F1: 0.314414\n",
      "Train F1: 0.628665\n",
      "Train F1: 0.961998\n",
      "epoch: 62, loss: 0.1778\n",
      "Test Loss: 0.846746, Acc: 0.770224, Pre: 0.861341, Rec: 0.848051, F1: 0.844427\n",
      "Train F1: 0.316098\n",
      "Train F1: 0.632107\n",
      "Train F1: 0.965440\n",
      "epoch: 63, loss: 0.0857\n",
      "Test Loss: 0.958106, Acc: 0.768072, Pre: 0.867299, Rec: 0.846881, F1: 0.841688\n",
      "Train F1: 0.315699\n",
      "Train F1: 0.631318\n",
      "Train F1: 0.964652\n",
      "epoch: 64, loss: 0.1565\n",
      "Test Loss: 0.921780, Acc: 0.771945, Pre: 0.867336, Rec: 0.849372, F1: 0.844796\n",
      "Train F1: 0.316724\n",
      "Train F1: 0.633398\n",
      "Train F1: 0.966731\n",
      "epoch: 65, loss: 0.06793\n",
      "Test Loss: 0.930030, Acc: 0.771515, Pre: 0.866207, Rec: 0.849058, F1: 0.844638\n",
      "Train F1: 0.316622\n",
      "Train F1: 0.633183\n",
      "Train F1: 0.966516\n",
      "epoch: 66, loss: 0.07321\n",
      "Test Loss: 0.920944, Acc: 0.764630, Pre: 0.852915, Rec: 0.844120, F1: 0.841438\n",
      "Train F1: 0.315741\n",
      "Train F1: 0.631390\n",
      "Train F1: 0.964723\n",
      "epoch: 67, loss: 0.1245\n",
      "Test Loss: 0.941840, Acc: 0.768933, Pre: 0.862777, Rec: 0.847281, F1: 0.843146\n",
      "Train F1: 0.315891\n",
      "Train F1: 0.631677\n",
      "Train F1: 0.965010\n",
      "epoch: 68, loss: 0.1318\n",
      "Test Loss: 0.864086, Acc: 0.769793, Pre: 0.855022, Rec: 0.847484, F1: 0.845179\n",
      "Train F1: 0.317464\n",
      "Train F1: 0.634904\n",
      "Train F1: 0.968237\n",
      "epoch: 69, loss: 0.1959\n",
      "Test Loss: 0.945268, Acc: 0.768072, Pre: 0.858382, Rec: 0.846557, F1: 0.843218\n",
      "Train F1: 0.318387\n",
      "Train F1: 0.636696\n",
      "Train F1: 0.970029\n",
      "epoch: 70, loss: 0.1103\n",
      "Test Loss: 1.056254, Acc: 0.774527, Pre: 0.874375, Rec: 0.851252, F1: 0.845732\n",
      "Train F1: 0.317879\n",
      "Train F1: 0.635692\n",
      "Train F1: 0.969025\n",
      "epoch: 71, loss: 0.3071\n",
      "Test Loss: 0.957122, Acc: 0.770654, Pre: 0.856818, Rec: 0.848121, F1: 0.845550\n",
      "Train F1: 0.316384\n",
      "Train F1: 0.632681\n",
      "Train F1: 0.966014\n",
      "epoch: 72, loss: 0.2053\n",
      "Test Loss: 0.945187, Acc: 0.776248, Pre: 0.864668, Rec: 0.852028, F1: 0.848666\n",
      "Train F1: 0.320187\n",
      "Train F1: 0.640424\n",
      "Train F1: 0.973758\n",
      "epoch: 73, loss: 0.1192\n",
      "Test Loss: 1.001453, Acc: 0.772806, Pre: 0.868204, Rec: 0.849954, F1: 0.845343\n",
      "Train F1: 0.318766\n",
      "Train F1: 0.637485\n",
      "Train F1: 0.970818\n",
      "epoch: 74, loss: 0.1402\n",
      "Test Loss: 0.972224, Acc: 0.776248, Pre: 0.864097, Rec: 0.852005, F1: 0.848760\n",
      "Train F1: 0.317361\n",
      "Train F1: 0.634760\n",
      "Train F1: 0.968093\n",
      "epoch: 75, loss: 0.2669\n",
      "Test Loss: 1.188825, Acc: 0.768072, Pre: 0.877159, Rec: 0.847150, F1: 0.840084\n",
      "Train F1: 0.317761\n",
      "Train F1: 0.635549\n",
      "Train F1: 0.968882\n",
      "epoch: 76, loss: 0.221\n",
      "Test Loss: 0.963759, Acc: 0.780120, Pre: 0.869568, Rec: 0.854694, F1: 0.850946\n",
      "Train F1: 0.319234\n",
      "Train F1: 0.638489\n",
      "Train F1: 0.971822\n",
      "epoch: 77, loss: 0.1764\n",
      "Test Loss: 1.236463, Acc: 0.766781, Pre: 0.872853, Rec: 0.846206, F1: 0.839725\n",
      "Train F1: 0.317754\n",
      "Train F1: 0.635549\n",
      "Train F1: 0.968882\n",
      "epoch: 78, loss: 0.07934\n",
      "Test Loss: 1.074629, Acc: 0.772375, Pre: 0.869476, Rec: 0.849719, F1: 0.844790\n",
      "Train F1: 0.320892\n",
      "Train F1: 0.641787\n",
      "Train F1: 0.975120\n",
      "epoch: 79, loss: 0.1891\n",
      "Test Loss: 1.022326, Acc: 0.774527, Pre: 0.862504, Rec: 0.850841, F1: 0.847659\n",
      "Train F1: 0.319025\n",
      "Train F1: 0.638058\n",
      "Train F1: 0.971392\n",
      "epoch: 80, loss: 0.1409\n",
      "Test Loss: 1.044852, Acc: 0.773236, Pre: 0.860782, Rec: 0.849945, F1: 0.846923\n",
      "Train F1: 0.317071\n",
      "Train F1: 0.634115\n",
      "Train F1: 0.967448\n",
      "epoch: 81, loss: 0.1246\n",
      "Test Loss: 1.105513, Acc: 0.775818, Pre: 0.870533, Rec: 0.851967, F1: 0.847374\n",
      "Train F1: 0.318410\n",
      "Train F1: 0.636839\n",
      "Train F1: 0.970173\n",
      "epoch: 82, loss: 0.0803\n",
      "Test Loss: 1.064874, Acc: 0.772806, Pre: 0.866806, Rec: 0.849907, F1: 0.845572\n",
      "Train F1: 0.321112\n",
      "Train F1: 0.642217\n",
      "Train F1: 0.975550\n",
      "epoch: 83, loss: 0.06962\n",
      "Test Loss: 1.016100, Acc: 0.778830, Pre: 0.867298, Rec: 0.853781, F1: 0.850286\n",
      "Train F1: 0.322467\n",
      "Train F1: 0.644942\n",
      "Train F1: 0.978275\n",
      "epoch: 84, loss: 0.1225\n",
      "Test Loss: 1.019700, Acc: 0.777108, Pre: 0.865475, Rec: 0.852610, F1: 0.849217\n",
      "Train F1: 0.321123\n",
      "Train F1: 0.642289\n",
      "Train F1: 0.975622\n",
      "epoch: 85, loss: 0.1363\n",
      "Test Loss: 1.063419, Acc: 0.776678, Pre: 0.865071, Rec: 0.852319, F1: 0.848942\n",
      "Train F1: 0.322126\n",
      "Train F1: 0.644296\n",
      "Train F1: 0.977630\n",
      "epoch: 86, loss: 0.1089\n",
      "Test Loss: 1.058017, Acc: 0.774527, Pre: 0.863442, Rec: 0.850881, F1: 0.847502\n",
      "Train F1: 0.322643\n",
      "Train F1: 0.645300\n",
      "Train F1: 0.978633\n",
      "epoch: 87, loss: 0.1147\n",
      "Test Loss: 1.083968, Acc: 0.769363, Pre: 0.855383, Rec: 0.847232, F1: 0.844774\n",
      "Train F1: 0.322159\n",
      "Train F1: 0.644368\n",
      "Train F1: 0.977701\n",
      "epoch: 88, loss: 0.07549\n",
      "Test Loss: 1.104170, Acc: 0.777969, Pre: 0.869209, Rec: 0.853302, F1: 0.849298\n",
      "Train F1: 0.323026\n",
      "Train F1: 0.646089\n",
      "Train F1: 0.979422\n",
      "epoch: 89, loss: 0.1728\n",
      "Test Loss: 1.097524, Acc: 0.776678, Pre: 0.864878, Rec: 0.852311, F1: 0.848973\n",
      "Train F1: 0.322576\n",
      "Train F1: 0.645157\n",
      "Train F1: 0.978490\n",
      "epoch: 90, loss: 0.06409\n",
      "Test Loss: 1.185196, Acc: 0.777539, Pre: 0.871313, Rec: 0.853098, F1: 0.848622\n",
      "Train F1: 0.322377\n",
      "Train F1: 0.644798\n",
      "Train F1: 0.978131\n",
      "epoch: 91, loss: 0.09544\n",
      "Test Loss: 1.073438, Acc: 0.780551, Pre: 0.866776, Rec: 0.854858, F1: 0.851729\n",
      "Train F1: 0.321422\n",
      "Train F1: 0.642934\n",
      "Train F1: 0.976267\n",
      "epoch: 92, loss: 0.04007\n",
      "Test Loss: 1.086326, Acc: 0.783563, Pre: 0.870377, Rec: 0.856926, F1: 0.853537\n",
      "Train F1: 0.322925\n",
      "Train F1: 0.645945\n",
      "Train F1: 0.979278\n",
      "epoch: 93, loss: 0.09351\n",
      "Test Loss: 1.167550, Acc: 0.777539, Pre: 0.866475, Rec: 0.852925, F1: 0.849396\n",
      "Train F1: 0.321336\n",
      "Train F1: 0.642719\n",
      "Train F1: 0.976052\n",
      "epoch: 94, loss: 0.1284\n",
      "Test Loss: 1.150031, Acc: 0.774527, Pre: 0.863252, Rec: 0.850873, F1: 0.847534\n",
      "Train F1: 0.320601\n",
      "Train F1: 0.641285\n",
      "Train F1: 0.974618\n",
      "epoch: 95, loss: 0.183\n",
      "Test Loss: 1.170956, Acc: 0.785284, Pre: 0.873942, Rec: 0.858160, F1: 0.854347\n",
      "Train F1: 0.323466\n",
      "Train F1: 0.647020\n",
      "Train F1: 0.980354\n",
      "epoch: 96, loss: 0.07014\n",
      "Test Loss: 1.125061, Acc: 0.780981, Pre: 0.868735, Rec: 0.855212, F1: 0.851758\n",
      "Train F1: 0.322446\n",
      "Train F1: 0.644941\n",
      "Train F1: 0.978275\n",
      "epoch: 97, loss: 0.1695\n",
      "Test Loss: 1.380786, Acc: 0.772375, Pre: 0.874596, Rec: 0.849869, F1: 0.843968\n",
      "Train F1: 0.323126\n",
      "Train F1: 0.646304\n",
      "Train F1: 0.979637\n",
      "epoch: 98, loss: 0.1036\n",
      "Test Loss: 1.168046, Acc: 0.778830, Pre: 0.867098, Rec: 0.853774, F1: 0.850318\n",
      "Train F1: 0.324766\n",
      "Train F1: 0.649602\n",
      "Train F1: 0.982935\n",
      "epoch: 99, loss: 0.06597\n",
      "Test Loss: 1.261985, Acc: 0.777108, Pre: 0.869710, Rec: 0.852768, F1: 0.848534\n",
      "The  3  th fold cross validation:\n",
      "Train F1: 0.215901\n",
      "Train F1: 0.242187\n",
      "Train F1: 0.242187\n",
      "epoch: 0, loss: 1.557\n",
      "Test Loss: 1.209367, Acc: 0.505594, Pre: 0.502010, Rec: 0.666100, F1: 0.557207\n",
      "Train F1: 0.194602\n",
      "Train F1: 0.407724\n",
      "Train F1: 0.741057\n",
      "epoch: 1, loss: 0.8641\n",
      "Test Loss: 0.859586, Acc: 0.684165, Pre: 0.849121, Rec: 0.791875, F1: 0.772398\n",
      "Train F1: 0.242090\n",
      "Train F1: 0.481793\n",
      "Train F1: 0.815127\n",
      "epoch: 2, loss: 0.6301\n",
      "Test Loss: 0.649301, Acc: 0.671256, Pre: 0.798711, Rec: 0.782371, F1: 0.774352\n",
      "Train F1: 0.258839\n",
      "Train F1: 0.497851\n",
      "Train F1: 0.831184\n",
      "epoch: 3, loss: 0.5036\n",
      "Test Loss: 0.601336, Acc: 0.674699, Pre: 0.845124, Rec: 0.785637, F1: 0.764357\n",
      "Train F1: 0.263145\n",
      "Train F1: 0.501865\n",
      "Train F1: 0.835199\n",
      "epoch: 4, loss: 0.5566\n",
      "Test Loss: 0.571005, Acc: 0.690620, Pre: 0.844304, Rec: 0.796012, F1: 0.779486\n",
      "Train F1: 0.265028\n",
      "Train F1: 0.508306\n",
      "Train F1: 0.841639\n",
      "epoch: 5, loss: 0.5127\n",
      "Test Loss: 0.561337, Acc: 0.698795, Pre: 0.855400, Rec: 0.801519, F1: 0.784547\n",
      "Train F1: 0.267209\n",
      "Train F1: 0.513536\n",
      "Train F1: 0.846869\n",
      "epoch: 6, loss: 0.4265\n",
      "Test Loss: 0.552874, Acc: 0.702238, Pre: 0.860665, Rec: 0.803844, F1: 0.786596\n",
      "Train F1: 0.268736\n",
      "Train F1: 0.518273\n",
      "Train F1: 0.851606\n",
      "epoch: 7, loss: 0.5131\n",
      "Test Loss: 0.531062, Acc: 0.712565, Pre: 0.854993, Rec: 0.810493, F1: 0.797007\n",
      "Train F1: 0.270603\n",
      "Train F1: 0.522579\n",
      "Train F1: 0.855913\n",
      "epoch: 8, loss: 0.3901\n",
      "Test Loss: 0.526707, Acc: 0.714716, Pre: 0.863426, Rec: 0.812034, F1: 0.797224\n",
      "Train F1: 0.270957\n",
      "Train F1: 0.524411\n",
      "Train F1: 0.857745\n",
      "epoch: 9, loss: 0.4238\n",
      "Test Loss: 0.506950, Acc: 0.724613, Pre: 0.850834, Rec: 0.818245, F1: 0.808440\n",
      "Train F1: 0.271127\n",
      "Train F1: 0.525971\n",
      "Train F1: 0.859305\n",
      "epoch: 10, loss: 0.4477\n",
      "Test Loss: 0.498792, Acc: 0.732358, Pre: 0.856426, Rec: 0.823394, F1: 0.813963\n",
      "Train F1: 0.272473\n",
      "Train F1: 0.528933\n",
      "Train F1: 0.862267\n",
      "epoch: 11, loss: 0.4899\n",
      "Test Loss: 0.499848, Acc: 0.728916, Pre: 0.861646, Rec: 0.821262, F1: 0.809982\n",
      "Train F1: 0.272572\n",
      "Train F1: 0.529790\n",
      "Train F1: 0.863123\n",
      "epoch: 12, loss: 0.3868\n",
      "Test Loss: 0.505788, Acc: 0.725043, Pre: 0.866561, Rec: 0.818824, F1: 0.805694\n",
      "Train F1: 0.273541\n",
      "Train F1: 0.532330\n",
      "Train F1: 0.865663\n",
      "epoch: 13, loss: 0.4827\n",
      "Test Loss: 0.504491, Acc: 0.724182, Pre: 0.867636, Rec: 0.818280, F1: 0.804744\n",
      "Train F1: 0.274267\n",
      "Train F1: 0.533737\n",
      "Train F1: 0.867070\n",
      "epoch: 14, loss: 0.476\n",
      "Test Loss: 0.484127, Acc: 0.735800, Pre: 0.866629, Rec: 0.825838, F1: 0.814956\n",
      "Train F1: 0.274141\n",
      "Train F1: 0.533807\n",
      "Train F1: 0.867140\n",
      "epoch: 15, loss: 0.3412\n",
      "Test Loss: 0.500850, Acc: 0.724613, Pre: 0.869668, Rec: 0.818593, F1: 0.804745\n",
      "Train F1: 0.274132\n",
      "Train F1: 0.534441\n",
      "Train F1: 0.867774\n",
      "epoch: 16, loss: 0.3646\n",
      "Test Loss: 0.492649, Acc: 0.728055, Pre: 0.871295, Rec: 0.820866, F1: 0.807443\n",
      "Train F1: 0.275506\n",
      "Train F1: 0.537480\n",
      "Train F1: 0.870813\n",
      "epoch: 17, loss: 0.4364\n",
      "Test Loss: 0.451672, Acc: 0.781842, Pre: 0.868575, Rec: 0.855681, F1: 0.852434\n",
      "Train F1: 0.275467\n",
      "Train F1: 0.537856\n",
      "Train F1: 0.871190\n",
      "epoch: 18, loss: 0.3026\n",
      "Test Loss: 0.446698, Acc: 0.782702, Pre: 0.873226, Rec: 0.856396, F1: 0.852388\n",
      "Train F1: 0.275905\n",
      "Train F1: 0.538968\n",
      "Train F1: 0.872302\n",
      "epoch: 19, loss: 0.3614\n",
      "Test Loss: 0.444806, Acc: 0.775387, Pre: 0.877056, Rec: 0.851781, F1: 0.845965\n",
      "Train F1: 0.277041\n",
      "Train F1: 0.542298\n",
      "Train F1: 0.875631\n",
      "epoch: 20, loss: 0.3852\n",
      "Test Loss: 0.440864, Acc: 0.772375, Pre: 0.880546, Rec: 0.849910, F1: 0.843000\n",
      "Train F1: 0.277761\n",
      "Train F1: 0.544758\n",
      "Train F1: 0.878092\n",
      "epoch: 21, loss: 0.4253\n",
      "Test Loss: 0.450372, Acc: 0.760327, Pre: 0.874836, Rec: 0.841958, F1: 0.834080\n",
      "Train F1: 0.279047\n",
      "Train F1: 0.547056\n",
      "Train F1: 0.880390\n",
      "epoch: 22, loss: 0.3163\n",
      "Test Loss: 0.418336, Acc: 0.789587, Pre: 0.887663, Rec: 0.861246, F1: 0.855724\n",
      "Train F1: 0.278732\n",
      "Train F1: 0.546675\n",
      "Train F1: 0.880008\n",
      "epoch: 23, loss: 0.3313\n",
      "Test Loss: 0.409386, Acc: 0.796041, Pre: 0.887132, Rec: 0.865412, F1: 0.860872\n",
      "Train F1: 0.279944\n",
      "Train F1: 0.550466\n",
      "Train F1: 0.883799\n",
      "epoch: 24, loss: 0.4182\n",
      "Test Loss: 0.413910, Acc: 0.779690, Pre: 0.888522, Rec: 0.854836, F1: 0.847718\n",
      "Train F1: 0.281131\n",
      "Train F1: 0.552982\n",
      "Train F1: 0.886315\n",
      "epoch: 25, loss: 0.4207\n",
      "Test Loss: 0.383997, Acc: 0.825301, Pre: 0.892522, Rec: 0.884389, F1: 0.882612\n",
      "Train F1: 0.282156\n",
      "Train F1: 0.555475\n",
      "Train F1: 0.888808\n",
      "epoch: 26, loss: 0.3745\n",
      "Test Loss: 0.385393, Acc: 0.817556, Pre: 0.897148, Rec: 0.879595, F1: 0.876225\n",
      "Train F1: 0.281618\n",
      "Train F1: 0.554828\n",
      "Train F1: 0.888161\n",
      "epoch: 27, loss: 0.3307\n",
      "Test Loss: 0.375636, Acc: 0.828313, Pre: 0.896950, Rec: 0.886497, F1: 0.884380\n",
      "Train F1: 0.282717\n",
      "Train F1: 0.557459\n",
      "Train F1: 0.890792\n",
      "epoch: 28, loss: 0.3764\n",
      "Test Loss: 0.364404, Acc: 0.840361, Pre: 0.900248, Rec: 0.894301, F1: 0.892995\n",
      "Train F1: 0.284205\n",
      "Train F1: 0.560791\n",
      "Train F1: 0.894125\n",
      "epoch: 29, loss: 0.3975\n",
      "Test Loss: 0.364544, Acc: 0.847246, Pre: 0.901109, Rec: 0.898647, F1: 0.897956\n",
      "Train F1: 0.284419\n",
      "Train F1: 0.561716\n",
      "Train F1: 0.895050\n",
      "epoch: 30, loss: 0.3611\n",
      "Test Loss: 0.386618, Acc: 0.812823, Pre: 0.894342, Rec: 0.876457, F1: 0.872940\n",
      "Train F1: 0.284804\n",
      "Train F1: 0.562421\n",
      "Train F1: 0.895755\n",
      "epoch: 31, loss: 0.3465\n",
      "Test Loss: 0.353679, Acc: 0.841652, Pre: 0.904875, Rec: 0.895336, F1: 0.893498\n",
      "Train F1: 0.284351\n",
      "Train F1: 0.561655\n",
      "Train F1: 0.894988\n",
      "epoch: 32, loss: 0.2843\n",
      "Test Loss: 0.342214, Acc: 0.852840, Pre: 0.909453, Rec: 0.902655, F1: 0.901301\n",
      "Train F1: 0.286667\n",
      "Train F1: 0.566954\n",
      "Train F1: 0.900287\n",
      "epoch: 33, loss: 0.4564\n",
      "Test Loss: 0.338719, Acc: 0.852840, Pre: 0.911922, Rec: 0.902766, F1: 0.901086\n",
      "Train F1: 0.287729\n",
      "Train F1: 0.569344\n",
      "Train F1: 0.902678\n",
      "epoch: 34, loss: 0.3159\n",
      "Test Loss: 0.339137, Acc: 0.860585, Pre: 0.909029, Rec: 0.907449, F1: 0.906947\n",
      "Train F1: 0.288140\n",
      "Train F1: 0.570532\n",
      "Train F1: 0.903865\n",
      "epoch: 35, loss: 0.4203\n",
      "Test Loss: 0.333912, Acc: 0.862306, Pre: 0.910034, Rec: 0.908582, F1: 0.908106\n",
      "Train F1: 0.287151\n",
      "Train F1: 0.568931\n",
      "Train F1: 0.902264\n",
      "epoch: 36, loss: 0.2742\n",
      "Test Loss: 0.330370, Acc: 0.864458, Pre: 0.917659, Rec: 0.910412, F1: 0.909075\n",
      "Train F1: 0.287566\n",
      "Train F1: 0.569632\n",
      "Train F1: 0.902965\n",
      "epoch: 37, loss: 0.2737\n",
      "Test Loss: 0.341009, Acc: 0.872633, Pre: 0.915168, Rec: 0.915178, F1: 0.915089\n",
      "Train F1: 0.286746\n",
      "Train F1: 0.568388\n",
      "Train F1: 0.901721\n",
      "epoch: 38, loss: 0.4236\n",
      "Test Loss: 0.322659, Acc: 0.865749, Pre: 0.914786, Rec: 0.911069, F1: 0.910227\n",
      "Train F1: 0.289563\n",
      "Train F1: 0.573877\n",
      "Train F1: 0.907210\n",
      "epoch: 39, loss: 0.3042\n",
      "Test Loss: 0.320880, Acc: 0.868330, Pre: 0.918582, Rec: 0.912909, F1: 0.911802\n",
      "Train F1: 0.289362\n",
      "Train F1: 0.574085\n",
      "Train F1: 0.907418\n",
      "epoch: 40, loss: 0.2928\n",
      "Test Loss: 0.325005, Acc: 0.860155, Pre: 0.917262, Rec: 0.907654, F1: 0.905980\n",
      "Train F1: 0.289754\n",
      "Train F1: 0.574951\n",
      "Train F1: 0.908284\n",
      "epoch: 41, loss: 0.271\n",
      "Test Loss: 0.327951, Acc: 0.873064, Pre: 0.917066, Rec: 0.915736, F1: 0.915297\n",
      "Train F1: 0.290451\n",
      "Train F1: 0.576306\n",
      "Train F1: 0.909639\n",
      "epoch: 42, loss: 0.2813\n",
      "Test Loss: 0.316910, Acc: 0.866179, Pre: 0.919663, Rec: 0.911597, F1: 0.910166\n",
      "Train F1: 0.289647\n",
      "Train F1: 0.574736\n",
      "Train F1: 0.908070\n",
      "epoch: 43, loss: 0.254\n",
      "Test Loss: 0.306503, Acc: 0.874355, Pre: 0.919627, Rec: 0.916741, F1: 0.916048\n",
      "Train F1: 0.292350\n",
      "Train F1: 0.580414\n",
      "Train F1: 0.913747\n",
      "epoch: 44, loss: 0.3331\n",
      "Test Loss: 0.316304, Acc: 0.876936, Pre: 0.922243, Rec: 0.918522, F1: 0.917716\n",
      "Train F1: 0.292763\n",
      "Train F1: 0.581592\n",
      "Train F1: 0.914925\n",
      "epoch: 45, loss: 0.3389\n",
      "Test Loss: 0.315000, Acc: 0.868330, Pre: 0.920892, Rec: 0.913020, F1: 0.911630\n",
      "Train F1: 0.291813\n",
      "Train F1: 0.579736\n",
      "Train F1: 0.913069\n",
      "epoch: 46, loss: 0.395\n",
      "Test Loss: 0.314721, Acc: 0.887263, Pre: 0.927450, Rec: 0.925282, F1: 0.924724\n",
      "Train F1: 0.293569\n",
      "Train F1: 0.583083\n",
      "Train F1: 0.916416\n",
      "epoch: 47, loss: 0.2833\n",
      "Test Loss: 0.314215, Acc: 0.877797, Pre: 0.923678, Rec: 0.919147, F1: 0.918235\n",
      "Train F1: 0.292785\n",
      "Train F1: 0.582058\n",
      "Train F1: 0.915392\n",
      "epoch: 48, loss: 0.2254\n",
      "Test Loss: 0.340913, Acc: 0.848107, Pre: 0.915534, Rec: 0.899858, F1: 0.897278\n",
      "Train F1: 0.293064\n",
      "Train F1: 0.582554\n",
      "Train F1: 0.915887\n",
      "epoch: 49, loss: 0.3016\n",
      "Test Loss: 0.299139, Acc: 0.894578, Pre: 0.932142, Rec: 0.930140, F1: 0.929620\n",
      "Train F1: 0.293215\n",
      "Train F1: 0.583151\n",
      "Train F1: 0.916484\n",
      "epoch: 50, loss: 0.2951\n",
      "Test Loss: 0.314794, Acc: 0.868761, Pre: 0.920295, Rec: 0.913267, F1: 0.911985\n",
      "Train F1: 0.294965\n",
      "Train F1: 0.586659\n",
      "Train F1: 0.919993\n",
      "epoch: 51, loss: 0.2369\n",
      "Test Loss: 0.304418, Acc: 0.888985, Pre: 0.928888, Rec: 0.926451, F1: 0.925858\n",
      "Train F1: 0.295875\n",
      "Train F1: 0.588680\n",
      "Train F1: 0.922013\n",
      "epoch: 52, loss: 0.2749\n",
      "Test Loss: 0.300526, Acc: 0.870482, Pre: 0.922798, Rec: 0.914474, F1: 0.913042\n",
      "Train F1: 0.294691\n",
      "Train F1: 0.586395\n",
      "Train F1: 0.919728\n",
      "epoch: 53, loss: 0.3517\n",
      "Test Loss: 0.306249, Acc: 0.872633, Pre: 0.923354, Rec: 0.915867, F1: 0.914553\n",
      "Train F1: 0.295222\n",
      "Train F1: 0.587392\n",
      "Train F1: 0.920726\n",
      "epoch: 54, loss: 0.1649\n",
      "Test Loss: 0.325084, Acc: 0.856282, Pre: 0.919815, Rec: 0.905261, F1: 0.902936\n",
      "Train F1: 0.296128\n",
      "Train F1: 0.589180\n",
      "Train F1: 0.922514\n",
      "epoch: 55, loss: 0.2736\n",
      "Test Loss: 0.296649, Acc: 0.876506, Pre: 0.923527, Rec: 0.918327, F1: 0.917322\n",
      "Train F1: 0.297075\n",
      "Train F1: 0.591657\n",
      "Train F1: 0.924991\n",
      "epoch: 56, loss: 0.2287\n",
      "Test Loss: 0.297158, Acc: 0.874355, Pre: 0.925730, Rec: 0.917067, F1: 0.915624\n",
      "Train F1: 0.295865\n",
      "Train F1: 0.589144\n",
      "Train F1: 0.922478\n",
      "epoch: 57, loss: 0.2392\n",
      "Test Loss: 0.305535, Acc: 0.877367, Pre: 0.926037, Rec: 0.918997, F1: 0.917767\n",
      "Train F1: 0.296603\n",
      "Train F1: 0.590265\n",
      "Train F1: 0.923598\n",
      "epoch: 58, loss: 0.2625\n",
      "Test Loss: 0.299987, Acc: 0.887263, Pre: 0.926201, Rec: 0.925163, F1: 0.924792\n",
      "Train F1: 0.298136\n",
      "Train F1: 0.593731\n",
      "Train F1: 0.927064\n",
      "epoch: 59, loss: 0.1935\n",
      "Test Loss: 0.314783, Acc: 0.866609, Pre: 0.919875, Rec: 0.911880, F1: 0.910462\n",
      "Train F1: 0.298107\n",
      "Train F1: 0.593735\n",
      "Train F1: 0.927069\n",
      "epoch: 60, loss: 0.2372\n",
      "Test Loss: 0.295688, Acc: 0.877367, Pre: 0.919421, Rec: 0.918546, F1: 0.918199\n",
      "Train F1: 0.299218\n",
      "Train F1: 0.596262\n",
      "Train F1: 0.929595\n",
      "epoch: 61, loss: 0.2396\n",
      "Test Loss: 0.287280, Acc: 0.887694, Pre: 0.925477, Rec: 0.925298, F1: 0.925124\n",
      "Train F1: 0.299955\n",
      "Train F1: 0.597844\n",
      "Train F1: 0.931178\n",
      "epoch: 62, loss: 0.1767\n",
      "Test Loss: 0.292565, Acc: 0.886833, Pre: 0.930009, Rec: 0.925184, F1: 0.924269\n",
      "Train F1: 0.300456\n",
      "Train F1: 0.598847\n",
      "Train F1: 0.932181\n",
      "epoch: 63, loss: 0.1787\n",
      "Test Loss: 0.308565, Acc: 0.880809, Pre: 0.928236, Rec: 0.921285, F1: 0.920086\n",
      "Train F1: 0.301665\n",
      "Train F1: 0.601588\n",
      "Train F1: 0.934921\n",
      "epoch: 64, loss: 0.2293\n",
      "Test Loss: 0.288403, Acc: 0.895439, Pre: 0.935070, Rec: 0.930877, F1: 0.930070\n",
      "Train F1: 0.301661\n",
      "Train F1: 0.601815\n",
      "Train F1: 0.935148\n",
      "epoch: 65, loss: 0.3257\n",
      "Test Loss: 0.275267, Acc: 0.889845, Pre: 0.929186, Rec: 0.927003, F1: 0.926448\n",
      "Train F1: 0.300276\n",
      "Train F1: 0.598717\n",
      "Train F1: 0.932051\n",
      "epoch: 66, loss: 0.3045\n",
      "Test Loss: 0.326311, Acc: 0.868761, Pre: 0.926260, Rec: 0.913504, F1: 0.911540\n",
      "Train F1: 0.300809\n",
      "Train F1: 0.600019\n",
      "Train F1: 0.933352\n",
      "epoch: 67, loss: 0.2836\n",
      "Test Loss: 0.286165, Acc: 0.895009, Pre: 0.930125, Rec: 0.930113, F1: 0.930006\n",
      "Train F1: 0.302559\n",
      "Train F1: 0.603606\n",
      "Train F1: 0.936939\n",
      "epoch: 68, loss: 0.2819\n",
      "Test Loss: 0.279189, Acc: 0.885542, Pre: 0.925943, Rec: 0.924104, F1: 0.923595\n",
      "Train F1: 0.302356\n",
      "Train F1: 0.603175\n",
      "Train F1: 0.936508\n",
      "epoch: 69, loss: 0.1897\n",
      "Test Loss: 0.300174, Acc: 0.881670, Pre: 0.924935, Rec: 0.921644, F1: 0.920913\n",
      "Train F1: 0.302438\n",
      "Train F1: 0.603392\n",
      "Train F1: 0.936726\n",
      "epoch: 70, loss: 0.1697\n",
      "Test Loss: 0.313797, Acc: 0.878657, Pre: 0.927001, Rec: 0.919862, F1: 0.918628\n",
      "Train F1: 0.303952\n",
      "Train F1: 0.606401\n",
      "Train F1: 0.939734\n",
      "epoch: 71, loss: 0.1892\n",
      "Test Loss: 0.296964, Acc: 0.882960, Pre: 0.923386, Rec: 0.922301, F1: 0.921918\n",
      "Train F1: 0.303394\n",
      "Train F1: 0.605553\n",
      "Train F1: 0.938887\n",
      "epoch: 72, loss: 0.2173\n",
      "Test Loss: 0.280925, Acc: 0.888124, Pre: 0.925429, Rec: 0.925470, F1: 0.925414\n",
      "Train F1: 0.305406\n",
      "Train F1: 0.609641\n",
      "Train F1: 0.942974\n",
      "epoch: 73, loss: 0.2669\n",
      "Test Loss: 0.298942, Acc: 0.896730, Pre: 0.932968, Rec: 0.931519, F1: 0.931087\n",
      "Train F1: 0.304232\n",
      "Train F1: 0.607275\n",
      "Train F1: 0.940609\n",
      "epoch: 74, loss: 0.2151\n",
      "Test Loss: 0.299906, Acc: 0.884251, Pre: 0.927134, Rec: 0.923395, F1: 0.922610\n",
      "Train F1: 0.305716\n",
      "Train F1: 0.610212\n",
      "Train F1: 0.943546\n",
      "epoch: 75, loss: 0.1408\n",
      "Test Loss: 0.301724, Acc: 0.891136, Pre: 0.927553, Rec: 0.927534, F1: 0.927424\n",
      "Train F1: 0.305456\n",
      "Train F1: 0.610009\n",
      "Train F1: 0.943343\n",
      "epoch: 76, loss: 0.1484\n",
      "Test Loss: 0.332385, Acc: 0.882100, Pre: 0.929856, Rec: 0.922179, F1: 0.920904\n",
      "Train F1: 0.306117\n",
      "Train F1: 0.611373\n",
      "Train F1: 0.944706\n",
      "epoch: 77, loss: 0.1001\n",
      "Test Loss: 0.329844, Acc: 0.874785, Pre: 0.924587, Rec: 0.917291, F1: 0.916014\n",
      "Train F1: 0.306127\n",
      "Train F1: 0.611223\n",
      "Train F1: 0.944557\n",
      "epoch: 78, loss: 0.1477\n",
      "Test Loss: 0.331445, Acc: 0.875215, Pre: 0.918472, Rec: 0.917167, F1: 0.916735\n",
      "Train F1: 0.306027\n",
      "Train F1: 0.611304\n",
      "Train F1: 0.944637\n",
      "epoch: 79, loss: 0.2257\n",
      "Test Loss: 0.352095, Acc: 0.871343, Pre: 0.925595, Rec: 0.915136, F1: 0.913460\n",
      "Train F1: 0.306061\n",
      "Train F1: 0.611154\n",
      "Train F1: 0.944487\n",
      "epoch: 80, loss: 0.3391\n",
      "Test Loss: 0.312975, Acc: 0.867470, Pre: 0.911743, Rec: 0.911743, F1: 0.911647\n",
      "Train F1: 0.307349\n",
      "Train F1: 0.614030\n",
      "Train F1: 0.947364\n",
      "epoch: 81, loss: 0.1949\n",
      "Test Loss: 0.292187, Acc: 0.888554, Pre: 0.927759, Rec: 0.926094, F1: 0.925617\n",
      "Train F1: 0.307051\n",
      "Train F1: 0.613310\n",
      "Train F1: 0.946643\n",
      "epoch: 82, loss: 0.2388\n",
      "Test Loss: 0.299315, Acc: 0.891136, Pre: 0.933343, Rec: 0.928075, F1: 0.927124\n",
      "Train F1: 0.307420\n",
      "Train F1: 0.614174\n",
      "Train F1: 0.947507\n",
      "epoch: 83, loss: 0.1984\n",
      "Test Loss: 0.309921, Acc: 0.882530, Pre: 0.924497, Rec: 0.922144, F1: 0.921550\n",
      "Train F1: 0.309296\n",
      "Train F1: 0.618048\n",
      "Train F1: 0.951381\n",
      "epoch: 84, loss: 0.2065\n",
      "Test Loss: 0.335218, Acc: 0.867040, Pre: 0.912044, Rec: 0.911593, F1: 0.911337\n",
      "Train F1: 0.307812\n",
      "Train F1: 0.614889\n",
      "Train F1: 0.948222\n",
      "epoch: 85, loss: 0.1609\n",
      "Test Loss: 0.313874, Acc: 0.882960, Pre: 0.922335, Rec: 0.922146, F1: 0.921968\n",
      "Train F1: 0.308550\n",
      "Train F1: 0.616615\n",
      "Train F1: 0.949949\n",
      "epoch: 86, loss: 0.1756\n",
      "Test Loss: 0.295632, Acc: 0.896299, Pre: 0.933762, Rec: 0.931325, F1: 0.930746\n",
      "Train F1: 0.309321\n",
      "Train F1: 0.618047\n",
      "Train F1: 0.951380\n",
      "epoch: 87, loss: 0.1917\n",
      "Test Loss: 0.293723, Acc: 0.898021, Pre: 0.934455, Rec: 0.932436, F1: 0.931918\n",
      "Train F1: 0.308303\n",
      "Train F1: 0.616040\n",
      "Train F1: 0.949373\n",
      "epoch: 88, loss: 0.1677\n",
      "Test Loss: 0.360565, Acc: 0.873494, Pre: 0.921014, Rec: 0.916293, F1: 0.915338\n",
      "Train F1: 0.308774\n",
      "Train F1: 0.616825\n",
      "Train F1: 0.950158\n",
      "epoch: 89, loss: 0.2433\n",
      "Test Loss: 0.331617, Acc: 0.866179, Pre: 0.911291, Rec: 0.910590, F1: 0.910683\n",
      "Train F1: 0.311678\n",
      "Train F1: 0.622852\n",
      "Train F1: 0.956185\n",
      "epoch: 90, loss: 0.1275\n",
      "Test Loss: 0.316334, Acc: 0.885112, Pre: 0.927371, Rec: 0.923947, F1: 0.923207\n",
      "Train F1: 0.312052\n",
      "Train F1: 0.623642\n",
      "Train F1: 0.956975\n",
      "epoch: 91, loss: 0.1863\n",
      "Test Loss: 0.325763, Acc: 0.888985, Pre: 0.931404, Rec: 0.926614, F1: 0.925712\n",
      "Train F1: 0.309501\n",
      "Train F1: 0.618552\n",
      "Train F1: 0.951885\n",
      "epoch: 92, loss: 0.1386\n",
      "Test Loss: 0.340046, Acc: 0.880809, Pre: 0.926050, Rec: 0.921174, F1: 0.920229\n",
      "Train F1: 0.312039\n",
      "Train F1: 0.623642\n",
      "Train F1: 0.956976\n",
      "epoch: 93, loss: 0.1259\n",
      "Test Loss: 0.325622, Acc: 0.884682, Pre: 0.924473, Rec: 0.923442, F1: 0.923070\n",
      "Train F1: 0.312149\n",
      "Train F1: 0.623930\n",
      "Train F1: 0.957264\n",
      "epoch: 94, loss: 0.236\n",
      "Test Loss: 0.356432, Acc: 0.881239, Pre: 0.926680, Rec: 0.921480, F1: 0.920495\n",
      "Train F1: 0.311476\n",
      "Train F1: 0.622568\n",
      "Train F1: 0.955901\n",
      "epoch: 95, loss: 0.09116\n",
      "Test Loss: 0.340412, Acc: 0.885972, Pre: 0.927515, Rec: 0.924491, F1: 0.923808\n",
      "Train F1: 0.310750\n",
      "Train F1: 0.621207\n",
      "Train F1: 0.954540\n",
      "epoch: 96, loss: 0.2984\n",
      "Test Loss: 0.404370, Acc: 0.868330, Pre: 0.924391, Rec: 0.913161, F1: 0.911367\n",
      "Train F1: 0.309666\n",
      "Train F1: 0.618911\n",
      "Train F1: 0.952244\n",
      "epoch: 97, loss: 0.1185\n",
      "Test Loss: 0.316013, Acc: 0.873924, Pre: 0.917240, Rec: 0.916265, F1: 0.915895\n",
      "Train F1: 0.312011\n",
      "Train F1: 0.623643\n",
      "Train F1: 0.956977\n",
      "epoch: 98, loss: 0.1166\n",
      "Test Loss: 0.322199, Acc: 0.888985, Pre: 0.930276, Rec: 0.926548, F1: 0.925778\n",
      "Train F1: 0.314547\n",
      "Train F1: 0.628880\n",
      "Train F1: 0.962213\n",
      "epoch: 99, loss: 0.1186\n",
      "Test Loss: 0.330014, Acc: 0.893287, Pre: 0.933676, Rec: 0.929446, F1: 0.928628\n",
      "The  4  th fold cross validation:\n",
      "Train F1: 0.216480\n",
      "Train F1: 0.246124\n",
      "Train F1: 0.246124\n",
      "epoch: 0, loss: 1.334\n",
      "Test Loss: 1.203534, Acc: 0.630809, Pre: 0.819422, Rec: 0.756646, F1: 0.726929\n",
      "Train F1: 0.190172\n",
      "Train F1: 0.395510\n",
      "Train F1: 0.728843\n",
      "epoch: 1, loss: 0.7584\n",
      "Test Loss: 0.733759, Acc: 0.506454, Pre: 0.835485, Rec: 0.666667, F1: 0.557460\n",
      "Train F1: 0.210294\n",
      "Train F1: 0.430104\n",
      "Train F1: 0.763437\n",
      "epoch: 2, loss: 0.5713\n",
      "Test Loss: 0.609668, Acc: 0.776248, Pre: 0.858185, Rec: 0.851659, F1: 0.849719\n",
      "Train F1: 0.250813\n",
      "Train F1: 0.460050\n",
      "Train F1: 0.793383\n",
      "epoch: 3, loss: 0.5473\n",
      "Test Loss: 0.559576, Acc: 0.789157, Pre: 0.863819, Rec: 0.860066, F1: 0.858881\n",
      "Train F1: 0.250802\n",
      "Train F1: 0.464354\n",
      "Train F1: 0.797687\n",
      "epoch: 4, loss: 0.5617\n",
      "Test Loss: 0.515514, Acc: 0.836059, Pre: 0.891676, Rec: 0.890987, F1: 0.890653\n",
      "Train F1: 0.251354\n",
      "Train F1: 0.467179\n",
      "Train F1: 0.800512\n",
      "epoch: 5, loss: 0.4979\n",
      "Test Loss: 0.464764, Acc: 0.882530, Pre: 0.922146, Rec: 0.921507, F1: 0.921605\n",
      "Train F1: 0.253770\n",
      "Train F1: 0.475013\n",
      "Train F1: 0.808347\n",
      "epoch: 6, loss: 0.5303\n",
      "Test Loss: 0.405148, Acc: 0.909208, Pre: 0.945070, Rec: 0.938873, F1: 0.939069\n",
      "Train F1: 0.255731\n",
      "Train F1: 0.481796\n",
      "Train F1: 0.815129\n",
      "epoch: 7, loss: 0.5044\n",
      "Test Loss: 0.364808, Acc: 0.917384, Pre: 0.951610, Rec: 0.944276, F1: 0.944503\n",
      "Train F1: 0.258692\n",
      "Train F1: 0.491963\n",
      "Train F1: 0.825296\n",
      "epoch: 8, loss: 0.443\n",
      "Test Loss: 0.303952, Acc: 0.942341, Pre: 0.965089, Rec: 0.961110, F1: 0.961400\n",
      "Train F1: 0.261237\n",
      "Train F1: 0.499350\n",
      "Train F1: 0.832683\n",
      "epoch: 9, loss: 0.5205\n",
      "Test Loss: 0.323322, Acc: 0.923408, Pre: 0.955423, Rec: 0.948308, F1: 0.948565\n",
      "Train F1: 0.263795\n",
      "Train F1: 0.508853\n",
      "Train F1: 0.842186\n",
      "epoch: 10, loss: 0.4848\n",
      "Test Loss: 0.312755, Acc: 0.929002, Pre: 0.958423, Rec: 0.952078, F1: 0.952359\n",
      "Train F1: 0.264409\n",
      "Train F1: 0.511514\n",
      "Train F1: 0.844847\n",
      "epoch: 11, loss: 0.4423\n",
      "Test Loss: 0.351447, Acc: 0.907487, Pre: 0.948324, Rec: 0.937526, F1: 0.937643\n",
      "Train F1: 0.265885\n",
      "Train F1: 0.516741\n",
      "Train F1: 0.850075\n",
      "epoch: 12, loss: 0.4116\n",
      "Test Loss: 0.274676, Acc: 0.939329, Pre: 0.963825, Rec: 0.959053, F1: 0.959353\n",
      "Train F1: 0.268164\n",
      "Train F1: 0.523215\n",
      "Train F1: 0.856548\n",
      "epoch: 13, loss: 0.5032\n",
      "Test Loss: 0.286227, Acc: 0.930723, Pre: 0.959596, Rec: 0.953226, F1: 0.953514\n",
      "Train F1: 0.268753\n",
      "Train F1: 0.525381\n",
      "Train F1: 0.858714\n",
      "epoch: 14, loss: 0.3691\n",
      "Test Loss: 0.313413, Acc: 0.912651, Pre: 0.950785, Rec: 0.941013, F1: 0.941186\n",
      "Train F1: 0.268946\n",
      "Train F1: 0.527344\n",
      "Train F1: 0.860677\n",
      "epoch: 15, loss: 0.5621\n",
      "Test Loss: 0.279799, Acc: 0.919105, Pre: 0.953423, Rec: 0.945394, F1: 0.945626\n",
      "Train F1: 0.270368\n",
      "Train F1: 0.531365\n",
      "Train F1: 0.864698\n",
      "epoch: 16, loss: 0.3744\n",
      "Test Loss: 0.262014, Acc: 0.924699, Pre: 0.956229, Rec: 0.949172, F1: 0.949436\n",
      "Train F1: 0.271142\n",
      "Train F1: 0.533351\n",
      "Train F1: 0.866685\n",
      "epoch: 17, loss: 0.4074\n",
      "Test Loss: 0.269004, Acc: 0.919105, Pre: 0.953423, Rec: 0.945394, F1: 0.945626\n",
      "Train F1: 0.271738\n",
      "Train F1: 0.535955\n",
      "Train F1: 0.869288\n",
      "epoch: 18, loss: 0.3274\n",
      "Test Loss: 0.300239, Acc: 0.897590, Pre: 0.942511, Rec: 0.930886, F1: 0.930902\n",
      "Train F1: 0.272313\n",
      "Train F1: 0.537661\n",
      "Train F1: 0.870994\n",
      "epoch: 19, loss: 0.441\n",
      "Test Loss: 0.272007, Acc: 0.909639, Pre: 0.947415, Rec: 0.939060, F1: 0.939234\n",
      "Train F1: 0.273827\n",
      "Train F1: 0.541459\n",
      "Train F1: 0.874792\n",
      "epoch: 20, loss: 0.4656\n",
      "Test Loss: 0.278685, Acc: 0.906196, Pre: 0.945718, Rec: 0.936735, F1: 0.936878\n",
      "Train F1: 0.273344\n",
      "Train F1: 0.540928\n",
      "Train F1: 0.874261\n",
      "epoch: 21, loss: 0.3639\n",
      "Test Loss: 0.292526, Acc: 0.892427, Pre: 0.937254, Rec: 0.927510, F1: 0.927537\n",
      "Train F1: 0.275754\n",
      "Train F1: 0.545719\n",
      "Train F1: 0.879052\n",
      "epoch: 22, loss: 0.3743\n",
      "Test Loss: 0.285682, Acc: 0.896730, Pre: 0.937922, Rec: 0.930482, F1: 0.930597\n",
      "Train F1: 0.275362\n",
      "Train F1: 0.546028\n",
      "Train F1: 0.879361\n",
      "epoch: 23, loss: 0.3945\n",
      "Test Loss: 0.249484, Acc: 0.917814, Pre: 0.947857, Rec: 0.944804, F1: 0.945018\n",
      "Train F1: 0.276396\n",
      "Train F1: 0.548337\n",
      "Train F1: 0.881671\n",
      "epoch: 24, loss: 0.2584\n",
      "Test Loss: 0.221744, Acc: 0.925559, Pre: 0.952442, Rec: 0.950020, F1: 0.950234\n",
      "Train F1: 0.278598\n",
      "Train F1: 0.552390\n",
      "Train F1: 0.885723\n",
      "epoch: 25, loss: 0.3317\n",
      "Test Loss: 0.261506, Acc: 0.904905, Pre: 0.939947, Rec: 0.936137, F1: 0.936328\n",
      "Train F1: 0.278807\n",
      "Train F1: 0.553132\n",
      "Train F1: 0.886465\n",
      "epoch: 26, loss: 0.5193\n",
      "Test Loss: 0.371485, Acc: 0.826592, Pre: 0.905493, Rec: 0.883113, F1: 0.881372\n",
      "Train F1: 0.279454\n",
      "Train F1: 0.555116\n",
      "Train F1: 0.888449\n",
      "epoch: 27, loss: 0.4224\n",
      "Test Loss: 0.312762, Acc: 0.867900, Pre: 0.918658, Rec: 0.911234, F1: 0.911173\n",
      "Train F1: 0.280207\n",
      "Train F1: 0.557079\n",
      "Train F1: 0.890413\n",
      "epoch: 28, loss: 0.4425\n",
      "Test Loss: 0.288471, Acc: 0.887694, Pre: 0.928834, Rec: 0.924624, F1: 0.924759\n",
      "Train F1: 0.281639\n",
      "Train F1: 0.560021\n",
      "Train F1: 0.893354\n",
      "epoch: 29, loss: 0.4093\n",
      "Test Loss: 0.278055, Acc: 0.891566, Pre: 0.929394, Rec: 0.927373, F1: 0.927524\n",
      "Train F1: 0.281679\n",
      "Train F1: 0.560092\n",
      "Train F1: 0.893425\n",
      "epoch: 30, loss: 0.3368\n",
      "Test Loss: 0.309530, Acc: 0.881239, Pre: 0.921821, Rec: 0.920561, F1: 0.920685\n",
      "Train F1: 0.283281\n",
      "Train F1: 0.563317\n",
      "Train F1: 0.896650\n",
      "epoch: 31, loss: 0.3206\n",
      "Test Loss: 0.382002, Acc: 0.829174, Pre: 0.892052, Rec: 0.885412, F1: 0.885133\n",
      "Train F1: 0.283951\n",
      "Train F1: 0.565663\n",
      "Train F1: 0.898997\n",
      "epoch: 32, loss: 0.3064\n",
      "Test Loss: 0.334002, Acc: 0.864028, Pre: 0.910396, Rec: 0.909070, F1: 0.909177\n",
      "Train F1: 0.284607\n",
      "Train F1: 0.566500\n",
      "Train F1: 0.899833\n",
      "epoch: 33, loss: 0.4267\n",
      "Test Loss: 0.347738, Acc: 0.848537, Pre: 0.900680, Rec: 0.898660, F1: 0.898736\n",
      "Train F1: 0.285694\n",
      "Train F1: 0.568799\n",
      "Train F1: 0.902133\n",
      "epoch: 34, loss: 0.315\n",
      "Test Loss: 0.334308, Acc: 0.859294, Pre: 0.907039, Rec: 0.905940, F1: 0.906039\n",
      "Train F1: 0.286531\n",
      "Train F1: 0.570370\n",
      "Train F1: 0.903703\n",
      "epoch: 35, loss: 0.3404\n",
      "Test Loss: 0.438624, Acc: 0.812392, Pre: 0.876494, Rec: 0.874544, F1: 0.874548\n",
      "Train F1: 0.287586\n",
      "Train F1: 0.572975\n",
      "Train F1: 0.906308\n",
      "epoch: 36, loss: 0.47\n",
      "Test Loss: 0.419570, Acc: 0.820138, Pre: 0.880666, Rec: 0.879857, F1: 0.879920\n",
      "Train F1: 0.288638\n",
      "Train F1: 0.575501\n",
      "Train F1: 0.908835\n",
      "epoch: 37, loss: 0.3716\n",
      "Test Loss: 0.371471, Acc: 0.848967, Pre: 0.899460, Rec: 0.899195, F1: 0.899251\n",
      "Train F1: 0.288790\n",
      "Train F1: 0.575862\n",
      "Train F1: 0.909195\n",
      "epoch: 38, loss: 0.2949\n",
      "Test Loss: 0.444168, Acc: 0.830034, Pre: 0.887976, Rec: 0.887015, F1: 0.886606\n",
      "Train F1: 0.289687\n",
      "Train F1: 0.577504\n",
      "Train F1: 0.910838\n",
      "epoch: 39, loss: 0.5661\n",
      "Test Loss: 0.467694, Acc: 0.806368, Pre: 0.872259, Rec: 0.870550, F1: 0.870555\n",
      "Train F1: 0.289030\n",
      "Train F1: 0.576587\n",
      "Train F1: 0.909921\n",
      "epoch: 40, loss: 0.4333\n",
      "Test Loss: 0.453574, Acc: 0.818417, Pre: 0.878927, Rec: 0.878917, F1: 0.878921\n",
      "Train F1: 0.290005\n",
      "Train F1: 0.578299\n",
      "Train F1: 0.911632\n",
      "epoch: 41, loss: 0.2639\n",
      "Test Loss: 0.361707, Acc: 0.857143, Pre: 0.905092, Rec: 0.904598, F1: 0.904675\n",
      "Train F1: 0.291981\n",
      "Train F1: 0.582834\n",
      "Train F1: 0.916168\n",
      "epoch: 42, loss: 0.465\n",
      "Test Loss: 0.545144, Acc: 0.803356, Pre: 0.870718, Rec: 0.869301, F1: 0.868737\n",
      "Train F1: 0.290553\n",
      "Train F1: 0.579893\n",
      "Train F1: 0.913226\n",
      "epoch: 43, loss: 0.3102\n",
      "Test Loss: 0.487419, Acc: 0.808520, Pre: 0.872325, Rec: 0.872329, F1: 0.872327\n",
      "Train F1: 0.291680\n",
      "Train F1: 0.582261\n",
      "Train F1: 0.915595\n",
      "epoch: 44, loss: 0.3865\n",
      "Test Loss: 0.607562, Acc: 0.783133, Pre: 0.857235, Rec: 0.855827, F1: 0.855223\n",
      "Train F1: 0.292892\n",
      "Train F1: 0.584699\n",
      "Train F1: 0.918033\n",
      "epoch: 45, loss: 0.3189\n",
      "Test Loss: 0.454879, Acc: 0.828313, Pre: 0.885530, Rec: 0.885564, F1: 0.885535\n",
      "Train F1: 0.294095\n",
      "Train F1: 0.587137\n",
      "Train F1: 0.920471\n",
      "epoch: 46, loss: 0.3907\n",
      "Test Loss: 0.449519, Acc: 0.839501, Pre: 0.892988, Rec: 0.892972, F1: 0.892979\n",
      "Train F1: 0.295277\n",
      "Train F1: 0.589281\n",
      "Train F1: 0.922615\n",
      "epoch: 47, loss: 0.359\n",
      "Test Loss: 0.507495, Acc: 0.821859, Pre: 0.881803, Rec: 0.881456, F1: 0.881214\n",
      "Train F1: 0.295423\n",
      "Train F1: 0.589863\n",
      "Train F1: 0.923197\n",
      "epoch: 48, loss: 0.295\n",
      "Test Loss: 0.553059, Acc: 0.787005, Pre: 0.859496, Rec: 0.857606, F1: 0.857551\n",
      "Train F1: 0.295722\n",
      "Train F1: 0.590510\n",
      "Train F1: 0.923843\n",
      "epoch: 49, loss: 0.3006\n",
      "Test Loss: 0.481554, Acc: 0.841222, Pre: 0.894755, Rec: 0.894371, F1: 0.894124\n",
      "Train F1: 0.297231\n",
      "Train F1: 0.593447\n",
      "Train F1: 0.926780\n",
      "epoch: 50, loss: 0.1275\n",
      "Test Loss: 0.449474, Acc: 0.851119, Pre: 0.901088, Rec: 0.900914, F1: 0.900738\n",
      "Train F1: 0.297695\n",
      "Train F1: 0.594379\n",
      "Train F1: 0.927712\n",
      "epoch: 51, loss: 0.3132\n",
      "Test Loss: 0.459508, Acc: 0.848537, Pre: 0.899006, Rec: 0.899015, F1: 0.899010\n",
      "Train F1: 0.297569\n",
      "Train F1: 0.594091\n",
      "Train F1: 0.927425\n",
      "epoch: 52, loss: 0.2465\n",
      "Test Loss: 0.525234, Acc: 0.828313, Pre: 0.885526, Rec: 0.885556, F1: 0.885533\n",
      "Train F1: 0.298445\n",
      "Train F1: 0.596032\n",
      "Train F1: 0.929365\n",
      "epoch: 53, loss: 0.1884\n",
      "Test Loss: 0.462314, Acc: 0.855852, Pre: 0.904025, Rec: 0.904008, F1: 0.903901\n",
      "Train F1: 0.298559\n",
      "Train F1: 0.596393\n",
      "Train F1: 0.929727\n",
      "epoch: 54, loss: 0.3377\n",
      "Test Loss: 0.546965, Acc: 0.819707, Pre: 0.880066, Rec: 0.879640, F1: 0.879699\n",
      "Train F1: 0.299549\n",
      "Train F1: 0.598474\n",
      "Train F1: 0.931808\n",
      "epoch: 55, loss: 0.2999\n",
      "Test Loss: 0.463996, Acc: 0.845095, Pre: 0.897105, Rec: 0.896550, F1: 0.896625\n",
      "Train F1: 0.299178\n",
      "Train F1: 0.597538\n",
      "Train F1: 0.930872\n",
      "epoch: 56, loss: 0.2701\n",
      "Test Loss: 0.520228, Acc: 0.846386, Pre: 0.897631, Rec: 0.897658, F1: 0.897589\n",
      "Train F1: 0.299837\n",
      "Train F1: 0.598828\n",
      "Train F1: 0.932161\n",
      "epoch: 57, loss: 0.2039\n",
      "Test Loss: 0.482216, Acc: 0.855422, Pre: 0.903600, Rec: 0.903635, F1: 0.903607\n",
      "Train F1: 0.300482\n",
      "Train F1: 0.600265\n",
      "Train F1: 0.933598\n",
      "epoch: 58, loss: 0.2902\n",
      "Test Loss: 0.441926, Acc: 0.882100, Pre: 0.921561, Rec: 0.921520, F1: 0.921399\n",
      "Train F1: 0.302010\n",
      "Train F1: 0.603422\n",
      "Train F1: 0.936755\n",
      "epoch: 59, loss: 0.1694\n",
      "Test Loss: 0.542582, Acc: 0.840361, Pre: 0.894027, Rec: 0.893375, F1: 0.893451\n",
      "Train F1: 0.301919\n",
      "Train F1: 0.603279\n",
      "Train F1: 0.936612\n",
      "epoch: 60, loss: 0.2484\n",
      "Test Loss: 0.555499, Acc: 0.849398, Pre: 0.899636, Rec: 0.899530, F1: 0.899562\n",
      "Train F1: 0.303069\n",
      "Train F1: 0.605500\n",
      "Train F1: 0.938833\n",
      "epoch: 61, loss: 0.2298\n",
      "Test Loss: 0.532928, Acc: 0.852840, Pre: 0.901932, Rec: 0.901825, F1: 0.901858\n",
      "Train F1: 0.303089\n",
      "Train F1: 0.605791\n",
      "Train F1: 0.939124\n",
      "epoch: 62, loss: 0.214\n",
      "Test Loss: 0.531068, Acc: 0.871773, Pre: 0.914749, Rec: 0.914657, F1: 0.914513\n",
      "Train F1: 0.304382\n",
      "Train F1: 0.608299\n",
      "Train F1: 0.941632\n",
      "epoch: 63, loss: 0.2477\n",
      "Test Loss: 0.496760, Acc: 0.878227, Pre: 0.918925, Rec: 0.918919, F1: 0.918818\n",
      "Train F1: 0.305618\n",
      "Train F1: 0.610663\n",
      "Train F1: 0.943996\n",
      "epoch: 64, loss: 0.299\n",
      "Test Loss: 0.558393, Acc: 0.864458, Pre: 0.909620, Rec: 0.909642, F1: 0.909628\n",
      "Train F1: 0.303399\n",
      "Train F1: 0.606364\n",
      "Train F1: 0.939697\n",
      "epoch: 65, loss: 0.1354\n",
      "Test Loss: 0.473662, Acc: 0.896299, Pre: 0.931589, Rec: 0.931103, F1: 0.930849\n",
      "Train F1: 0.305825\n",
      "Train F1: 0.611240\n",
      "Train F1: 0.944573\n",
      "epoch: 66, loss: 0.2411\n",
      "Test Loss: 0.584830, Acc: 0.856713, Pre: 0.904731, Rec: 0.904329, F1: 0.904400\n",
      "Train F1: 0.306514\n",
      "Train F1: 0.612674\n",
      "Train F1: 0.946008\n",
      "epoch: 67, loss: 0.2701\n",
      "Test Loss: 0.594796, Acc: 0.869621, Pre: 0.913110, Rec: 0.913144, F1: 0.913080\n",
      "Train F1: 0.305600\n",
      "Train F1: 0.610810\n",
      "Train F1: 0.944143\n",
      "epoch: 68, loss: 0.2562\n",
      "Test Loss: 0.446825, Acc: 0.898021, Pre: 0.932056, Rec: 0.932088, F1: 0.932013\n",
      "Train F1: 0.305793\n",
      "Train F1: 0.611168\n",
      "Train F1: 0.944501\n",
      "epoch: 69, loss: 0.3465\n",
      "Test Loss: 0.571613, Acc: 0.847676, Pre: 0.900314, Rec: 0.898064, F1: 0.898132\n",
      "Train F1: 0.306279\n",
      "Train F1: 0.612172\n",
      "Train F1: 0.945505\n",
      "epoch: 70, loss: 0.2658\n",
      "Test Loss: 0.591336, Acc: 0.861446, Pre: 0.908000, Rec: 0.907460, F1: 0.907542\n",
      "Train F1: 0.306343\n",
      "Train F1: 0.612388\n",
      "Train F1: 0.945721\n",
      "epoch: 71, loss: 0.1855\n",
      "Test Loss: 0.491533, Acc: 0.882530, Pre: 0.921819, Rec: 0.921589, F1: 0.921646\n",
      "Train F1: 0.308774\n",
      "Train F1: 0.617336\n",
      "Train F1: 0.950669\n",
      "epoch: 72, loss: 0.1767\n",
      "Test Loss: 0.516070, Acc: 0.875645, Pre: 0.917507, Rec: 0.916924, F1: 0.917015\n",
      "Train F1: 0.309544\n",
      "Train F1: 0.618914\n",
      "Train F1: 0.952247\n",
      "epoch: 73, loss: 0.1395\n",
      "Test Loss: 0.573692, Acc: 0.898021, Pre: 0.932517, Rec: 0.932214, F1: 0.932005\n",
      "Train F1: 0.308667\n",
      "Train F1: 0.616976\n",
      "Train F1: 0.950309\n",
      "epoch: 74, loss: 0.3211\n",
      "Test Loss: 0.685909, Acc: 0.880809, Pre: 0.920763, Rec: 0.920678, F1: 0.920538\n",
      "Train F1: 0.309567\n",
      "Train F1: 0.618841\n",
      "Train F1: 0.952174\n",
      "epoch: 75, loss: 0.2003\n",
      "Test Loss: 0.667132, Acc: 0.839931, Pre: 0.895171, Rec: 0.892892, F1: 0.892942\n",
      "Train F1: 0.308365\n",
      "Train F1: 0.616692\n",
      "Train F1: 0.950025\n",
      "epoch: 76, loss: 0.2096\n",
      "Test Loss: 0.587344, Acc: 0.892857, Pre: 0.928772, Rec: 0.928704, F1: 0.928570\n",
      "Train F1: 0.308302\n",
      "Train F1: 0.616404\n",
      "Train F1: 0.949737\n",
      "epoch: 77, loss: 0.3315\n",
      "Test Loss: 0.922450, Acc: 0.859294, Pre: 0.906870, Rec: 0.906429, F1: 0.906172\n",
      "Train F1: 0.309983\n",
      "Train F1: 0.619774\n",
      "Train F1: 0.953107\n",
      "epoch: 78, loss: 0.1428\n",
      "Test Loss: 0.653814, Acc: 0.872203, Pre: 0.914802, Rec: 0.914769, F1: 0.914783\n",
      "Train F1: 0.311249\n",
      "Train F1: 0.622283\n",
      "Train F1: 0.955617\n",
      "epoch: 79, loss: 0.1198\n",
      "Test Loss: 0.619262, Acc: 0.889845, Pre: 0.926612, Rec: 0.926640, F1: 0.926563\n",
      "Train F1: 0.311877\n",
      "Train F1: 0.623574\n",
      "Train F1: 0.956908\n",
      "epoch: 80, loss: 0.1809\n",
      "Test Loss: 0.564838, Acc: 0.889415, Pre: 0.926270, Rec: 0.926312, F1: 0.926273\n",
      "Train F1: 0.310787\n",
      "Train F1: 0.621496\n",
      "Train F1: 0.954829\n",
      "epoch: 81, loss: 0.2465\n",
      "Test Loss: 0.785485, Acc: 0.896730, Pre: 0.933723, Rec: 0.931586, F1: 0.931050\n",
      "Train F1: 0.309667\n",
      "Train F1: 0.619201\n",
      "Train F1: 0.952534\n",
      "epoch: 82, loss: 0.1064\n",
      "Test Loss: 0.671493, Acc: 0.878657, Pre: 0.919087, Rec: 0.919106, F1: 0.919095\n",
      "Train F1: 0.312022\n",
      "Train F1: 0.623933\n",
      "Train F1: 0.957267\n",
      "epoch: 83, loss: 0.1028\n",
      "Test Loss: 0.572356, Acc: 0.902754, Pre: 0.935508, Rec: 0.935336, F1: 0.935165\n",
      "Train F1: 0.313048\n",
      "Train F1: 0.626013\n",
      "Train F1: 0.959346\n",
      "epoch: 84, loss: 0.1233\n",
      "Test Loss: 0.578243, Acc: 0.888124, Pre: 0.925503, Rec: 0.925337, F1: 0.925384\n",
      "Train F1: 0.313616\n",
      "Train F1: 0.627160\n",
      "Train F1: 0.960493\n",
      "epoch: 85, loss: 0.09597\n",
      "Test Loss: 0.659217, Acc: 0.895009, Pre: 0.930194, Rec: 0.930135, F1: 0.930005\n",
      "Train F1: 0.313827\n",
      "Train F1: 0.627590\n",
      "Train F1: 0.960923\n",
      "epoch: 86, loss: 0.139\n",
      "Test Loss: 0.689405, Acc: 0.892857, Pre: 0.928993, Rec: 0.928756, F1: 0.928564\n",
      "Train F1: 0.315055\n",
      "Train F1: 0.630028\n",
      "Train F1: 0.963361\n",
      "epoch: 87, loss: 0.1288\n",
      "Test Loss: 0.604864, Acc: 0.893718, Pre: 0.929128, Rec: 0.929152, F1: 0.929137\n",
      "Train F1: 0.315955\n",
      "Train F1: 0.631892\n",
      "Train F1: 0.965225\n",
      "epoch: 88, loss: 0.1242\n",
      "Test Loss: 0.603579, Acc: 0.885972, Pre: 0.924076, Rec: 0.923899, F1: 0.923948\n",
      "Train F1: 0.314257\n",
      "Train F1: 0.628522\n",
      "Train F1: 0.961856\n",
      "epoch: 89, loss: 0.1781\n",
      "Test Loss: 0.800376, Acc: 0.868330, Pre: 0.912314, Rec: 0.912132, F1: 0.912180\n",
      "Train F1: 0.312693\n",
      "Train F1: 0.625367\n",
      "Train F1: 0.958701\n",
      "epoch: 90, loss: 0.1532\n",
      "Test Loss: 0.597746, Acc: 0.897160, Pre: 0.931427, Rec: 0.931432, F1: 0.931429\n",
      "Train F1: 0.313890\n",
      "Train F1: 0.627805\n",
      "Train F1: 0.961139\n",
      "epoch: 91, loss: 0.1476\n",
      "Test Loss: 0.673688, Acc: 0.888985, Pre: 0.926017, Rec: 0.925940, F1: 0.925968\n",
      "Train F1: 0.315642\n",
      "Train F1: 0.631319\n",
      "Train F1: 0.964652\n",
      "epoch: 92, loss: 0.1679\n",
      "Test Loss: 0.559293, Acc: 0.904905, Pre: 0.936630, Rec: 0.936560, F1: 0.936586\n",
      "Train F1: 0.316432\n",
      "Train F1: 0.632896\n",
      "Train F1: 0.966229\n",
      "epoch: 93, loss: 0.2201\n",
      "Test Loss: 0.567268, Acc: 0.907487, Pre: 0.938312, Rec: 0.938318, F1: 0.938315\n",
      "Train F1: 0.316774\n",
      "Train F1: 0.633541\n",
      "Train F1: 0.966875\n",
      "epoch: 94, loss: 0.08507\n",
      "Test Loss: 0.697762, Acc: 0.892857, Pre: 0.928620, Rec: 0.928511, F1: 0.928547\n",
      "Train F1: 0.315914\n",
      "Train F1: 0.631820\n",
      "Train F1: 0.965154\n",
      "epoch: 95, loss: 0.09372\n",
      "Test Loss: 0.572211, Acc: 0.913941, Pre: 0.942632, Rec: 0.942677, F1: 0.942626\n",
      "Train F1: 0.317508\n",
      "Train F1: 0.635047\n",
      "Train F1: 0.968380\n",
      "epoch: 96, loss: 0.1066\n",
      "Test Loss: 0.562391, Acc: 0.913081, Pre: 0.942078, Rec: 0.942118, F1: 0.942053\n",
      "Train F1: 0.318389\n",
      "Train F1: 0.636768\n",
      "Train F1: 0.970101\n",
      "epoch: 97, loss: 0.08431\n",
      "Test Loss: 0.632151, Acc: 0.903614, Pre: 0.935788, Rec: 0.935688, F1: 0.935722\n",
      "Train F1: 0.318647\n",
      "Train F1: 0.637341\n",
      "Train F1: 0.970675\n",
      "epoch: 98, loss: 0.2217\n",
      "Test Loss: 0.589492, Acc: 0.889415, Pre: 0.927636, Rec: 0.925972, F1: 0.926114\n",
      "Train F1: 0.319980\n",
      "Train F1: 0.640066\n",
      "Train F1: 0.973399\n",
      "epoch: 99, loss: 0.1475\n",
      "Test Loss: 0.628223, Acc: 0.914802, Pre: 0.943185, Rec: 0.943214, F1: 0.943195\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_validate, KFold\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "learning_rate = 0.002\n",
    "\n",
    "vector_vae = torch.from_numpy(pd.read_csv('track_vae_balanced.csv').values).float()\n",
    "label = torch.from_numpy(pd.read_csv('track_new_balanced.csv').values).float()\n",
    "\n",
    "totEpoch = 100\n",
    "num_class = 3\n",
    "\n",
    "results, F1_train, F1_test, Loss_train, Loss_test,TPR, FPR, y_pred, y_lable = train_cross_val(100, vector_vae, label, totEpoch, num_class, 5)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(results[0]).to_csv('accuracy_list.csv', index=False)\n",
    "pd.DataFrame(results[1]).to_csv('precision_list.csv', index=False)\n",
    "pd.DataFrame(results[2]).to_csv('recall_list.csv', index=False)\n",
    "pd.DataFrame(results[3]).to_csv('F1_list.csv', index=False)\n",
    "pd.DataFrame(results[4]).to_csv('F1_list_train.csv', index=False)\n",
    "pd.DataFrame(results[5]).to_csv('loss_list.csv', index=False)\n",
    "pd.DataFrame(results[6]).to_csv('train_loss_list.csv', index=False)\n",
    "pd.DataFrame(results[7]).to_csv('train_var_list.csv', index=False)\n",
    "pd.DataFrame(results[8]).to_csv('test_var_list.csv', index=False)\n",
    "pd.DataFrame(F1_train).to_csv('F1_train.csv', index=False)\n",
    "pd.DataFrame(F1_test).to_csv('F1_test.csv', index=False)\n",
    "pd.DataFrame(Loss_train).to_csv('Loss_train.csv', index=False)\n",
    "pd.DataFrame(Loss_test).to_csv('Loss_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numpy import interp\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def prec_rec_curve(y_lable, y_pred, titile=\"\"):\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    y_test = y_lable\n",
    "    y_score = y_pred\n",
    "    fpr[0], tpr[0], _ = roc_curve(y_test, y_score)\n",
    "    roc_auc[0] = auc(fpr[0], tpr[0])\n",
    "\n",
    "    plt.plot(fpr[0], tpr[0], color=\"cornflowerblue\", lw=2,\n",
    "                 label='ROC curve (area = {1:0.2f})'.format(0, roc_auc[0]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numpy import interp\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def prec_rec_curve(y_lable, y_pred, titile=\"\"):\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    n_classes = 2\n",
    "    y_test = label_binarize(y_lable, classes=[1, 2])\n",
    "    y_score = label_binarize(y_pred, classes=[1, 2])\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    try:\n",
    "        micro_auc = roc_auc_score(y_test, y_score, average='micro')\n",
    "    except ValueError:\n",
    "        pass\n",
    "    # micro_auc = roc_auc_score(y_test, y_score, average='micro')\n",
    "    \n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "    try:\n",
    "        macro_auc = roc_auc_score(y_test, y_score, average='macro')\n",
    "    except ValueError:\n",
    "        pass\n",
    "    # macro_auc = roc_auc_score(y_test, y_score, average='macro')\n",
    "\n",
    "    print(roc_auc)\n",
    "    print('micro auc:', micro_auc)\n",
    "    print('macro auc:', macro_auc)\n",
    "    \n",
    "    # Plot all ROC curves\n",
    "    plt.figure()\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label='micro-average ROC curve (area = {0:0.2f})'.format(roc_auc[\"micro\"]),\n",
    "             color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "             label='macro-average ROC curve (area = {0:0.2f})'.format(roc_auc[\"macro\"]),\n",
    "             color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "    colors = ['aqua', 'darkorange', 'cornflowerblue']\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                 label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('figures/VAE_ROC.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# prec_rec_curve(np.array(y_lable).ravel(), np.array(y_pred).ravel())\n",
    "prec_rec_curve(np.concatenate( y_lable, axis=0 ), np.concatenate( y_pred, axis=0 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "cm = confusion_matrix(np.concatenate( y_lable, axis=0 ), np.concatenate( y_pred, axis=0 ), normalize = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "normalize = True\n",
    "cmap = 'RdPu'\n",
    "classes = [0, 1]\n",
    "title = 'Cofusion Matrix'\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "ax.figure.colorbar(im, ax = ax)\n",
    "ax.set(xticks = np.arange(cm.shape[1]), yticks = np.arange(cm.shape[0]), xticklabels = classes, yticklabels = classes, ylabel = 'True label', xlabel = 'Predicted label', title = title)\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha = 'right', rotation_mode = 'anchor')\n",
    "fmt = '.4%' if normalize else 'd'\n",
    "thresh = cm.max() / 2\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, format(cm[i, j], fmt), ha = 'center', va = 'center', color = 'white' if cm[i,j] > thresh else 'black')\n",
    "        fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "NQA08EUIFZf7",
    "outputId": "89db85a0-c568-4522-f875-5d3725abc06e"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'figures/VAE_Loss_F1.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9254/1577149023.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoches'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figures/VAE_Loss_F1.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# need this if 'transparent=True' to reset colors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   3013\u001b[0m                 \u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_edgecolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3015\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3017\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtransparent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2259\u001b[0m                         \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2260\u001b[0m                         \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2261\u001b[0;31m                         **kwargs)\n\u001b[0m\u001b[1;32m   2262\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2263\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/_api/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m                          \u001b[0;32melse\u001b[0m \u001b[0mdeprecation_addendum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                 **kwargs)\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/_api/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m                          \u001b[0;32melse\u001b[0m \u001b[0mdeprecation_addendum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                 **kwargs)\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/_api/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m                          \u001b[0;32melse\u001b[0m \u001b[0mdeprecation_addendum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                 **kwargs)\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_jpg\u001b[0;34m(self, filename_or_obj, pil_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;31m# Drop alpha channel now.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         return (Image.fromarray(np.asarray(self.buffer_rgba())[..., :3])\n\u001b[0;32m--> 585\u001b[0;31m                 .save(filename_or_obj, format='jpeg', **pil_kwargs))\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0mprint_jpeg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprint_jpg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2230\u001b[0m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2232\u001b[0;31m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2234\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'figures/VAE_Loss_F1.jpg'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "accuracy_list = torch.from_numpy(pd.read_csv('accuracy_list.csv').values).float()\n",
    "precision_list = torch.from_numpy(pd.read_csv('precision_list.csv').values).float()\n",
    "recall_list = torch.from_numpy(pd.read_csv('recall_list.csv').values).float()\n",
    "F1_list = torch.from_numpy(pd.read_csv('F1_list.csv').values).float()\n",
    "F1_list_train = torch.from_numpy(pd.read_csv('F1_list_train.csv').values).float()\n",
    "loss_list = torch.from_numpy(pd.read_csv('loss_list.csv').values).float()\n",
    "train_loss_list = torch.from_numpy(pd.read_csv('train_loss_list.csv').values).float()\n",
    "train_var_list = torch.from_numpy(pd.read_csv('train_var_list.csv').values).float()\n",
    "test_var_list = torch.from_numpy(pd.read_csv('test_var_list.csv').values).float()\n",
    "\n",
    "totEpoch = 100\n",
    "x = range(0, totEpoch)\n",
    "plt.figure(figsize=(14,3))\n",
    "grid = plt.GridSpec(3, 2, wspace=0.5, hspace=0.5)\n",
    "plt.subplot(grid[:,0])\n",
    "# plt.plot(x, F1_list_train, color=\"b\", marker='o',markersize='1.5',markeredgecolor='b',markeredgewidth = 1.5, label = 'Train F1 score')\n",
    "plt.plot(x, F1_list, color=\"r\", marker='o',markersize='1.5',markeredgecolor='r',markeredgewidth = 1.5, label = 'Test F1 score')\n",
    "# plt.plot(x, train_var_list, color=\"g\", marker='o',markersize='1.5',markeredgecolor='g',markeredgewidth = 1.5, label = 'Train variance')\n",
    "#plt.plot(x, test_var_list, color=\"c\", marker='o',markersize='1.5',markeredgecolor='c',markeredgewidth = 1.5, label = 'Test variance')\n",
    "plt.legend()\n",
    "plt.title('F1 Score vs Epoches')\n",
    "plt.xlabel('Epoches')\n",
    "plt.ylabel('F1 score')\n",
    "plt.subplot(grid[:,1])\n",
    "plt.plot(x, train_loss_list, color=\"red\", marker='o',markersize='1.5',markeredgecolor='b',markeredgewidth = 1.5, label = 'Train Loss')\n",
    "plt.plot(x, loss_list, color=\"red\", marker='o',markersize='1.5',markeredgecolor='b',markeredgewidth = 1.5, label = 'Test Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss vs Epoches')\n",
    "plt.xlabel('Epoches')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig('figures/VAE_Loss_F1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "NQA08EUIFZf7",
    "outputId": "89db85a0-c568-4522-f875-5d3725abc06e"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'figures/VAE_Loss_F1.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2993/1577149023.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoches'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figures/VAE_Loss_F1.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# need this if 'transparent=True' to reset colors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   3013\u001b[0m                 \u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_edgecolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3015\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3017\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtransparent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2259\u001b[0m                         \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2260\u001b[0m                         \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2261\u001b[0;31m                         **kwargs)\n\u001b[0m\u001b[1;32m   2262\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2263\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/_api/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m                          \u001b[0;32melse\u001b[0m \u001b[0mdeprecation_addendum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                 **kwargs)\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/_api/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m                          \u001b[0;32melse\u001b[0m \u001b[0mdeprecation_addendum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                 **kwargs)\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/_api/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m                          \u001b[0;32melse\u001b[0m \u001b[0mdeprecation_addendum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                 **kwargs)\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_jpg\u001b[0;34m(self, filename_or_obj, pil_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;31m# Drop alpha channel now.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         return (Image.fromarray(np.asarray(self.buffer_rgba())[..., :3])\n\u001b[0;32m--> 585\u001b[0;31m                 .save(filename_or_obj, format='jpeg', **pil_kwargs))\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0mprint_jpeg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprint_jpg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2230\u001b[0m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2232\u001b[0;31m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2234\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'figures/VAE_Loss_F1.jpg'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "accuracy_list = torch.from_numpy(pd.read_csv('accuracy_list.csv').values).float()\n",
    "precision_list = torch.from_numpy(pd.read_csv('precision_list.csv').values).float()\n",
    "recall_list = torch.from_numpy(pd.read_csv('recall_list.csv').values).float()\n",
    "F1_list = torch.from_numpy(pd.read_csv('F1_list.csv').values).float()\n",
    "F1_list_train = torch.from_numpy(pd.read_csv('F1_list_train.csv').values).float()\n",
    "loss_list = torch.from_numpy(pd.read_csv('loss_list.csv').values).float()\n",
    "train_loss_list = torch.from_numpy(pd.read_csv('train_loss_list.csv').values).float()\n",
    "train_var_list = torch.from_numpy(pd.read_csv('train_var_list.csv').values).float()\n",
    "test_var_list = torch.from_numpy(pd.read_csv('test_var_list.csv').values).float()\n",
    "\n",
    "totEpoch = 100\n",
    "x = range(0, totEpoch)\n",
    "plt.figure(figsize=(14,3))\n",
    "grid = plt.GridSpec(3, 2, wspace=0.5, hspace=0.5)\n",
    "plt.subplot(grid[:,0])\n",
    "# plt.plot(x, F1_list_train, color=\"b\", marker='o',markersize='1.5',markeredgecolor='b',markeredgewidth = 1.5, label = 'Train F1 score')\n",
    "plt.plot(x, F1_list, color=\"r\", marker='o',markersize='1.5',markeredgecolor='r',markeredgewidth = 1.5, label = 'Test F1 score')\n",
    "# plt.plot(x, train_var_list, color=\"g\", marker='o',markersize='1.5',markeredgecolor='g',markeredgewidth = 1.5, label = 'Train variance')\n",
    "#plt.plot(x, test_var_list, color=\"c\", marker='o',markersize='1.5',markeredgecolor='c',markeredgewidth = 1.5, label = 'Test variance')\n",
    "plt.legend()\n",
    "plt.title('F1 Score vs Epoches')\n",
    "plt.xlabel('Epoches')\n",
    "plt.ylabel('F1 score')\n",
    "plt.subplot(grid[:,1])\n",
    "plt.plot(x, train_loss_list, color=\"red\", marker='o',markersize='1.5',markeredgecolor='b',markeredgewidth = 1.5, label = 'Train Loss')\n",
    "plt.plot(x, loss_list, color=\"red\", marker='o',markersize='1.5',markeredgecolor='b',markeredgewidth = 1.5, label = 'Test Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss vs Epoches')\n",
    "plt.xlabel('Epoches')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig('figures/VAE_Loss_F1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-9.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m82"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
